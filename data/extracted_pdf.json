{
  "page_1": "1\nText Data Augmentation for Large Language\nModels: A Comprehensive Survey of Methods,\nChallenges, and Opportunities\nYaping Chai, Haoran Xie, Joe S. Qin\nAbstract—The increasing size and complexity of pre-trained lan-\nguage models have demonstrated superior performance in many\napplications, but they usually require large training datasets to be\nadequately trained. Insufficient training sets could unexpectedly\nmake the model overfit and fail to cope with complex tasks.\nLarge language models (LLMs) trained on extensive corpora\nhave prominent text generation capabilities, which improve the\nquality and quantity of data and play a crucial role in data\naugmentation. Specifically, distinctive prompt templates are given\nin personalised tasks to guide LLMs in generating the required\ncontent. Recent promising retrieval-based techniques further im-\nprove the expressive performance of LLMs in data augmentation\nby introducing external knowledge to enable them to produce\nmore grounded-truth data. This survey provides an in-depth\nanalysis of data augmentation in LLMs, classifying the tech-\nniques into Simple Augmentation, Prompt-based Augmentation,\nRetrieval-based Augmentation and Hybrid Augmentation. We\nsummarise the post-processing approaches in data augmentation,\nwhich contributes significantly to refining the augmented data\nand enabling the model to filter out unfaithful content. Then, we\nprovide the common tasks and evaluation metrics. Finally, we\nintroduce existing challenges and future opportunities that could\nbring further improvement to data augmentation.\nIndex Terms—Data Augmentation, Large Language Models,\nText Processing, Natural Language Processing\nI. INTRODUCTION\nLLMs have demonstrated remarkable language understand-\ning and generation capabilities with their massive number of\nparameters and training data, bringing innovation to many\napplications [1]. However, training and optimizing pre-trained\nlanguage models (PLMs) requires enormous amounts of high-\nquality data. Poor data quality and data scarcity issues hinder\nthe further improvement of PLMs [2]. To solve these chal-\nlenges, data augmentation is an effective technique to generate\nmore available training data by transforming and expanding\nexisting data, improving models’ performance in many natural\nlanguage processing (NLP) tasks [3]. The earliest popular data\naugmentation methods include synonym replacement, word\norder scrambling and random word deletion [4]. Although these\nThis work was supported by a grant from the Research Grants Council\nof the Hong Kong Special Administrative Region, China (R1015-23) and the\nFaculty Research Grant (SDS24A8) and the Direct Grant (DR25E8) of Lingnan\nUniversity, Hong Kong. (Corresponding author: Haoran Xie.)\nYaping Chai, Haoran Xie, and Joe S. Qin are with the School of\nData Science, Lingnan University, Hong Kong (e-mail: yapingchai@ln.hk;\nhrxie@ln.edu.hk; joeqin@ln.edu.hk).\nconventional methods could enhance existing data, simple word\ntransformations may not be able to fully utilize the potential of\nLLMs [5].\nWith the increasing development of prompt engineering\nand its application in LLMs, many researchers augment the\nseed training data by designing crafted prompts for LLMs\nto generate diverse datasets, contributing to producing high-\nquality and semantically rich data [6].\nProviding prompts to LLMs cannot access up-to-date factual\nknowledge, unavoidably causing the hallucination phenomenon\n[7], [8]. Many studies have explored retrieving external data\nfor data augmentation. Integrating additional knowledge sig-\nnificantly enhances PLMs’ understanding and generalization\ncapabilities by accessing relevant facts from a large-scale\nexternal corpus [9].\nAlong with the continuous progress of research, studies have\nnot only retrieved grounded facts from external corpora but also\ncombined them with LLMs’ powerful few-shot capability to\neffectively apply them to various tasks [8]. A typical application\nis problem-answering tasks, where a retrieval model is used to\nobtain relevant documents from a database given a query. Then,\nadopting few-shot learning to prompt the model to answer\nquestions based on the retrieved documents without fine-tuning\nor learning additional parameters [8].\nBased on the background of LLMs, we divide the techniques\nof text data augmentation into four categories: Simple Aug-\nmentation, Prompt-based Augmentation, Retrieval-based Aug-\nmentation, and Hybrid Augmentation. Figure 1 demonstrates\neach example of four techniques. We present recent research\non the four categories of text data augmentation in Figure 2. We\nfocus on summarising how recent studies have performed data\naugmentation under four technique categories. We introduce\na comprehensive review of multiple data augmentation aspects\nand granularity [9], aiming to illustrate the foundational prelim-\ninaries. Considering that the effectiveness of data augmentation\ncannot be entirely guaranteed, we demonstrate common post-\nprocessing approaches to refine generated data quality further.\nIn summary, the contributions of this survey are as follows:\n• We comprehensively demonstrate data augmentation tech-\nniques in the context of large-scale language models,\nincluding early Simple Augmentation, Prompt-based Aug-\nmentation, Retrieval-based Augmentation, and Hybrid\nAugmentation, outlining the development stages of data\narXiv:2501.18845v1  [cs.CL]  31 Jan 2025\n",
  "page_2": "2\nFigure 1: Four categories of data augmentation techniques.\naugmentation under the trend of increasing model size.\n• We present two prerequisites before data augmentation,\naspect and granularity, and introduce the refinement ap-\nproaches after data augmentation, providing a systematic\nand complete structure.\n• We summarize the common tasks and evaluation metrics\nof data augmentation in various applications and analyze\nthe current limitations and worthwhile future research\ndirections.\nThe rest of this survey is structured as follows: section II\nintroduces LLMs, which typically refer to encoder-only models,\ndecoder-only models, and encoder-decoder models. Followed\nby presenting data augmentation aspects and granularity from\nthe finest to the coarsest. Section III showcases a detailed\nbreakdown of the four technique categories, revealing typical\ntraditional methods, the complexity of distinct designs for\nprompts, diverse retriever types and the various combinations of\nPrompt-based Augmentation and Retrieval-based Augmentation\nin recent studies. Section IV focuses on interpreting widely\nused post-processing approaches for data augmentation. Section\nV describes common tasks and evaluation metrics. Section VI\nexplains the unsolved challenges and future research directions.\nFinally, we conclude this survey in section VII.\nII. PRELIMINARIES\nA. Large Language Models\nLarge language models (LLMs) trained on an enormous\ncorpus with rich language knowledge and prominent generation\ncapabilities are widely used in academia and industry. A\npopular approach to divide LLMs is based on the Transformer\n[67] architecture: encoder-only models (e.g. the BERT families\n[68]), decoder-only models (e.g. the GPT families [69]), and\nencoder-decoder models (e.g. T5 [70] and BART [71]).\n1) Encoder-only Models: The encoder-only model is rep-\nresented by BERT [68], which has the main feature of bidi-\nrectional encoding. BERT performs contextual bidirectional\nencoding on the input data and captures multiple semantic\ninformation of each word in different contexts, enabling BERT\nand its variants to showcase outstanding performance in down-\nstream tasks such as sentiment classification [10], [20], [72]\nnamed entity recognition [35], [5], and question-answering\nsystems [24], [33], [4].\n2) Decoder-only Models:\nCompared with encoder-only\nmodels, decoder-only models include ChatGPT [73] that only\nuse the Transformer decoder structure and demonstrate power-\nful text generation abilities. They predict text word by word\nthrough unidirectional decoding and produce coherent and\n",
  "page_3": "3\nFigure 2: Recent studies on four categories of data augmentation techniques. As mentioned in section III, Hybrid Augmentation\ntechnique combines superior few-shot learning capabilities similar to prompt engineering and a retriever to obtain external\nknowledge. Using only the prompt portion of the RAG itself, we categorise it as the Retrieval-based Augmentation technique.\nnatural responses, allowing them to generate diverse and high-\nquality training data that effectively improve models’ robust-\nness and generalisation ability [3], [37], [40].\n3) Encoder-decoder Models: Encoder-decoder models com-\nbine the advantages of Transformer’s encoder and decoder.\nRepresentative models are T5 [70] and BART [71]. The dual-\nmodule structure empowers them not only to understand the\ndeep semantics of the input content but also to generate\nsemantically augmented data. Many researches train encoder-\ndecoder models for diverse NLP tasks [26], [29], [34], [36],\n[39] and utilize them for text generation without fine-tuning\n[4], [11], [25], [33].\nB. Data Augmentation Aspects\nData augmentation (DA) is one of the critical techniques to\nimprove the performance of pre-trained language models by\ntransforming existing data to obtain diverse and augmented\ndata. Instead of limiting one DA aspect, combining multiple\naspects could generate an extensive synthetic dataset. The\ncommon aspects of data augmentation include the following\nways and are illustrated in Table I.\n1) Data Generation: Data generation is one of the most\nbasic and crucial aspects of data augmentation [74]. A growing\ntrend in generation tasks is to employ a Transformer-based\nlanguage model with a decoder component to create new\ndatasets [11].\n",
  "page_4": "4\nTable I: Data Augmentation Aspects.\nMethods\nGeneration\nParaphrasing\nTranslation\nLabeling\nRetrieval\nEditing\nSimple Augmentation\nTransformersDA [10]\n✓\nDAGAM [11]\n✓\n✓\nGenAug [12]\n✓\nAuGPT [13]\n✓\nCOCA [14]\n✓\nSelection-DA [15]\n✓\nLAMBADA [16]\n✓\n✓\nLeCA [17]\n✓\nG-DAUGc [18]\n✓\nMRC-QA [19]\n✓\nPrompt-based Augmentation\nGPT3Mix [20]\n✓\nDA-intent [21]\n✓\nWANLI [22]\n✓\nFlipDA [4]\n✓\n✓\nAugESC [23]\n✓\nAugGPT [2]\n✓\nRead-Com [24]\n✓\nDAIL [25]\n✓\n✓\nDA-NMT [26]\n✓\n✓\n✓\nEPA [27]\n✓\nZeroShotDataAug [28]\n✓\nDialogue-Convert [29]\n✓\nHiPSTG [30]\n✓\nSUNGEN [31]\n✓\nLLM-powered [1]\n✓\nLLM-PTM [32]\n✓\nGenerative-DA [33]\n✓\nICLEF [34]\n✓\n✓\nLLM-DA [35]\n✓\n✓\nSynthetic-DA [36]\n✓\n✓\nLLM–Assisted [3]\n✓\nLLM2LLM [37]\n✓\nPromptMix [38]\n✓\n✓\nUnnatural-instructions [39]\n✓\nGENIUS [5]\n✓\nTAPP [40]\n✓\nX-GEAR [41]\n✓\nInPars [42]\n✓\n✓\nConvAug [43]\n✓\n✓\nPromptagator [44]\n✓\nDAPDR [45]\n✓\nUDAPDR [46]\n✓\nRetrieval-based Augmentation\nAugmentedSBERT [47]\n✓\n✓\nzicl [48]\n✓\n✓\nRetGen [7]\n✓\nInternet-Aug [49]\n✓\nDialogGen [50]\n✓\nChatPLUG [51]\n✓\nEDGE [52]\n✓\nRGQA [53]\n✓\nCGRG [54]\n✓\nIM-RAG [55]\n✓\nEAE-RAG [56]\n✓\nSeeKeR [57]\n✓\nEfficient-RAG [58]\n✓\nLAPDOG [59]\n✓\nPersonae-DA [60]\n✓\nHybrid Augmentation\nDAICL [61]\n✓\n✓\nKAPING [62]\n✓\nALCE [63]\n✓\nRADA [64]\n✓\n✓\nUniMS-RAG [65]\n✓\nQA-Internet [8]\n✓\n✓\nReAct [66]\n✓\n",
  "page_5": "5\nFor decoder-only models, such as Read-Com [24] leverages\nGPT-4 [75] to obtain synthesised datasets similar to the original\ndata’s style and semantics. LLM–Assisted [3] adopts Llama2\n[76] to produce three different level augmentations based on a\nprovided sample in the training set.\nFor encoder-decoder models, DAGAM [11] summarises a\nvast set of sentences from original datasets and utilizes T5\n[70] to generate synthetic data with a similar representation\ndistribution to original datasets. TransformersDA [10] first\nemploys two word-level masking strategies to reconstruct the\noriginal sequence. Then, it applies BART [71] to decode the\nmasked sequence and produce high-quality output text.\n2) Data Paraphrasing: Data paraphrasing refers to express-\ning the original training data differently within a reasonable\ndeviation. The original training data is rephrased by changing\nthe sentence’s structure, wording or grammar. EPA [27] pro-\nposes to paraphrase the available demonstrations on the source\nand target sides to acquire generated demonstrations for in-\ncontext learning. DAIL [25] expands the data by using the\nLLM itself to paraphrase the original test samples and obtain\nmultiple candidate texts. The final labels are created by not\nonly considering the original text but also taking into account\nall paraphrased candidates. Unnatural-instructions [39] first\nuse structured instruction format and some filtering heuristic\nmethods to collect a core sample set. Then, it rephrases struc-\ntured instructions to extend the core dataset by prompting the\nlanguage model to use manually constructed samples. Similarly,\nAugGPT [2] designs prompts to guide LLMs to rephrase the\noriginal data and obtain more data. ConvAug [43] introduces\nto mimic the variety of user expressions for similar intentions\nby utilising the LLM to extend the language’s diversity by\nparaphrasing the whole context, reducing the model to overfit\nspecific phrases or patterns.\n3) Data Translation: Data translation aims to translate text\nfrom one language to another language and then translate\nit back to the original language. DA-NMT [26] generates n\ntranslations with the same meaning but different languages for\neach original source sentence s, then maps a source sentence\nto n target sentences, and finally generates n parallel data.\nAuGPT [13] utilizes the advantages of back-translation [77]\nand employs a well-trained multilingual machine translation\nmodel to rephrase the data. It uses ten intermediate languages\nto obtain distinct new data for each input utterance.\n4) Data Editing: Data editing refers to performing simple\ntransformations on the text, which includes insertion[12], re-\nordering [14], deletion [12], rewriting [35] and shuffling [11].\nDAGAM [11] implements a character order change strategy\nby fixing the first and last characters in a word and shuffling\nthe rest to augment the data. LLM-DA [35] creates entities\nthat are semantically similar to the sentence but diverse by\nreplacing entities in the sentence with other entities of the\nsame type. ConvAug [43] proposes Entity Replacing as one\nof its augmentation strategies. It uses the LLM to recognise\nand replace entities in a context similar to the original but\ndiffering in significant details. The model could focus more\nclosely on crucial information rather than trivial aspects through\ncontrastive learning. Many tasks assume a strict sequence order.\nHowever, COCA [14] observes that the sequence of users’\nsearch behaviour is more flexible. It proposes a behavioural\nreordering strategy to avoid relying too much on sequential\ninformation.\n5) Data labeling: Data labeling refers to assigning labels to\nsynthetic samples or unlabeled data [74]. In practice, adopting\nmanual annotation to obtain more ground-truth labels is ex-\npensive and cannot be performed on a considerable scale [36].\nManual annotation by experts may require additional implicit\nknowledge that is difficult to capture through statistical methods\n[15].\nA common way to automatically label synthesised examples\nis using a language model [25]. In the domain adaptation\ndata augmentation strategy, AugmentedSBERT [47] first fine-\ntune BERT on the source domain comprising paired anno-\ntations. After fine-tuning, the fine-tuned BERT is applied to\nannotate the target domain. Finally, training SBERT on the\nannotated target domain sentence pairs. Once the retrieved\nsentences are obtained, zicl [48] pairs each sentence with\na label by a synonym labeling strategy to reduce the copy\neffect phenomenon. DAIL [25] considers candidate labels from\nthe original sample and all paraphrased texts and determines\nthe final label by majority voting method. Synthetic-DA [36]\nattempts two scenarios: one is to annotate the set of unlabeled\ninstances when there is a significant amount of unlabeled text,\nbut the cost of obtaining annotations is high; the other is to\ngenerate the entire input-output pair without available annotated\ninstances. PromptMix [38] first directs an LLM to produce\nsamples that mix two classes and then uses relabeling prompts\nto enhance the faithfulness of the synthesised examples.\n6) Data Retrieval: Data retrieval is an efficient way to use\nexternal data to retrieve samples related to the current task from\na large-scale text corpus and concatenate them into the training\nset [9]. AugmentedSBERT [47] and zicl [48] expand the data\nvolume by incorporating data retrieved from the external source\nwith the original data. DAICL [61] combines each input from\nthe source domain with relevant retrieved texts from the target\ndomain to enrich the dataset. Then, the model learns task\ndiscrimination based on the source content and target texts.\nRather than incorporating the retrieved content with existing\ndata for training, more studies have incorporated relevant in-\nformation and ground truth retrieved from the external corpus\ninto LLM’s input context, thereby enhancing LLM’s factual\nresponse [64]. KAPING [62] and ALCE [63] employ the\nretrieved information as additional knowledge to guide the\nmodel to produce more accurate responses.\nAlso, recent researches explore utilising search engines [50]\nand various APIs [66], [8], [57] to conduct retrieval and access\nup-to-date knowledge from a more extensive source.\nC. Data Augmentation Granularity\nIn addition to data augmentation aspects, data augmenta-\ntion granularity significantly affects the augmented content. A\n",
  "page_6": "6\nTable II: Data Augmentation Granularity.\nMethods\nToken Level\nToken-span Level\nSentence Level\nPassage Level\nContext Level\nDocument Level\nSimple Augmentation\nTransformersDA [10]\n✓\n✓\nDAGAM [11]\n✓\n✓\nGenAug [12]\n✓\nAuGPT [13]\n✓\nCOCA [14]\n✓\n✓\nSelection-DA [15]\n✓\nLAMBADA [16]\n✓\n✓\nLeCA [17]\n✓\n✓\nG-DAUGc [18]\n✓\nMRC-QA [19]\n✓\nPrompt-based Augmentation\nGPT3Mix [20]\n✓\nDA-intent [21]\n✓\nWANLI [22]\n✓\nFlipDA [4]\n✓\n✓\nAugESC [23]\n✓\nAugGPT [2]\n✓\nRead- Com [24]\n✓\nDAIL [25]\n✓\nDA-NMT [26]\n✓\nEPA [27]\n✓\nZeroShotDataAug [28]\n✓\nDialogue-Convert [29]\n✓\nHiPSTG [30]\n✓\nSUNGEN [31]\n✓\nLLM-powered [1]\n✓\nLLM-PTM [32]\n✓\nGenerative-DA [33]\n✓\nICLEF [34]\n✓\nLLM-DA [35]\n✓\n✓\nSynthetic-DA [36]\n✓\nLLM–Assisted [3]\n✓\n✓\nLLM2LLM [37]\n✓\nPromptMix [38]\n✓\nUnnatural-instructions [39]\n✓\nGENIUS [5]\n✓\n✓\nTAPP [40]\n✓\nX-GEAR [41]\n✓\n✓\nInPars [42]\n✓\nConvAug [43]\n✓\n✓\nPromptagator [44]\n✓\nDAPDR [45]\n✓\nUDAPDR [46]\n✓\nRetrieval-based Augmentation\nAugmentedSBERT [47]\n✓\nzicl [48]\n✓\nRetGen [7]\n✓\nInternet-Aug [49]\n✓\nDialogGen [50]\n✓\nChatPLUG [51]\n✓\nEDGE [52]\n✓\nRGQA [53]\n✓\nCGRG [54]\n✓\nIM-RAG [55]\n✓\nEAE-RAG [56]\n✓\nSeeKeR [57]\n✓\nEfficient-RAG [58]\n✓\nLAPDOG [59]\n✓\nPersonae-DA [60]\n✓\nHybrid Augmentation\nDAICL [61]\n✓\nKAPING [62]\n✓\nALCE [63]\n✓\nRADA [64]\n✓\nUniMS-RAG [65]\n✓\nQA-Internet [8]\n✓\nReAct [66]\n✓\n",
  "page_7": "7\ncoarser granularity typically less perturbs the original data and\nretains more initial information, while a finer granularity is\nmore likely to produce more significant perturbations [9]. Dif-\nferent data augmentation granularity could control the created\ndata’s diversity and preserve the original data’s characteristics.\nDetailed information is depicted in Table II to present diverse\ngranularity.\n1) Token Level: Token level data augmentation is the most\nfine-grained granularity, mainly performed on single words.\nFlipDA [4] first combines text x and labels y into a sequence\nusing a cloze pattern [78] and then randomly masks a fixed per-\ncentage of the input tokens. DAGAM [11] proposes a character\norder change (COC) strategy to achieve token insertion, token\ndeletion and token replacement effects. RGQA [53] introduces\nthree new tokens to demonstrate each component of the input\nsequence in the prompt and instantiate all portions to construct\nthe final input sequence. ConvAug [43] treats a context as a\nsequence of tokens C and introduces two token level approaches\non C, which are Token Masking and Entity Replacing, to help\nthe model learn subtle information differences. Inspired by the\nword mask technique, COCA [14] proposes to mask terms\nrandomly over user behavioural sequences to prevent model\noverfitting and enhance the robustness of behaviour sentence\nrepresentations.\n2) Token-span Level: Compared with single token level\naugmentation, the token-span level involves a set of con-\ntinuous tokens that could obtain more semantic information\nbetween continuous tokens [79]. TransformersDA [10] applies\ntwo masking strategies to fine-tune BART: one is replacing\na single word with a mask token <mask>, and the other is\nmasking a continuous chunk of words with a single <mask>.\nMotivated by the outstanding performance of predicting masked\nwords [79] and contiguous spans [70] as self-supervised train-\ning objectives, Dialogue-Convert [29] concatenates gap text\nspans into a pseudo-summary in the pre-training stage for the\ngeneration task in the medical field. KAPING [62] proposes\nto retrieve triples that are question-only related. One triple\ncomprises (s, r, o), where s and o represent the subject and\nobject entities, respectively, and r is a specific relation type\nbetween them. LLM-DA [35] adopts entity-level augmentation\nto tackle the NER task, which is necessary to understand the\nsyntactic structure and semantic information of each token\nin the sentence to identify entities precisely. MRC-QA [19]\nfirstly conducts probability sampling on which answers need\nto be employed for data augmentation. Secondly, once the\nquestion-answer pairs are selected, a set of fuzzy answer spans\nare generated by shifting the correct answer span by certain\ncharacters to the left or right.\n3) Sentence Level: Sentence level is the most widely used\ngranularity in data augmentation, mainly focusing on the whole\nsentence, such as sentence generation and translation. Given\nan input sentence and its related entities, LLM-DA [35] aims\nto generate a collection of sentence variants. These variants\ninclude different aspects such as vocabulary usage, clauses,\nand expression styles. New sentences with rewritten context\nare obtained throughout the process while keeping the entities\nfrom the initial sentences. ZeroShotDataAug [28] leverages the\nLLM to produce a significant number of sentences for each\ntask in a zero-shot setting, improving the model’s performance\nin the low-resourced scenario. LAMBADA [16] fine-tunes the\nLLM to learn the training data patterns and produce synthesised\nsentences based on specific labels. Some studies paraphrase the\noriginal sentence to increase the training dataset’s diversity [2],\n[25], [26], [27].\nIn addition to common NLP tasks, in information retrieval\n(IR), InPars [42] and Promptagator [44] adopt query generation\nfor data augmentation and improve retrieval accuracy by gen-\nerating a query and retrieving documents related to the query\nin the document set. UDAPDR [46] and DAPDR [45] prompt\nan LLM to generate an enormous number of synthesis queries\nfor passages, improving retrieval accuracy.\nAn innovative approach is to employ external datasets to\nachieve data augmentation. zicl [48] and AugmentedSBERT\n[47] augment the limited dataset by retrieving sentences from\nan external corpus related to the training sentences. Then,\nconcatenate the retrieved and original sentences to expand the\ndataset.\n4) Passage Level: Passage level is a coarser level of data\naugmentation granularity. It could be adjusting the order of\npassages or inserting new content between passages. Given a\nquestion, ALCE [63] creates text while citing corresponding\nsupporting passages from a large-scale retrieval corpus. For\nthe event argument extraction task, X-GEAR [41] extracts\ninformation from the input passage to fill in the language-\nagnostic template and guides the model to create a target string\nbased on the language-agnostic template.\n5) Context Level: Context level data augmentation requires\nlanguage models to generate responses that demand an un-\nderstanding of the input context. The most common applica-\ntions are question-answering and conversation-generation tasks.\nSynthetic-DA [36] create more training data by providing LLM\nwith prompts to understand the text context. LLM2LLM [37]\ndevises different system prompts for each task, which enable\nusers to utilize domain-specific knowledge in the dataset gener-\nation process, and it leverages in-context learning to create more\nrelevant examples. Given a context, RADA [64] aims to gener-\nate an extracted question-answer pair and requires obtaining the\nanswer to the question from the provided context. Unnatural-\ninstructions [39] constructs the meta-prompt containing the in-\ncontext example’s number and each example’s constant. This\nmethod prompts the model to create an example based on the\nin-context demonstrations.\n6) Document Level: Document level data augmentation is\nthe coarsest-grained granularity, usually utilized for document\ngeneration and retrieval tasks. AugESC [23] introduces the im-\nplementation of the LLM to accomplish the entire dialogue for\ndiverse topics. HiPSTG [30] proposes a hierarchical discipline\nstructure to generate the whole proposal by comprehending the\nstructured prompt. COCA [14] proposes three augmentation\nstrategies, one of which is document level. This strategy utilizes\n",
  "page_8": "8\ncontrastive learning for query/document deletion to enhance the\nlearning of user behaviour sequence representations.\nAs for document retrieval tasks, SeeKeR [57] employs the\nsearch engine to retrieve relevant documents and then keep\nonly the top 5 documents to generate the knowledge response.\nRetGen [7] introduces a framework with a dense document re-\ntriever component to obtain the most significant documents and\na knowledge-grounded text generator component to produce the\ntarget output.\nIII. DATA AUGMENTATION TECHNIQUES FOR TEXT IN\nLLMS\nEarly traditional data augmentation methods include basic\ntext transformation [12], [11], [19], and Back-translation [13].\nThe arrival of large language models (LLMs) has given rise\nto an increasing trend of utilizing the power of generative\nmodels and prompts them to generate diverse datasets [80].\nTo address LLMs’ inevitable limitations, such as hallucina-\ntions, retrieving relevant facts from extensive external corpora\neffectively improves the generated response [81]. With the con-\ntinuous progress of research, combining up-to-date knowledge\nwith LLM’s prominent few-shot learning capability has jointly\npromoted the application of data augmentation in various tasks\n[64], [61], [63]. We classify data augmentation techniques into\nfour categories based on the critical technologies used in recent\nyears. Each technique has distinct characteristics, as Figure 3\nillustrates. We classify four DA techniques based on Prompt\nComplexity and Retrieval Model Complexity.\nSimple\nAugmentation is a conservative augmentation\nmethod that does not construct prompts or utilize retrieval\nmethods. The augmented data is only slightly modified based\non the original data and has a high similarity with the exist-\ning data [5]. Similar to Simple Augmentation, Prompt-based\nAugmentation does not employ a retrieval module, but the\ndifference is that it is a widely used technique during the\nflourishing period of prompt engineering, which opened up\nguiding large-scale language models to generate an extensive\nset of data by designing diverse prompts [20]. Based on the\nPrompt Complexity, we categorize the recent prompt-based\nstudies into basic Single-step Prompting (such as Zero-shot\nPrompting), Multi-step Prompting (such as Chain-of-Thought\nPrompting), and advanced Structured Prompting. In contrast,\nRetrieval-based Augmentation does not utilize LLM’s few-\nshot learning ability, but instead obtains external knowledge\nthrough a retrieval model. The Retrieval Model Complexity\nranges from Sparse Retrieval and Dense Retrieval in the early\ninformation retrieval stage [82], to more complex Graph-based\nRetrieval, and now with the emergence of RAG [81] in the\nera of LLMs, retrieval methods can be more personalized and\nadvanced through search engines. Hybrid Augmentation has\nexceptional few-shot learning ability which is well-known in\nthe prompt engineering era and includes a retrieval module that\ncould retrieve additional knowledge, although standard RAG\n[81] methods also have a prompt portion, we classify them as\nRetrieval-based Augmentation since it does not exhibit the few-\nshot learning capacity. The detailed data augmentation methods\nfor four techniques in recent studies are depicted in Figure 4.\nA. Simple Augmentation\nSimple Augmentation is an early data augmentation tech-\nnique, such as synonym replacement, word deletion [4] and\nback translation [77]. Many traditional data augmentation\nworks have applied large language models to create diverse\ndatasets and alleviate data scarcity issues.\n1) Text Transformation: A conventional approach for data\naugmentation is through simple text transformations [20].\nGenAug [12] explores multiple text augmentation approaches,\nincluding randomly swapping two words’ positions, deleting\none word, or performing character insertion. For keyword\nreplacement, this method uses RAKE [83] to extract keywords\nand incorporates external information through WordNet [84]\nby replacing keywords. The replaced keywords are sorted by\ntheir RAKE scores. One of DAGAM [11] data augmentation\nschemes is to adopt a character order change strategy. This\nstrategy involves fixing a word’s first and last characters and\nrandomly changing the remaining characters. MRC-QA [19]\ncreates augmented data by shifting the correct answer span a\nfew characters to the left or right. Each synthetic answer span\nis used as additional training data.\n2) Seed Selection Strategies: During the model training\nphase, using selected seed training data could enable the model\nto learn key features of the data [15]. Choosing a high-\nquality dataset for model training is crucial to reduce the\nnumber of training examples and lower computational costs\n[47]. Selection-DA [15] implements various seed selection\nstrategies and highlights the impact of the seed training samples\nselection process on the quality of synthetic data and classifier\nperformance. Further, it proposes a human-in-the-loop method\nfor selecting seed samples.\n3) Back-translation: Back-translation [77] is a typical data\naugmentation technique used in machine translation, which\ngenerates parallel training data by translating monolingual data\nfrom the target language back to the source language. AuGPT\n[13] utilizes back-translation to paraphrase the original data to\nincrease data diversity.\n4) Sequence-based Methods: More recently, a growing num-\nber of researches have employed pre-trained language models\nto obtain augmented datasets. COCA [14] randomly masks a\nfixed percentage of the input tokens to enhance the robustness\nof the user’s behaviour sentence representation and incorporates\nthe user’s behaviour information into the document ranking\nprocess. TransformersDA [10] conditions the pre-trained mod-\nels by prepending class labels in text sequences and produces\none synthesized example for every example in the training\nset. LAMBADA [16] fine-tunes GPT-2 [85] using existing\nlabelled data. It leverages a simple form, using [SEP] and\n[EOS] to concatenate multiple sentences. For any class of\nlabels y, use the fine-tuned language model to predict the\nduration of the sequence y [SEP] until [EOS], allowing\neach class to synthesize any number of sentences. For the\n",
  "page_9": "9\nFigure 3: Data augmentation techniques in the No Prompt-Basic-Advanced spectrum according to Prompt Complexity and No\nRetrieval-Basic-Advanced spectrum according to Retrieval Model Complexity.\nquestion-answering task, G-DAUGc [18] fine-tunes a language\nmodel on the training question set Qi to train the question\ngenerator, where Qi is the word sequence representing the\nith problem. Then, it generates new questions using the fine-\ntuned model and uses a similar method to create synthetic\nanswers and distractors to obtain an augmented dataset. LeCA\n[17] designs the input representation to distinguish between\nthe source sentence and each constraint and applies lexical\nconstraints to augment data in neural machine translation.\nB. Prompt-based Augmentation\nThe success of prompt engineering has dramatically stim-\nulated the capabilities of LLMs [86]. Providing LLMs with\ncarefully designed prompts can make LLMs produce more\nhuman-like responses. Prompt-based Augmentation technique\neffectively improve many downstream tasks’ performance by\nproviding task-related or cross-task prompts to guide LLMs in\ngenerating high-quality data.\n1) Single-step Prompts: Single-step Prompts only devise\none-step instructions. The most common form is a straight-\nforward question or a specific task requirement that does not\nrequire the model to perform multi-step reasoning.\nZero-shot Prompting LLMs have been adjusted to follow\ninstructions and are trained on the extensive corpus. Their\nenormous knowledge enables them to perform specific tasks in\na zero-shot manner [28]. DA-NMT [26] provides storytelling,\nparaphrasing and multi-target prompts without any examples\nto acquire synthetic parallel datasets by utilizing ChatGPT.\nZeroShotDataAug [28] employs LLMs to directly produce data\nbased on the prompt in a zero-shot setting.\nFor information retrieval (IR) tasks, DAPDR [45] creates\nrelevant queries for a given text document in a zero-shot\nsetting, then leverages IR models to acquire top-ranked can-\ndidate datasets for each synthetic query by calculating the\ncosine similarity between texts. UDAPDR [46] uses zero-shot\nprompting and also experiments with few-shot prompting to\nguide an LLM to create a set of synthesized queries. The\nqueries generated from each prompt are applied to train a\nseparate reranker, which is distilled into a single dense retriever\nfor the target domain.\nFew-shot Prompting Unlike Zero-shot Prompting, Few-\nshot Prompting provides specific examples or one example to\nprompt LLMs to generate desired responses [80]. AugGPT [2]\nuses ChatGPT [73] as a data augmentation tool in few-shot\nlearning. In particular, ChatGPT is employed to paraphrase\neach input sentence into a collection of extra sentences, thereby\nexpanding the training samples. EPA [27] and DAIL [25] apply\na one-shot example to prompt the LLM to rephrase the original\nexample and create new paraphrased examples. LLM–Assisted\n[3] leverages few-shot prompting to guide LLMs to paraphrase\nthe original sentences or content. DA-intent [21] addresses the\ndifficulty of obtaining a large set of example utterances with the\nsame intent by feeding an intent with available K-shot examples\nto an LLM.\nIn addition to common NLP tasks, many studies have ex-\n",
  "page_10": "10\nText Data Augmentation\nTechniques in LLMs\nSimple Augmentation\nText Transformation\nGenAug [12]; DAGAM [11]; MRC-QA [19]\nSeed Selection Strategies\nSelection-DA [15]\nBack-translation\nAuGPT [13]\nSequence-based Methods\nCOCA [14]; TransformersDA [10]; LAMBADA [16]; LeCA [17]; G-DAUGc [18]\nPrompt-based\nAugmentation\nSingle-step Prompts\nCloze Prompting\nFlipDA[4]; GENIUS[5]; SUNGEN[31]\nZero-shot Prompting\nDA-NMT[26]; ZeroShotDataAug[28]; UDAPDR[46]; DAPDR[45];\nGenerative-DA[33]; UniMS-RAG[65]\nFew-shot Prompting\nAugGPT[2]; EPA[27]; DAIL[25];Promptagator[44]; InPars[42];\nDA-intent[21]; LLM-Assisted[3]; Read-Com[24]\nDialogue-Convert[29]; UDAPDR[46]; Synthetic-DA[36];\nRADA[64]; DAICL[61]; QA-Internet[8]; ALCE[63]\nMulti-step Prompts\nChain-of-Thought\nPrompting\nLLM-PTM[32]; LLM–Assisted[3]; ConvAug[43];ReAct[66]\nStructured Prompts\nRole Prompting\nAugESC[23]; ICLEF[34]\nTuple Prompting\nGPT3Mix[20]; KAPING[62];\nTemplate Prompting\nHiPSTG[30]; LLM2LLM[37]; LLM-DA[35]; TAPP[40]; PromptMix[38];\nLLM-powered[1];WANLI[22]; Unnatural-instructions[39]; X-GEAR [41]\nRetrieval-based\nAugmentation\nSparse Retrieval\nBM25\nAugmentedSBERT [47]; ALCE [63]; UniMS-RAG [65]\nTF-IDF\nCGRG [54]; UniMS-RAG [65]\nDense Retrieval\nANCE\nRetGen [7]\nContriever\nLAPDOG [59]\nPoly-encoder\nEDGE [52]\nS-BERT\nEAE-RAG [56]; RGQA [53]\nRoBERTa\nEfficient-RAG [58]\nSimCSE\nzicl [48]; DAICL [61]\nDPR\nIM-RAG [55]; ALCE [63]\nTAS-B\nRADA [64]\nGTR\nALCE [63]\nGraph-based Retrieval\nPersonae-DA [60]; KAPING [62]\nSearch Engine Retrieval\nWikipedia Search\nDialogGen [50]\nAPIs\nChatPLUG [51]; Internet-Aug [49]; SeeKeR [57]; ReAct [66]; QA-Internet [8]\nHybrid Augmentation\nDAICL [61]; KAPING [62]; QA-Internet [8]; SeeKeR [57]; ALCE [63]; KAPING [62];\nRADA[64]; ReAct [66]; UniMS-RAG [65]\nFigure 4: Detailed data augmentation methods for four techniques. For a better understanding, the grey font means that the paper\nis from Hybrid Augmentation, and we could see from the figure how Hybrid Augmentation designs the prompt and performs\nthe retrieval.\nploited query generation in information retrieval by providing\nfew-shot examples for prompt-based LLMs to improve retrieval\nperformance significantly, such as InPars [42] provides three\nexamples of few-shot learning for the language model to gen-\nerate document-related queries, then acquires the most likely\noptimal documents for each query. Promptagator [44] directs\nLLMs with up to 8 examples in the prompt to produce a massive\nset of synthesized query-document pairs.\nCloze Prompting involves filling in blanks in a sentence\nor context and operates similarly to masked language models\nknown as BERT [68]. Masking part of the input text and\napplying the model to infer the masked part based on the\ncontext. FlipDA [4] forms a new sample by randomly masking\na fixed percentage of the input contents and then employing a\npre-trained language model to fill in the cloze blanks. SUN-\nGEN [31] leverages prompts that are composed of <mask>\nand <mask> position will be replaced with the label word.\nGENIUS [5] produces new samples based on the sketch of\ntraining samples. It fills in blanks [M] with several words or\nlong spans while keeping the main parts of the sketch in the\nsynthetic text, ensuring that the augmented text does not have\nsignificant semantic deviations from the initial text.\n",
  "page_11": "11\n2) Multi-step Prompts: To reduce the errors generated by\nthe language model, unlike Single-step Prompts that ask the\nmodel to produce a direct response, Multi-step Prompts direct\nthe generative model to provide a desired response step by step.\nChain-of-Thought\nPrompting (CoT Prompting) guides\nLLMs to reason step by step to solve complex tasks [80].\nA typical prompt contains a structure similar to let’s think\nstep by step. LLM-PTM [32] proposes implementing Chain-of-\nThought Prompting to direct LLMs in creating additional data\npoints while maintaining semantic consistency in the inclusion\nand exclusion criteria of the original trials. LLM–Assisted [3]\nemploys CoT Prompting to rewrite the source input content\nand gradually instruct LLMs for dependency tree generation.\nConvAug [43] introduces a three-step prompting approach\nmotivated by the theory of human cognition to reduce halluci-\nnations and improve data quality.\n3) Structured Prompts: Structured Prompts explicitly pro-\nvide the structure of the output content in the prompt template\nto adapt to more specific structured tasks. This ensures the\nmodel produces the expected content and complies with the\nstandard framework.\nRole Prompting assigns specific roles or personas to the\nmodel, devising its response style and content based on prede-\nfined characteristics. AugESC [23] uses a task description and\nrole prompts of Human and AI to distinguish between seekers\nand supporters. In the following dialogue completion process,\nthe model receives the task description and a starting phrase\nstarting with Human. Then, it is fed with the next AI. Finally,\ncreate the following dialogue. Inspired by LLM’s self-criticism\nability, ICLEF [34] provides ChatGPT with a small amount of\nexpert revision and asks it to behave as an annotator, criticizing\nits output results.\nTuple Prompting structures prompt as tuples or triples\nin a structured data pairs format to guide the model better\nunderstanding user’s intents [87]. GPT3Mix [20] uses the triple\nconsisted of Text Type T, Label Type L, and Label-token\nVerbalizer v: Y →V to create the task specification S = (T,\nL, v). Each task has a task description and is used to construct\nprompts.\nTemplate Prompting utilizes structured templates that guide\nthe model to respond according to a designed format or\ninstruction. LLM-DA [35] provides LLMs with given sen-\ntences and entities and requires LLMs to construct the ex-\npected format. After giving the definitions of age limit and\natm support, PromptMix [38] requires the LLMs to produce\nutterances composed of different proportions of the age limit\nand atm support. Unnatural-instructions [39] constructs well-\ndefined input formats and applies them to the article summa-\nrization task. LLM2LLM [37] gives detailed requirements in\nthe prompt and asks the model to return a structured response.\nOther studies, such as WANLI [22] and LLM-powered [1], also\nadopt a predefined template to instruct LLMs to generate new\ndata.\nC. Retrieval-based Augmentation\nAlthough LLMs have shown satisfactory capabilities in many\nfields, they inevitably suffer from producing hallucinations and\nbeing unable to use external information [8]. Retrieval-based\nAugmentation effectively overcomes some existing limitations\nof LLMs and provides an innovative way for data augmentation\nby retrieving enormous and dynamic knowledge from corpus\nbases or external documents [55]. Recently, many studies have\nused Retrieval-augmented Generation (RAG) [81] to obtain\nexternal timely updated information and achieved exceptional\nperformance on different tasks. Retrieval-based Augmentation\nmethods could be categorised into Sparse Retrieval, Dense\nRetrieval, Graph-based Retrieval and Search Engine Retrieval.\n1) Sparse Retrieval: Sparse Retrieval is a traditional infor-\nmation retrieval method that relies on explicit word matching in\ndocuments. The most commonly used models include TF-IDF\n[88] and BM25 [89].\nTF-IDF [88] computes text relevance by calculating the\nword’s frequency weights in the document and the whole corpus\n[82]. CGRG [54] employs IDF-weighted to rank grounding\ninformation from external knowledge sources to allow the\nmodel to create answers consistent with the facts. The main\nidea of using IDF-weighted is to measure word overlaps and\nselect words with high overlaps and IDF weights.\nBM25 [89] is an improved version of TF-IDF, which intro-\nduces a document length normalisation factor [82]. Augment-\nedSBERT [47] uses BM25 to sample similar sentences from\nunlabeled sentence pairs to form a silver dataset and then merge\nthe silver and gold datasets for prediction.\n2) Dense Retrieval: Dense Retrieval maps the query and\ndocuments into the same continuous vector space and calculates\nthe distance between the vectors to measure the similarity.\nCompared to Sparse Retrieval, Dense Retrieval is superior at\ncapturing semantic information [82].\nDPR [90] uses a dual-tower structure to encode queries\nand documents separately. It obtains semantic representation\nbased on BERT structure and optimises the model through\ncomparative learning [90]. IM-RAG [55] implements DPR to\nembed documents and then uses the FAISS library [91] for fast\nsemantic similarity retrieval.\nANCE [92] is a dense retrieval model that uses an asyn-\nchronously updated pool of negative samples for contrastive\nlearning, which improves the ability to distinguish negative\nsamples [92]. RetGen [7] adopts ANCE as the dense retrieval\nmodel and initialises the dense retriever with ANCE.\nSimCSE [93] is a sentence embedding model trained by\ncontrast learning. It improves the semantic representation of\nsentences by maximising the similarity of positive sample pairs\nand minimising the similarity of negative sample pairs using\nboth labelled and unlabelled data [93]. zicl [48] proposes to\nretrieve sentences with similar input distributions to the test\ninput by calculating the cosine similarity between two different\nsentence embeddings using SimCSE.\nPoly-encoder [94] captures the multilayer semantics of an\ninput text by using multiple fixed numbers of context vectors,\n",
  "page_12": "12\nTable III: Data Augmentation tasks, sub-tasks, and datasets.\nTasks\nSub-tasks\nDatasets\nMethods\nText Classification\nSentiment Classification\nCR, Yelp, Yelp5, Tweet-Eval, MR\nzicl [48]\nSST-2\nTransformersDA [10],GPT3Mix [20],LLM2LLM [37],\nDAIL [25], ZeroShotDataAug [28],\nSUNGEN [31],GENIUS [5], zicl [48]\nIMDb\nDAGAM [11],SUNGEN [31],GENIUS [5]\nAmazon\nAugGPT [2],SUNGEN [31], DAICL [61], zicl [48]\nTwitter Complaints\nPromptMix [38]\nIntent Classification\nSNIPS\nTransformersDA [10],DA-intent [21],\nLLM2LLM [37],ZeroShotDataAug [28]\nBanking\nDA-intent [21],PromptMix [38]\nQuestion Classification\nTREC\nTransformersDA [10],GPT3Mix [20],\nDAGAM [11],LLM2LLM [37], LAMBADA [16]\nZeroShotDataAug [28],PromptMix [38]\nQuora-QP\nAugmentedSBERT [47]\nTopic Classification\nAGnews\nDAGAM [11],DAIL [25],SUNGEN [31]\nYahoo\nDAIL [25],GENIUS [5]\nSubjectivity Classification\nSubjectivity\nSUNGEN [31],PromptMix [38]\nText Generation\nReviews Generation\nYelp Reviews\nGenAug [12]\nMachine Translation\nAI-hub corpus\nDA-NMT [26]\nnewstest2014\nLeCA [17]\nFLORES-200\nEPA [27]\nParaphrasing\nQuora Question Pairs (QQP)\nEPA [27]\nDialogue Generation\nReddit\nCGRG [54], RetGen [7]\nDailydialog, fraudulent e-mails\nEDGE [52]\nDuLeMon,KBP\nUniMS-RAG [65]\nWizard of the Internet\nInternet-Aug [49]\nWizard-of-Wikipedia\nDialogGen [50]\nCommon Document Corpus, Social Media Data,\nBenchmark Tasks\nChatPLUG [51]\nMultiWOZ 2.1\nEfficient-RAG [58]\nCONVAI2\nLAPDOG [59]\nESConv\nAugESC [23]\nMultiWOZ\nAuGPT [13]\nPersonaChat,Empathetic Dialogues,\nBlended Skill Talk,Multi-Session Chat\nSeeKeR [57]\nSchema Guided Dialog\nSynthetic-DA [36]\nDialogue Summarization\nMIMIC-III MTSamples\nDialogue-Convert [29]\nSAMSum\nEPA [27]\nInformation Extraction\nEvent Argument Extraction\nWikiEvents\nEAE-RAG [56], RGQA [53]\nACE 2005\nX-GEAR [41], RGQA [53]\nRAMS\nEAE-RAG [56]\nNamed Entity Recognition\nOntoNotes 5.0,MIT-Movie,FEW-NERD\nLLM-DA [35]\nCoNLL03\nGENIUS [5], LLM-DA [35], DAICL [61]\nWNUT, FIN, BC2GM , BioNLP09, BC5CDR\nDAICL [61]\nQuestion Answering\nSingle-hop\nTechQA\nMRC-QA [19], Read-Com [24], RADA [24]\nPolicyQA\nMRC-QA [19], Read-Com [24], RADA [24]\nCovidQA\nRead-Com [24], RADA [64]\nBoolQ\nFlipDA [4]\nSQUAD\nGenerative-DA [33], GENIUS [5]\nNewWiki,NYT,Amazon\nGenerative-DA [33]\nWebQuestionsSP\nKAPING [62]\nNatural Questions\nQA-Internet [8]\nMulti-hop QA\nASQA, QAMPARI, ELI5\nALCE [63]\nReddit\nGenerative-DA [33]\nMintaka\nKAPING [62]\nHotPotQA\nIM-RAG [55], ReAct [66], QA-Internet [8]\nSTRATEGYQA\nQA-Internet [8]\nMultiRC, ReCoRD\nFlipDA [4]\nNatural Language Inference\nMulti-task NLI\nMultiNLI\nWANLI [22], EPA [27]\nMultilingual Commonsense Reasoning\nXCOPA, XWinograd, XStoryCloze\nLLM-powered [1]\nMultiple-choice Reasoning\nGSM8K, CaseHOLD\nLLM2LLM [37]\nTextual Entailment\nSNLI\nEPA [27], G-DAUGc [18]\nWord Relation Reasoning\nSUPERNI\nTAPP [40]\nFact Verification\nFEVER\nQA-Internet [8]\nCommonsense Reasoning\nCOMMONSENSEQA, WINOGRANDE,\nCODAH and HellaSwag\nG-DAUGc [18]\nSemeval-2012\nFlipDA [4]\nInformation Retrieval\nMARCO, TREC-DL, Robust04,\nNatural Questions, TREC-COVID\nInPars [42]\nBEIR (exclude MS MARCO, NQ and Quora)\nPromptagator [44]\nQReCC, TopiOCQA, CAsT-20 and CAsT-21\nConvAug [43]\nNTCIR and ACODAR\nDAPDR [45]\nAOL search log and Tiangong-ST\nCOCA [14]\nse LoTTE, BEIR, NQ, and SQuAD\nUDAPDR [46]\nRegression\nSTS,BWS\nAugmentedSBERT [47]\n",
  "page_13": "13\nwhich can efficiently represent different information in long\ntexts [94]. EDGE [52] implements a poly-encoder model to\nperform retrieval and then choose the highest-ranked exemplar\nresponses.\nS-BERT [95] is a BERT-based model specialised for sen-\ntence embedding generation. It captures the semantic similarity\nbetween sentences by applying a dual-encoder architecture\n[95]. Compared with traditional BERT, S-BERT significantly\nimproves the efficiency of similarity computation tasks. EAE-\nRAG [56] leverages S-BERT to retrieve top-K ranked docu-\nments with the highest relevance to the original input document\nfrom the training corpus and uses the retrieved documents as\na set of discrete demonstrations. RGQA [53] first introduces\nthree additional tokens [demo], [tgr], and [sep_arg]\nto obtain distinct parts of the final input sequence. [demo]\nrepresents the most significant demonstration of the question\nand input examples. Then, it utilizes S-BERT and computes\nthe similarity scores to retrieve the demonstration.\nContriever [96] is a retrieval model pre-trained with un-\nsupervised data using a dual encoder architecture. It encodes\nquery and document independently and obtains the similarity\nbetween query and document usually by calculating the dot\nproduct of their corresponding outputs [96]. LAPDOG [59]\nimplements Contriever as the retriever to embed query and story\ncorpus separately. Then, the retriever computes the dot product\nsimilarity between the query and each story to obtain stories\nwith the top-K ranked similarity scores.\nRoBERTa [97] could be used as an encoder to encode\nqueries and documents. It converts text into vector represen-\ntations and calculates the similarity between these vectors\nto find the most relevant documents to the query from a\nlarge-scale document collection. Efficient-RAG [58] proposes\nto utilize a siamese network structure with two encoders to\nencode dialogue context and knowledge snippets separately and\nestablish a suitable ranking function using the distance between\nthem. It uses pre-trained RoBERTa [97] models as the encoders.\nTAS-B [98] combines BERT dense representation and sparse\nfeatures to optimise long text retrieval through multi-level\nsemantic aggregation and contrastive learning [98]. Detailed\nmethods will be introduced in Section III-D2.\nGTR [99] is a dense retrieval method based on the T5\n[70]. It converts queries and documents into dense vectors and\nidentifies candidate documents by calculating the similarity to\nachieve efficient information retrieval [99]. In Section III-D2,\nwe will introduce how the recent paper uses GTR for retrieval.\n3) Graph-based Retrieval: Graph-based Retrieval typically\nutilizes graph structures to retrieve documents and queries. To\nimprove LLM’s performance, Personae-DA [60] introduces a\nnew memory framework in the information retrieval model\nto reflect how humans dynamically access memory during\ninteractive processes and use a memory model with an adaptive\ngraph-based architecture to obtain relevant retrieved data.\n4) Search Engine Retrieval: Search Engine Retrieval is a\ntechnique for retrieval-related tasks through external knowledge\nbases. The model’s output may be out of control or inaccurate\nwithout providing external knowledge [54]. Retrieving external\ndocuments and injecting grounding knowledge into the model\nhelps the model generate faithful and context-aware responses.\nWikipedia Search is a system designed for searching within\nthe Wikipedia database. DialogGen [50] employs the query\ngenerator to generate queries based on the conversation context.\nThis method then implements Wikipedia Search 1 as the search\nengine to acquire significant articles. The response generator\ngenerates responses based on the retrieved articles and the\nconversation context.\nAPIs return search results by calling interfaces provided\nby a particular platform or service (e.g. Google, Bing, and\nWikipedia). ChatPLUG [51] accesses the internet-relevant\nknowledge by Quark Search API 2 based on the conversation\ncontext. Internet-Aug [49] first employs the Bing Search API\nto generate a URL list for each query, then retrieves up-to-\nthe-minute relevant information based on these URLs, and the\nretrieved knowledge is prepended to the conversation history.\nReAct [66] performs retrieval by the Wikipedia web API to\nsearch for an entity and return the first five sentences of\nthe corresponding entity’s wiki page if it exists. Given the\nsynthesised query, SeeKeR [57] adopts the Bing Web Search\nAPI 3 to retrieve documents. The retrieved document is used to\nproduce a knowledge response composed of relevant sentences\nor phrases.\nD. Hybrid Augmentation\nThe combination of constructing prompts and retrieval com-\nponents not only stimulates LLMs to produce diverse datasets\nbut also exploits retrievers to obtain timely updated information.\nHybrid Augmentation technique is shown in a grey font in\nFigure 4. There could be many different combinations to\nconstruct prompts and perform retrieval. The following will\nintroduce the various combinations of the papers covered in\nthis survey.\n1) Single-step Prompts and Sparse Retrieval: UniMS-RAG\n[65] adopts existing methods such as TF-IDF and BM25 to\nobtain similarity scores between dialogue context and evidence.\nMotivated by recent studies that use LLMs to predict similarity\nrelationships between queries and evidence, it provides dialogue\ncontext and multiple pieces of evidence to prompt ChatGPT\n[73] to predict similarity scores in a zero-shot setting. ALCE\n[63] employs BM25 to retrieve relevant passages from an exten-\nsive collection of documents and returns candidate passages. It\nconstructs two in-context demonstrations in input prompts and\napplies them to an LLM to interact with the sparse retriever.\n2) Single-step Prompts and Dense Retrieval: For the senti-\nment analysis task, DAICL [61] leverages SimCSE Roberta as\nthe retrieval model to retrieve examples semantically similar to\nthe input sentence from the target unlabeled corpus as the con-\ntext of the source query. Then, it concatenates the source query\nand the retrieved context as input few-shot demonstrations\nfor domain-adaptive in-context learning. To acquire examples\n1https://en.wikipedia.org/wiki/Special:Search\n2https://quark.sm.cn/\n3www.microsoft.com/en-us/bing/apis/bing-web-search-api\n",
  "page_14": "14\nsimilar to the original samples, RADA [64] implements Distil-\nBert TAS-B to retrieve examples from external resources and\nleverages the retrieved samples to create new input-output pairs\nin the few-shot in-context setting. Given a question, ALCE [63]\nfirst employs GTR [99] and DPR [90] to generate content while\nproviding related cited passages from a vast retrieval corpus.\nFollowed by prompting an LLM to create and cite related\nevidence without fine-tuning the model’s internal parameters.\n3) Single-step Prompts and Search Engine Retrieval: QA-\nInternet [8] applies the question verbatim as a query and\nleverages Google Search API 4 to extend knowledge of the\nmodel by retrieving a set of documents for each query. Followed\nby adopting k-shot prompting and the retrieved paragraphs for\nquestion-answering scenarios.\n4) Multi-step Prompts and Search Engine Retrieval: ReAct\n[66] randomly selects cases from the training set and manually\nconstructs trajectories that compose diverse thought-action-\nobservation processes. It performs retrieval by the Wikipedia\nweb API to search for an entity and return the first five\nsentences of the corresponding entity’s wiki page if it exists.\n5) Structured Prompts and Graph-based Retrieval: KAP-\nING [62] uses MPNet [100] as the symmetric retriever and\napplies TAS-B [98] as the asymmetric retriever to retrieve only\nthe related facts from the external Knowledge Graph. Then, it\naugments them to the prompt composed of relevant fact triples\nto produce a grounded response.\nIV. POST-PROCESSING APPROACHES\nThe augmented data by LLMs is not fully guaranteed to be\nvalid for training, and it is crucial to refine the generated data\nand further ensure data quality and relevance [1]. We catego-\nrize the post-processing approaches into consistency measures,\nfiltering techniques, heuristic methods, and human involvement.\nA. Consistency Measures\nConsistency measures ensure that the generated data is\nlogically and semantically consistent with the original data.\nLAMBADA [16] ranks the generated sentences for the same\nlabel by calculating the confidence score and keeping the top-\nranked sentences. After developing a set of new examples,\nLLM-powered [1] performs post-processing by only adding\nefficient and consistent output samples to the training set.\nPromptagator [44] and Generative-DA [33] clean the synthetic\ndata by employing round-trip consistency. Many studies intro-\nduce multiple reranking strategies to refine the response. ALCE\n[63] utilizes the citation recall store as a rerank strategy to\nfurther improve the output’s quality. Aiming to get the top-\nranked supporting knowledge, IM-RAG [55] leverages a refiner\nto retain significant information of the retriever’s output.\n4https://developers.google.com/custom-search\nB. Filtering Techniques\nFiltering techniques are used to retain generated data that is\nhelpful to the model and to filter redundant or invalid data. G-\nDAUGc [18] applies influence functions to filter out detrimental\ntraining examples. LLM-DA [35], Unnatural-instructions [39]\nand RGQA [53] remove non-qualified augmented content by\nimplementing specific filtering metrics. To select high-quality\nsynthetic queries, UDAPDR [46] applies a filter to refine\ngenerated queries that could return its gold passage among the\nfirst 20 of the retrieved results. To further enhance the quality of\ngenerated data, Read-Com [24] harnesses a round-trip filtration\nstrategy. LLM2LLM [37] and DAPDR [45] harness similarity-\nbased filtering approaches, which specifically use ROUGE filter\nand compute cosine similarity, respectively. Aiming to ensure\nthe LLM could generate examples for the majority class,\nPromptMix [38] selects top-5 classes based on the similarity\nbetween the synthetic examples and current examples. MRC-\nQA [19] only retains the documents related to answer spans\nthat obtain the top-K highest scores. To reduce the number of\nretrieved factual triples, KAPING [62] only keeps the top-K\nrelevant triples to the question.\nC. Heuristic Methods\nHeuristic methods avoid common problems by adopting\nspecific rules or principles. AugESC [23] discards undesirable\ngenerated content based on designed heuristics in the last post-\nprocessing stage. WANLI [22] firstly filters failure examples\nby applying heuristics and then calculates the estimated max\nvariability for the rest of the examples.\nD. Human Involvement\nHuman revision plays a critical role in refining augmented\ndatasets. WANLI [22] recruits crowd-workers to review unla-\nbeled examples and remove offensive examples.\nThe practical application of post-processing approaches im-\nproves the quality of the augmented datasets and enhances the\noverall performance of language models [46], making them\nmore robust and reliable.\nV. TASKS AND EVALUATION\nA. Tasks\nMore recently, the parameters of deep neural network models\nhave significantly increased, and the quantity and quality of\ndata have become an essential role in the model’s training\nprocess and received significant attention. Large-scale language\nmodels alleviate data scarcity by generating more augmented\ndatasets with similar distribution to the original data [72].\nVarious data augmentation techniques extend existing text data\nand significantly improve the performance of natural language\nprocessing (NLP) tasks. Common NLP tasks include Informa-\ntion Extraction, Question Answering, and Text Classification\n[101].\n",
  "page_15": "15\nTable IV: Evaluation Metrics of four Data Augmentation Techniques.\nDA techniques\nEvaluation\nMetrics\nMethods\nSimple Augmentation\nAutomatic Evaluation\nAccuracy\nDAGAM [11], MRC-QA [19]\nExact Match score\nMRC-QA [19]\nRecall\nAuGPT [13], MRC-QA [19]\nF1 Score\nAuGPT [13], Selection-DA [15], MRC-QA [19]\nPerplexity\nGenAug [12]\nBLEU\nGenAug [12], AuGPT [13], LeCA [17]\nPrompt-based Augmentation\nAutomatic Evaluation\nAccuracy\nWANLI [22],LLM2LLM [37],FlipDA [4],GPT3Mix [20],DA-intent [21]\nDAIL [25],EPA [27],SUNGEN [31],PromptMix [38]\nROUGE\nDialogue-Convert [29],Synthetic-DA [36],EPA [27],TAPP [40]\nExact Match score\nRead-Com [24],TAPP [40],Generative-DA [33]\nRecall\nDA-intent [21],GENIUS [5]\nF1 Score\nDialogue-Convert [29],LLM-DA [35],X-GEAR [41],Read-Com [24]\nGenerative-DA [33],EPA [27],FlipDA [4]\nPerplexity\nGENIUS [5]\nBLEU\nDA-NMT [26],EPA [27]\nHuman Evaluation\nConsistency\nAugESC [23]\nCoherence\nInformativeness\nSafety\nRetrieval-based Augmentation\nAutomatic Evaluation\nAccuracy\nzicl [48]\nROUGE\nLAPDOG [59]\nExact Match score\nIM-RAG[55]\nRecall\nDialogGen [50]\nF1 Score\nEAE-RAG [56],X-GEAR [41],IM-RAG [55],AugmentedSBERT [47]\nInternet-Aug [49],DialogGen [50],Efficient-RAG [58],LAPDOG [59]\nPerplexity\nInternet-Aug [49],DialogGen [50]\nBLEU\nRetGen [7],Efficient-RAG [58],LAPDOG [59]\nHuman Evaluation\nConsistency\nSeeKeR [57]\nCoherence\nChatPLUG [51],RetGen [7]\nInformativeness\nChatPLUG [51],RetGen [7]\nSafety\nChatPLUG [51]\nHallucination\nChatPLUG [51]\nKnowledgeable\nInternet-Aug [49], SeeKeR [57]\nEngaging\nInternet-Aug [49]\nHybrid Augmentation\nAutomatic Evaluation\nAccuracy\nKAPING [62],ReAct[66],QA-Internet [8],DAICL [61]\nROUGE\nUniMS-RAG[65]\nExact Match score\nRGQA [53],ReAct [66],QA-Internet [8]\nRecall\nRGQA [53],ConvAug [43]\nF1 Score\nRGQA [53],RADA [64],DAICL [61],UniMS-RAG [65]\nBLEU\nCGRG [54],UniMS-RAG [65]\nHuman Evaluation\nConsistency\nSeeKeR [57]\nCoherence\nUniMS-RAG [65]\nFluency\nALCE [63]\nFactually Correct\nALCE [63]\nIn addition to typical NLP tasks, LLMs have been employed\nto improve the accuracy and new domain adaptability of infor-\nmation retrieval (IR) tasks [46]. In information retrieval, given a\nquery, candidate documents related to the query are retrieved in\nthe document set. A widely used method in the context of LLMs\nfor IR tasks is query generation for data augmentation, which\napplies LLMs to generate queries or query-document pairs [45].\nThe synthetic queries could help the retrieval model adapt to\ntargeted domains in multiple tasks. InPars [42], Promptagator\n[44], UDAPDR [46] and DAPDR [45] improve the performance\nof passage and document retrieval tasks by enhancing queries.\nBesides, COCA [14] and ConvAug [43] utilize contrastive\nlearning to obtain augmented data in document ranking and\nconversational dense retrieval tasks, respectively.\nMoreover, With the powerful generation capability of LLMs,\nincorporating a retrieval module to retrieve relevant information\nfrom large-scale corpora demonstrates excellent superiority in\nthe question answering (QA) and dialogue generation tasks\n[81].\nThe broad application of data augmentation techniques ef-\nfectively alleviates data scarcity issues and promotes the break-\nthrough and development of LLMs in diverse NLP tasks. Table\nIII shows the popular tasks of data augmentation in recent\nresearch.\nB. Evaluation Metrics\nAutomatic and human evaluation are two main perspec-\ntives on a model’s performance. Automatic evaluation is more\nsuitable when the task contains a numerous dataset. Typical\nautomatic evaluation includes accuracy [22], [37], [4], exact\nmatch score [33], [19] and F1 Score [35], [41], [24].\nIn contrast to automatic evaluation, human evaluation could\ndeeply analyse the model’s output through the involvement of\ncrowd workers or experts. Common human evaluation criteria\n",
  "page_16": "16\ninclude consistency [23], [65], [54], [57], coherence [65], [51]\nand informativeness [23], [51]. Human evaluation could provide\na more comprehensive understanding of the model’s perfor-\nmance, but it could be time-consuming and labour-intensive on\nmassive datasets. The detailed evaluation of data augmentation\nstudies is illustrated in table IV.\nVI. CHALLENGES AND OPPORTUNITIES\nAlthough data augmentation has showcased excellent perfor-\nmance in many studies and tasks, it still faces challenges and\nlimitations. This chapter underscores some directions that could\nbe investigated in greater depth for future research.\nA. The Quality and Diversity of Generated Data\nThe prerequisite for effective data augmentation is the valid-\nity of the generated data. Even though generating semantically\nrich data that is highly relevant to seed training data [30], [31],\n[1] and employing various filtering strategies to filter out repet-\nitive and irrelevant content generated by LLMs could address\nthe limitations of the low-quality augmented datasets [18], [35],\n[53]. However, there is no exact solution to determine how\nmuch synthetic data is efficient and how to ensure that the\nsynthesised data is helpful to improve the performance of the\nmodel [44].\nB. Tasks Adaptation\nCurrent experiments mainly focused on a single task, such\nas classification tasks. Inputs and test inputs need more flexible\noptions for sharing between diverse tasks [48]. Specifically, the\nissues of adapting the model to different tasks and correctly\nevaluating the model’s output on distinct tasks are unresolved\n[56].\nC. Reducing Hallucination\nLLMs’ transformative generative ability has brought enor-\nmous benefits to both academic and industrial, but LLMs\nunavoidably produce factually incorrect responses and con-\ntradictory content [57]. Many studies propose to mitigate the\nhallucination by retrieving external and up-to-date knowledge\nand improving the accuracy of the generated content by retriev-\ning relevant articles and providing citations for LLMs [63].\nAnother popular approach is to incorporate grounded truths\nthrough multiple APIs [66], [57], [51], search engines [50],\nand modulars [57], thereby alleviating hallucination.\nD. Retrieval Dependencies\nDuring the retrieval process, the model’s performance de-\npends to some extent on the quality of the retrieved data and\nthe relevance between external and existing data [64]. If the\nretriever fails to retrieve related grounded facts, the model may\nproduce unfaithful answers [62], [64].\nE. Large Number of Parameters and High Training Cost\nLLMs have numerous training parameters, which take up\na significant amount of GPU resources during the training\nprocess. The inference procedure also requires high computa-\ntional and storage costs [53], [64]. Future work could explore\ninnovative methods to better transfer knowledge from large-\nscale language models to smaller language models. Investigate\nhow to effectively integrate various domain knowledge into the\nmodel to improve its adaptability to complex tasks rather than\nincreasing the model parameters.\nF. Ethics and Potential Risks\nLLMs may pose uncontrolled risks [54], as their synthesised\ncontent may contain sensitive and private information [43]. In\naddition, LLMs have inherited biases regarding specific topics,\nwhich may produce harmful content [46].\nApplying text data augmentation techniques in large lan-\nguage models still faces unresolved challenges. Future research\ncould focus on optimising the quality of the generated data,\nimproving the model’s adaptability to different tasks, and\nreducing the training cost to further promote the effectiveness\nand development of data augmentation.\nVII. CONCLUSION\nThis survey classifies text data augmentation into four tech-\nniques: Simple Augmentation, Prompt-based Augmentation,\nRetrieval-based Augmentation, and Hybrid Augmentation. Each\ntechnique is further categorised according to its unique charac-\nteristics. Providing crafted prompt templates to large language\nmodels has shown prominent performance in data augmen-\ntation. Combining external data with existing data through a\nretriever offers more possibilities for cross-domain tasks. Data\naugmentation indeed contributes to expanding the dataset and\ngenerating diverse content, but more techniques are needed\nto test the validity and factuality of the generated data. The\ncontinuous development and improvement of LLMs makes\nthem increasingly effective in data augmentation and deserves\ncontinued exploration in the future.\nREFERENCES\n[1] C. Whitehouse, M. Choudhury, and A. F. Aji, “Llm-powered data\naugmentation for enhanced cross-lingual performance,” in Proceedings\nof the 2023 Conference on Empirical Methods in Natural Language Pro-\ncessing, EMNLP 2023, Singapore, December 6-10, 2023.\nAssociation\nfor Computational Linguistics, 2023, pp. 671–686.\n[2] H. Dai, Z. Liu, W. Liao, X. Huang, Y. Cao, Z. Wu, L. Zhao, S. Xu,\nW. Liu, N. Liu et al., “Auggpt: Leveraging chatgpt for text data\naugmentation,” arXiv preprint arXiv:2302.13007, 2023.\n[3] M. Zhang, G. Jiang, S. Liu, J. Chen, and M. Zhang, “LLM–Assisted\nData Augmentation for Chinese Dialogue–Level Dependency Parsing,”\n2024.\n[4] J. Zhou, Y. Zheng, J. Tang, L. Jian, and Z. Yang, “Flipda: Effective and\nrobust data augmentation for few-shot learning,” in Proceedings of the\n60th Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022.\nAssociation for Computational Linguistics, 2022, pp. 8646–8665.\n",
  "page_17": "17\n[5] B. Guo, Y. Gong, Y. Shen, S. Han, H. Huang, N. Duan, and W. Chen,\n“GENIUS: sketch-based language model pre-training via extreme and\nselective masking for text generation and augmentation,” CoRR, vol.\nabs/2211.10330, 2022.\n[6] A. Patel, B. Li, M. S. Rasooli, N. Constant, C. Raffel, and C. Callison-\nBurch, “Bidirectional language models are also few-shot learners,” in\nThe Eleventh International Conference on Learning Representations,\nICLR 2023, Kigali, Rwanda, May 1-5, 2023.\nOpenReview.net, 2023.\n[7] Y. Zhang, S. Sun, X. Gao, Y. Fang, C. Brockett, M. Galley, J. Gao,\nand B. Dolan, “Retgen: A joint framework for retrieval and grounded\ntext generation modeling,” in Thirty-Sixth AAAI Conference on Artificial\nIntelligence, AAAI 2022, Thirty-Fourth Conference on Innovative Appli-\ncations of Artificial Intelligence, IAAI 2022, The Twelveth Symposium\non Educational Advances in Artificial Intelligence, EAAI 2022 Virtual\nEvent, February 22 - March 1, 2022.\nAAAI Press, 2022, pp. 11 739–\n11 747.\n[8] A. Lazaridou, E. Gribovskaya, W. Stokowiec, and N. Grigorev, “Internet-\naugmented language models through few-shot prompting for open-\ndomain question answering,” CoRR, vol. abs/2203.05115, 2022.\n[9] Y. Gao, Y. Xiong, X. Gao, K. Jia, J. Pan, Y. Bi, Y. Dai, J. Sun, M. Wang,\nand H. Wang, “Retrieval-Augmented Generation for Large Language\nModels: A Survey,” Mar. 2024.\n[10] V. Kumar, A. Choudhary, and E. Cho, “Data augmentation using pre-\ntrained transformer models,” CoRR, vol. abs/2003.02245, 2020.\n[11] B. Jo, T. Heo, Y. Park, Y. Yoo, W. Cho, and K. Kim, “DAGAM:\ndata augmentation with generation and modification,” CoRR, vol.\nabs/2204.02633, 2022.\n[12] S. Y. Feng, V. Gangal, D. Kang, T. Mitamura, and E. H. Hovy,\n“Genaug: Data augmentation for finetuning text generators,” CoRR, vol.\nabs/2010.01794, 2020.\n[13] J. Kulh´anek, V. Hudeˇcek, T. Nekvinda, and O. Duˇsek, “Augpt: Auxiliary\ntasks and data augmentation for end-to-end dialogue with pre-trained\nlanguage models,” arXiv preprint arXiv:2102.05126, 2021.\n[14] Y. Zhu, J. Nie, Z. Dou, Z. Ma, X. Zhang, P. Du, X. Zuo, and\nH. Jiang, “Contrastive learning of user behavior sequence for context-\naware document ranking,” in CIKM ’21: The 30th ACM International\nConference on Information and Knowledge Management, Virtual Event,\nQueensland, Australia, November 1 - 5, 2021.\nACM, 2021, pp. 2780–\n2791.\n[15] A. Edwards, A. Ushio, J. Camacho-Collados, H. de Ribaupierre, and\nA. D. Preece, “Guiding generative language models for data augmenta-\ntion in few-shot text classification,” CoRR, vol. abs/2111.09064, 2021.\n[16] A. Anaby-Tavor, B. Carmeli, E. Goldbraich, A. Kantor, G. Kour,\nS. Shlomov, N. Tepper, and N. Zwerdling, “Do not have enough data?\ndeep learning to the rescue!” in The Thirty-Fourth AAAI Conference\non Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative\nApplications of Artificial Intelligence Conference, IAAI 2020, The Tenth\nAAAI Symposium on Educational Advances in Artificial Intelligence,\nEAAI 2020, New York, NY, USA, February 7-12, 2020.\nAAAI Press,\n2020, pp. 7383–7390.\n[17] G. Chen, Y. Chen, Y. Wang, and V. O. K. Li, “Lexical-constraint-aware\nneural machine translation via data augmentation,” in Proceedings of the\nTwenty-Ninth International Joint Conference on Artificial Intelligence,\nIJCAI 2020.\nijcai.org, 2020, pp. 3587–3593.\n[18] Y. Yang, C. Malaviya, J. Fernandez, S. Swayamdipta, R. L. Bras,\nJ. Wang, C. Bhagavatula, Y. Choi, and D. Downey, “G-daug: Generative\ndata augmentation for commonsense reasoning,” in Findings of the\nAssociation for Computational Linguistics: EMNLP 2020, Online Event,\n16-20 November 2020, ser. Findings of ACL, vol. EMNLP 2020.\nAssociation for Computational Linguistics, 2020, pp. 1008–1025.\n[19] H. Van, V. Yadav, and M. Surdeanu, “Cheap and good? simple and\neffective data augmentation for low resource machine reading,” in SIGIR\n’21: The 44th International ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval, Virtual Event, Canada, July 11-\n15, 2021.\nACM, 2021, pp. 2116–2120.\n[20] K. M. Yoo, D. Park, J. Kang, S. Lee, and W. Park, “Gpt3mix: Leveraging\nlarge-scale language models for text augmentation,” in Findings of the\nAssociation for Computational Linguistics: EMNLP 2021, Virtual Event\n/ Punta Cana, Dominican Republic, 16-20 November, 2021. Association\nfor Computational Linguistics, 2021, pp. 2225–2239.\n[21] G. Sahu, P. Rodr´ıguez, I. H. Laradji, P. Atighehchian, D. V´azquez, and\nD. Bahdanau, “Data augmentation for intent classification with off-the-\nshelf large language models,” in Proceedings of the 4th Workshop on\nNLP for Conversational AI, ConvAI@ACL 2022, Dublin, Ireland, May\n27, 2022.\nAssociation for Computational Linguistics, 2022, pp. 47–57.\n[22] A. Liu, S. Swayamdipta, N. A. Smith, and Y. Choi, “WANLI: worker\nand AI collaboration for natural language inference dataset creation,”\nin Findings of the Association for Computational Linguistics: EMNLP\n2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022.\nAs-\nsociation for Computational Linguistics, 2022, pp. 6826–6847.\n[23] C. Zheng, S. Sabour, J. Wen, Z. Zhang, and M. Huang, “Augesc:\nDialogue augmentation with large language models for emotional sup-\nport conversation,” in Findings of the Association for Computational\nLinguistics: ACL 2023, Toronto, Canada, July 9-14, 2023, A. Rogers,\nJ. L. Boyd-Graber, and N. Okazaki, Eds. Association for Computational\nLinguistics, 2023, pp. 1552–1568.\n[24] V. Samuel, H. Aynaou, A. G. Chowdhury, K. V. Ramanan, and\nA. Chadha, “Can llms augment low-resource reading comprehension\ndatasets? opportunities and challenges,” in Proceedings of the 62nd\nAnnual Meeting of the Association for Computational Linguistics, ACL\n2024 - Student Research Workshop, Bangkok, Thailand, August 11-16,\n2024.\nAssociation for Computational Linguistics, 2024, pp. 411–421.\n[25] D. Li, Y. Li, D. Mekala, S. Li, Y. Wang, X. Wang, W. Hogan, and\nJ. Shang, “DAIL: data augmentation for in-context learning via self-\nparaphrase,” CoRR, vol. abs/2311.03319, 2023.\n[26] S. Oh, S. A. Lee, and W. Jung, “Data augmentation for neural\nmachine translation using generative language model,” CoRR, vol.\nabs/2307.16833, 2023.\n[27] H. Lu and W. Lam, “EPA: easy prompt augmentation on large lan-\nguage models via multiple sources and multiple targets,” CoRR, vol.\nabs/2309.04725, 2023.\n[28] S. Ubani, S. O. Polat, and R. Nielsen, “Zeroshotdataaug: Generating\nand augmenting training data with chatgpt,” CoRR, vol. abs/2304.14334,\n2023.\n[29] V. Schlegel, H. Li, Y. Wu, A. Subramanian, T. Nguyen, A. R. Kashyap,\nD. Beck, X. Zeng, R. T. Batista-Navarro, S. Winkler, and G. Nenadic,\n“PULSAR at mediqa-sum 2023: Large language models augmented by\nsynthetic dialogue convert patient dialogues to medical records,” in\nWorking Notes of the Conference and Labs of the Evaluation Forum\n(CLEF 2023), Thessaloniki, Greece, September 18th to 21st, 2023, ser.\nCEUR Workshop Proceedings, vol. 3497.\nCEUR-WS.org, 2023, pp.\n1668–1679.\n[30] X. Cai, M. Xiao, Z. Ning, and Y. Zhou, “Resolving the imbalance\nissue in hierarchical disciplinary topic inference via llm-based data\naugmentation,” in IEEE International Conference on Data Mining,\nICDM 2023, Shanghai, China, December 1-4, 2023.\nIEEE, 2023, pp.\n956–961.\n[31] J. Gao, R. Pi, Y. Lin, H. Xu, J. Ye, Z. Wu, W. Zhang, X. Liang, Z. Li,\nand L. Kong, “Self-guided noise-free data generation for efficient zero-\nshot learning,” in The Eleventh International Conference on Learning\nRepresentations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023.\nOpen-\nReview.net, 2023.\n[32] J. Yuan, R. Tang, X. Jiang, and X. Hu, “Large Language Models for\nHealthcare Data Augmentation: An Example on Patient-Trial Matching,”\nAug. 2023.\n[33] A. G. Chowdhury and A. Chadha, “Generative data augmentation\nusing llms improves distributional robustness in question answering,” in\nProceedings of the 18th Conference of the European Chapter of the As-\nsociation for Computational Linguistics, EACL 2024: Student Research\nWorkshop, St. Julian’s, Malta, March 21-22, 2024.\nAssociation for\nComputational Linguistics, 2024, pp. 258–265.\n[34] A. Saakyan and S. Muresan, “ICLEF: in-context learning with expert\nfeedback for explainable style transfer,” in Proceedings of the 62nd An-\nnual Meeting of the Association for Computational Linguistics (Volume\n1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024.\nAssociation for Computational Linguistics, 2024, pp. 16 141–16 163.\n[35] J. Ye, N. Xu, Y. Wang, J. Zhou, Q. Zhang, T. Gui, and X. Huang, “LLM-\nDA: data augmentation via large language models for few-shot named\nentity recognition,” CoRR, vol. abs/2402.14568, 2024.\n[36] J.\nKaddour\nand\nQ.\nLiu,\n“Synthetic\nData\nGeneration\nin\nLow-\nResource Settings via Fine-Tuning of Large Language Models,” no.\narXiv:2310.01119, Jan. 2024.\n[37] N. Lee, T. Wattanawong, S. Kim, K. Mangalam, S. Shen, G. Anu-\nmanchipalli, M. W. Mahoney, K. Keutzer, and A. Gholami, “LLM2LLM:\nboosting llms with novel iterative data enhancement,” in Findings of\nthe Association for Computational Linguistics, ACL 2024, Bangkok,\n",
  "page_18": "18\nThailand and virtual meeting, August 11-16, 2024.\nAssociation for\nComputational Linguistics, 2024, pp. 6498–6526.\n[38] G. Sahu, O. Vechtomova, D. Bahdanau, and I. H. Laradji, “Promptmix:\nA class boundary augmentation method for large language model distil-\nlation,” in Proceedings of the 2023 Conference on Empirical Methods\nin Natural Language Processing, EMNLP 2023, Singapore, December\n6-10, 2023. Association for Computational Linguistics, 2023, pp. 5316–\n5327.\n[39] O. Honovich, T. Scialom, O. Levy, and T. Schick, “Unnatural in-\nstructions: Tuning language models with (almost) no human labor,”\nin Proceedings of the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto,\nCanada, July 9-14, 2023.\nAssociation for Computational Linguistics,\n2023, pp. 14 409–14 428.\n[40] S. Ye, H. Hwang, S. Yang, H. Yun, Y. Kim, and M. Seo, “Investi-\ngating the effectiveness of task-agnostic prefix prompt for instruction\nfollowing,” in Thirty-Eighth AAAI Conference on Artificial Intelligence,\nAAAI 2024, Thirty-Sixth Conference on Innovative Applications of\nArtificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational\nAdvances in Artificial Intelligence, EAAI 2014, February 20-27, 2024,\nVancouver, Canada.\nAAAI Press, 2024, pp. 19 386–19 394.\n[41] K. Huang, I. Hsu, P. Natarajan, K. Chang, and N. Peng, “Multilingual\ngenerative language models for zero-shot cross-lingual event argument\nextraction,” in Proceedings of the 60th Annual Meeting of the Asso-\nciation for Computational Linguistics (Volume 1: Long Papers), ACL\n2022, Dublin, Ireland, May 22-27, 2022. Association for Computational\nLinguistics, 2022, pp. 4633–4646.\n[42] L. H. Bonifacio, H. Q. Abonizio, M. Fadaee, and R. F. Nogueira, “In-\npars: Data augmentation for information retrieval using large language\nmodels,” CoRR, vol. abs/2202.05144, 2022.\n[43] H. Chen, Z. Dou, K. Mao, J. Liu, and Z. Zhao, “Generalizing conversa-\ntional dense retrieval via llm-cognition data augmentation,” in Proceed-\nings of the 62nd Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand,\nAugust 11-16, 2024.\nAssociation for Computational Linguistics, 2024,\npp. 2700–2718.\n[44] Z. Dai, V. Y. Zhao, J. Ma, Y. Luan, J. Ni, J. Lu, A. Bakalov,\nK. Guu, K. B. Hall, and M. Chang, “Promptagator: Few-shot dense\nretrieval from 8 examples,” in The Eleventh International Conference\non Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5,\n2023.\nOpenReview.net, 2023.\n[45] L. de Souza Silva and L. Barbosa, “Improving dense retrieval models\nwith LLM augmented data for dataset search,” Knowl. Based Syst., vol.\n294, p. 111740, 2024.\n[46] J. Saad-Falcon, O. Khattab, K. Santhanam, R. Florian, M. Franz,\nS. Roukos, A. Sil, M. A. Sultan, and C. Potts, “UDAPDR: unsupervised\ndomain adaptation via LLM prompting and distillation of rerankers,” in\nProceedings of the 2023 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP 2023, Singapore, December 6-10, 2023.\nAssociation for Computational Linguistics, 2023, pp. 11 265–11 279.\n[47] N. Thakur, N. Reimers, J. Daxenberger, and I. Gurevych, “Augmented\nSBERT: data augmentation method for improving bi-encoders for pair-\nwise sentence scoring tasks,” in Proceedings of the 2021 Conference\nof the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, NAACL-HLT 2021, Online,\nJune 6-11, 2021.\nAssociation for Computational Linguistics, 2021, pp.\n296–310.\n[48] X. Lyu, S. Min, I. Beltagy, L. Zettlemoyer, and H. Hajishirzi, “Z-ICL:\nzero-shot in-context learning with pseudo-demonstrations,” in Proceed-\nings of the 61st Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July\n9-14, 2023. Association for Computational Linguistics, 2023, pp. 2304–\n2317.\n[49] M. Komeili, K. Shuster, and J. Weston, “Internet-augmented dialogue\ngeneration,” in Proceedings of the 60th Annual Meeting of the Asso-\nciation for Computational Linguistics (Volume 1: Long Papers), ACL\n2022, Dublin, Ireland, May 22-27, 2022. Association for Computational\nLinguistics, 2022, pp. 8460–8478.\n[50] A. Wang, L. Song, Q. Liu, H. Mi, L. Wang, Z. Tu, J. Su, and D. Yu,\n“Search-engine-augmented dialogue response generation with cheaply\nsupervised query production,” Artif. Intell., vol. 319, p. 103874, 2023.\n[51] J. Tian, H. Chen, G. Xu, M. Yan, X. Gao, J. Zhang, C. Li, J. Liu,\nW. Xu, H. Xu, Q. Qian, W. Wang, Q. Ye, J. Zhang, J. Zhang, F. Huang,\nand J. Zhou, “Chatplug: Open-domain generative dialogue system with\ninternet-augmented instruction tuning for digital human,” CoRR, vol.\nabs/2304.07849, 2023.\n[52] P. Gupta, J. P. Bigham, Y. Tsvetkov, and A. Pavel, “Controlling\ndialogue generation with semantic exemplars,” in Proceedings of the\n2021 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, NAACL-\nHLT 2021, Online, June 6-11, 2021.\nAssociation for Computational\nLinguistics, 2021, pp. 3018–3029.\n[53] X. Du and H. Ji, “Retrieval-augmented generative question answering\nfor event argument extraction,” in Proceedings of the 2022 Conference\non Empirical Methods in Natural Language Processing, EMNLP 2022,\nAbu Dhabi, United Arab Emirates, December 7-11, 2022.\nAssociation\nfor Computational Linguistics, 2022, pp. 4649–4666.\n[54] Z. Wu, M. Galley, C. Brockett, Y. Zhang, X. Gao, C. Quirk, R. Koncel-\nKedziorski, J. Gao, H. Hajishirzi, M. Ostendorf, and B. Dolan, “A\ncontrollable model of grounded response generation,” in Thirty-Fifth\nAAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third\nConference on Innovative Applications of Artificial Intelligence, IAAI\n2021, The Eleventh Symposium on Educational Advances in Artificial\nIntelligence, EAAI 2021, Virtual Event, February 2-9, 2021.\nAAAI\nPress, 2021, pp. 14 085–14 093.\n[55] D. Yang, J. Rao, K. Chen, X. Guo, Y. Zhang, J. Yang, and Y. Zhang,\n“IM-RAG: multi-round retrieval-augmented generation through learning\ninner monologues,” in Proceedings of the 47th International ACM SIGIR\nConference on Research and Development in Information Retrieval,\nSIGIR 2024, Washington DC, USA, July 14-18, 2024.\nACM, 2024,\npp. 730–740.\n[56] Y. Ren, Y. Cao, P. Guo, F. Fang, W. Ma, and Z. Lin, “Retrieve-and-\nsample: Document-level event argument extraction via hybrid retrieval\naugmentation,” in Proceedings of the 61st Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1: Long Papers), ACL\n2023, Toronto, Canada, July 9-14, 2023. Association for Computational\nLinguistics, 2023, pp. 293–306.\n[57] K. Shuster, M. Komeili, L. Adolphs, S. Roller, A. Szlam, and J. Weston,\n“Language models that seek for knowledge: Modular search & genera-\ntion for dialogue and prompt completion,” in Findings of the Association\nfor Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab\nEmirates, December 7-11, 2022.\nAssociation for Computational\nLinguistics, 2022, pp. 373–393.\n[58] D. Thulke, N. Daheim, C. Dugast, and H. Ney, “Efficient retrieval\naugmented generation from unstructured knowledge for task-oriented\ndialog,” CoRR, vol. abs/2102.04643, 2021.\n[59] Q. Huang, S. Fu, X. Liu, W. Wang, T. Ko, Y. Zhang, and L. Tang,\n“Learning retrieval augmentation for personalized dialogue generation,”\nin Proceedings of the 2023 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP 2023, Singapore, December 6-10, 2023.\nAssociation for Computational Linguistics, 2023, pp. 2523–2540.\n[60] R. A. Gonzalez and S. DiPaola, “Exploring augmentation and cognitive\nstrategies for AI based synthetic personae,” CoRR, vol. abs/2404.10890,\n2024.\n[61] Q. Long, W. Wang, and S. J. Pan, “Adapt in contexts: Retrieval-\naugmented domain adaptation via in-context learning,” in Proceedings\nof the 2023 Conference on Empirical Methods in Natural Language Pro-\ncessing, EMNLP 2023, Singapore, December 6-10, 2023.\nAssociation\nfor Computational Linguistics, 2023, pp. 6525–6542.\n[62] J. Baek, A. F. Aji, and A. Saffari, “Knowledge-augmented language\nmodel prompting for zero-shot knowledge graph question answering,”\nCoRR, vol. abs/2306.04136, 2023.\n[63] T. Gao, H. Yen, J. Yu, and D. Chen, “Enabling large language models\nto generate text with citations,” in Proceedings of the 2023 Conference\non Empirical Methods in Natural Language Processing, EMNLP 2023,\nSingapore, December 6-10, 2023.\nAssociation for Computational\nLinguistics, 2023, pp. 6465–6488.\n[64] M. Seo, J. Baek, J. Thorne, and S. J. Hwang, “Retrieval-augmented\ndata\naugmentation\nfor\nlow-resource\ndomain\ntasks,”\nCoRR,\nvol.\nabs/2402.13482, 2024.\n[65] H. Wang, W. Huang, Y. Deng, R. Wang, Z. Wang, Y. Wang, F. Mi,\nJ. Z. Pan, and K. Wong, “Unims-rag: A unified multi-source retrieval-\naugmented generation for personalized dialogue systems,” CoRR, vol.\nabs/2401.13256, 2024.\n[66] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. R. Narasimhan, and Y. Cao,\n“React: Synergizing reasoning and acting in language models,” in The\n",
  "page_19": "19\nEleventh International Conference on Learning Representations, ICLR\n2023, Kigali, Rwanda, May 1-5, 2023.\nOpenReview.net, 2023.\n[67] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin, “Attention is all you need,” in Advances\nin Neural Information Processing Systems 30: Annual Conference on\nNeural Information Processing Systems 2017, December 4-9, 2017, Long\nBeach, CA, USA, 2017, pp. 5998–6008.\n[68] J. Devlin, M. Chang, K. Lee, and K. Toutanova, “BERT: pre-training\nof deep bidirectional transformers for language understanding,” in Pro-\nceedings of the 2019 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technolo-\ngies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume\n1 (Long and Short Papers). Association for Computational Linguistics,\n2019, pp. 4171–4186.\n[69] A. Radford, “Improving language understanding by generative pre-\ntraining,” 2018.\n[70] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,\nY. Zhou, W. Li, and P. J. Liu, “Exploring the limits of transfer learning\nwith a unified text-to-text transformer,” J. Mach. Learn. Res., vol. 21,\npp. 140:1–140:67, 2020.\n[71] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy,\nV. Stoyanov, and L. Zettlemoyer, “BART: denoising sequence-to-\nsequence pre-training for natural language generation, translation, and\ncomprehension,” in Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics, ACL 2020, Online, July 5-10,\n2020. Association for Computational Linguistics, 2020, pp. 7871–7880.\n[72] L. Xu, H. Xie, S. J. Qin, F. L. Wang, and X. Tao, “Exploring ChatGPT-\nbased Augmentation Strategies for Contrastive Aspect-based Sentiment\nAnalysis,” Sep. 2024.\n[73] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin,\nC. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton,\nF. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. F. Christiano,\nJ. Leike, and R. Lowe, “Training language models to follow instructions\nwith human feedback,” in Advances in Neural Information Processing\nSystems 35: Annual Conference on Neural Information Processing\nSystems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28\n- December 9, 2022, 2022.\n[74] B. Ding, C. Qin, R. Zhao, T. Luo, X. Li, G. Chen, W. Xia, J. Hu, A. T.\nLuu, and S. Joty, “Data Augmentation using LLMs: Data Perspectives,\nLearning Paradigms and Challenges,” Mar. 2024.\n[75] OpenAI, “GPT-4 technical report,” CoRR, vol. abs/2303.08774, 2023.\n[76] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei,\nN. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, D. Bikel, L. Blecher,\nC. Canton-Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes,\nJ. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn,\nS. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa,\nI. Kloumann, A. Korenev, P. S. Koura, M. Lachaux, T. Lavril, J. Lee,\nD. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra,\nI. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi,\nA. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan, B. Tang,\nR. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang,\nA. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov,\nand T. Scialom, “Llama 2: Open foundation and fine-tuned chat models,”\nCoRR, vol. abs/2307.09288, 2023.\n[77] R. Sennrich, “Improving neural machine translation models with mono-\nlingual data,” arXiv preprint arXiv:1511.06709, 2015.\n[78] T. Schick and H. Sch¨utze, “It’s not just size that matters: Small\nlanguage models are also few-shot learners,” in Proceedings of the\n2021 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, NAACL-\nHLT 2021, Online, June 6-11, 2021.\nAssociation for Computational\nLinguistics, 2021, pp. 2339–2352.\n[79] M. Joshi, D. Chen, Y. Liu, D. S. Weld, L. Zettlemoyer, and O. Levy,\n“Spanbert: Improving pre-training by representing and predicting spans,”\nTrans. Assoc. Comput. Linguistics, vol. 8, pp. 64–77, 2020.\n[80] P. Sahoo, A. K. Singh, S. Saha, V. Jain, S. Mondal, and A. Chadha,\n“A systematic survey of prompt engineering in large language models:\nTechniques and applications,” arXiv preprint arXiv:2402.07927, 2024.\n[81] P. S. H. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal,\nH. K¨uttler, M. Lewis, W. Yih, T. Rockt¨aschel, S. Riedel, and D. Kiela,\n“Retrieval-augmented generation for knowledge-intensive NLP tasks,”\nin Advances in Neural Information Processing Systems 33: Annual\nConference on Neural Information Processing Systems 2020, NeurIPS\n2020, December 6-12, 2020, virtual, 2020.\n[82] X. Liu and W. B. Croft, “Statistical language modeling for information\nretrieval,” Annu. Rev. Inf. Sci. Technol., vol. 39, no. 1, pp. 1–31, 2005.\n[83] S. Rose, D. Engel, N. Cramer, and W. Cowley, “Automatic keyword\nextraction from individual documents,” Text mining: applications and\ntheory, pp. 1–20, 2010.\n[84] G. A. Miller, “Wordnet: a lexical database for english,” Communications\nof the ACM, vol. 38, no. 11, pp. 39–41, 1995.\n[85] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al.,\n“Language models are unsupervised multitask learners,” OpenAI Blog,\nvol. 1, no. 8, p. 9, 2019.\n[86] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig, “Pre-\ntrain, prompt, and predict: A systematic survey of prompting methods\nin natural language processing,” ACM Comput. Surv., vol. 55, no. 9, pp.\n195:1–195:35, 2023.\n[87] B. Chen, Z. Zhang, N. Langren´e, and S. Zhu, “Unleashing the potential\nof prompt engineering in large language models: a comprehensive\nreview,” CoRR, vol. abs/2310.14735, 2023.\n[88] G. Salton and C.-S. Yang, “On the specification of term values in\nautomatic indexing,” Journal of documentation, vol. 29, no. 4, pp. 351–\n372, 1973.\n[89] S. E. Robertson, S. Walker, S. Jones, M. M. Hancock-Beaulieu, M. Gat-\nford et al., “Okapi at trec-3,” Nist Special Publication Sp, vol. 109, p.\n109, 1995.\n[90] V. Karpukhin, B. Oguz, S. Min, P. S. H. Lewis, L. Wu, S. Edunov,\nD. Chen, and W. Yih, “Dense passage retrieval for open-domain question\nanswering,” in Proceedings of the 2020 Conference on Empirical Meth-\nods in Natural Language Processing, EMNLP 2020, Online, November\n16-20, 2020.\nAssociation for Computational Linguistics, 2020, pp.\n6769–6781.\n[91] J. Johnson, M. Douze, and H. J´egou, “Billion-scale similarity search\nwith gpus,” IEEE Trans. Big Data, vol. 7, no. 3, pp. 535–547, 2021.\n[92] L. Xiong, C. Xiong, Y. Li, K. Tang, J. Liu, P. N. Bennett, J. Ahmed,\nand A. Overwijk, “Approximate nearest neighbor negative contrastive\nlearning for dense text retrieval,” in 9th International Conference on\nLearning Representations, ICLR 2021, Virtual Event, Austria, May 3-7,\n2021.\nOpenReview.net, 2021.\n[93] T. Gao, X. Yao, and D. Chen, “Simcse: Simple contrastive learning\nof sentence embeddings,” in Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing, EMNLP 2021,\nVirtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021.\nAssociation for Computational Linguistics, 2021, pp. 6894–6910.\n[94] S. Humeau, K. Shuster, M.-A. Lachaux, and J. Weston, “Poly-encoders:\nArchitectures and pre-training strategies for fast and accurate multi-\nsentence scoring,” in International Conference on Learning Representa-\ntions, 2019.\n[95] N. Reimers and I. Gurevych, “Sentence-bert: Sentence embeddings using\nsiamese bert-networks,” in Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Processing, EMNLP-\nIJCNLP 2019, Hong Kong, China, November 3-7, 2019.\nAssociation\nfor Computational Linguistics, 2019, pp. 3980–3990.\n[96] G. Izacard, M. Caron, L. Hosseini, S. Riedel, P. Bojanowski, A. Joulin,\nand E. Grave, “Unsupervised Dense Information Retrieval with Con-\ntrastive Learning,” Aug. 2022.\n[97] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,\nL. Zettlemoyer, and V. Stoyanov, “Roberta: A robustly optimized BERT\npretraining approach,” CoRR, vol. abs/1907.11692, 2019.\n[98] S. Hofst¨atter, S. Lin, J. Yang, J. Lin, and A. Hanbury, “Efficiently teach-\ning an effective dense retriever with balanced topic aware sampling,” in\nSIGIR ’21: The 44th International ACM SIGIR Conference on Research\nand Development in Information Retrieval, Virtual Event, Canada, July\n11-15, 2021.\nACM, 2021, pp. 113–122.\n[99] J. Ni, C. Qu, J. Lu, Z. Dai, G. H. ´Abrego, J. Ma, V. Y. Zhao,\nY. Luan, K. B. Hall, M. Chang, and Y. Yang, “Large dual encoders\nare generalizable retrievers,” in Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing, EMNLP 2022, Abu\nDhabi, United Arab Emirates, December 7-11, 2022.\nAssociation for\nComputational Linguistics, 2022, pp. 9844–9855.\n[100] K. Song, X. Tan, T. Qin, J. Lu, and T. Liu, “Mpnet: Masked and\npermuted pre-training for language understanding,” in Advances in Neu-\nral Information Processing Systems 33: Annual Conference on Neural\n",
  "page_20": "20\nInformation Processing Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual, 2020.\n[101] X. Chen, H. Xie, S. J. Qin, Y. Chai, X. Tao, and F. L. Wang,\n“Cognitive-inspired deep learning models for aspect-based sentiment\nanalysis: A retrospective overview and bibliometric analysis,” Cognitive\nComputation, pp. 1–39, 2024.\n"
}