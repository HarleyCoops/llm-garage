[
  {
    "text": "Text Data Augmentation for Large Language Models: A Comprehensive Survey of Methods, Challenges, and Opportunities Yaping Chai, Haoran Xie, Joe S. Qin Abstract The increasing size and complexity of pre-trained lan- guage models have demonstrated superior performance in many applications, but they usually require large training datasets to be adequately trained. Insufficient training sets could unexpectedly make the model overfit and fail to cope with complex tasks. Large language models (LLMs) trained on extensive corpora have prominent text generation capabilities, which improve the quality and quantity of data and play a crucial role in data augmentation. Specifically, distinctive prompt templates"
  },
  {
    "text": "are given in personalised tasks to guide LLMs in generating the required content. Recent promising retrieval-based techniques further im- prove the expressive performance of LLMs in data augmentation by introducing external knowledge to enable them to produce more grounded-truth data. This survey provides an in-depth analysis of data augmentation in LLMs, classifying the tech- niques into Simple Augmentation, Prompt-based Augmentation, Retrieval-based Augmentation and Hybrid Augmentation. We summarise the post-processing approaches in data augmentation, which contributes significantly to refining the augmented data and enabling the model to filter out unfaithful content. Then, we provide the common tasks and evaluation metrics. Finally,"
  },
  {
    "text": "we introduce existing challenges and future opportunities that could bring further improvement to data augmentation. Index Terms Data Augmentation, Large Language Models, Text Processing, Natural Language Processing I. INTRODUCTION LLMs have demonstrated remarkable language understand- ing and generation capabilities with their massive number of parameters and training data, bringing innovation to many applications [1]. However, training and optimizing pre-trained language models (PLMs) requires enormous amounts of high- quality data. Poor data quality and data scarcity issues hinder the further improvement of PLMs [2]. To solve these chal- lenges, data augmentation is an effective technique to generate more available training data"
  },
  {
    "text": "by transforming and expanding existing data, improving models performance in many natural language processing (NLP) tasks [3]. The earliest popular data augmentation methods include synonym replacement, word order scrambling and random word deletion [4]. Although these This work was supported by a grant from the Research Grants Council of the Hong Kong Special Administrative Region, China (R1015-23) and the Faculty Research Grant (SDS24A8) and the Direct Grant (DR25E8) of Lingnan University, Hong Kong. (Corresponding author: Haoran Xie.) Yaping Chai, Haoran Xie, and Joe S. Qin are with the School of Data Science, Lingnan University, Hong Kong (e-mail: yapingchai@ln.hk; hrxie@ln.edu.hk; joeqin@ln.edu.hk)."
  },
  {
    "text": "conventional methods could enhance existing data, simple word transformations may not be able to fully utilize the potential of LLMs [5]. With the increasing development of prompt engineering and its application in LLMs, many researchers augment the seed training data by designing crafted prompts for LLMs to generate diverse datasets, contributing to producing high- quality and semantically rich data [6]. Providing prompts to LLMs cannot access up-to-date factual knowledge, unavoidably causing the hallucination phenomenon [7], [8]. Many studies have explored retrieving external data for data augmentation. Integrating additional knowledge sig- nificantly enhances PLMs understanding and generalization capabilities by accessing relevant"
  },
  {
    "text": "facts from a large-scale external corpus [9]. Along with the continuous progress of research, studies have not only retrieved grounded facts from external corpora but also combined them with LLMs powerful few-shot capability to effectively apply them to various tasks [8]. A typical application is problem-answering tasks, where a retrieval model is used to obtain relevant documents from a database given a query. Then, adopting few-shot learning to prompt the model to answer questions based on the retrieved documents without fine-tuning or learning additional parameters [8]. Based on the background of LLMs, we divide the techniques of text data augmentation"
  },
  {
    "text": "into four categories: Simple Aug- mentation, Prompt-based Augmentation, Retrieval-based Aug- mentation, and Hybrid Augmentation. Figure 1 demonstrates each example of four techniques. We present recent research on the four categories of text data augmentation in Figure 2. We focus on summarising how recent studies have performed data augmentation under four technique categories. We introduce a comprehensive review of multiple data augmentation aspects and granularity [9], aiming to illustrate the foundational prelim- inaries. Considering that the effectiveness of data augmentation cannot be entirely guaranteed, we demonstrate common post- processing approaches to refine generated data quality further. In summary, the contributions of"
  },
  {
    "text": "this survey are as follows: We comprehensively demonstrate data augmentation tech- niques in the context of large-scale language models, including early Simple Augmentation, Prompt-based Aug- mentation, Retrieval-based Augmentation, and Hybrid Augmentation, outlining the development stages of data arXiv:2501.18845v1 [cs.CL] 31 Jan 2025"
  },
  {
    "text": "Figure 1: Four categories of data augmentation techniques. augmentation under the trend of increasing model size. We present two prerequisites before data augmentation, aspect and granularity, and introduce the refinement ap- proaches after data augmentation, providing a systematic and complete structure. We summarize the common tasks and evaluation metrics of data augmentation in various applications and analyze the current limitations and worthwhile future research directions. The rest of this survey is structured as follows: section II introduces LLMs, which typically refer to encoder-only models, decoder-only models, and encoder-decoder models. Followed by presenting data augmentation aspects and granularity from the finest"
  },
  {
    "text": "to the coarsest. Section III showcases a detailed breakdown of the four technique categories, revealing typical traditional methods, the complexity of distinct designs for prompts, diverse retriever types and the various combinations of Prompt-based Augmentation and Retrieval-based Augmentation in recent studies. Section IV focuses on interpreting widely used post-processing approaches for data augmentation. Section V describes common tasks and evaluation metrics. Section VI explains the unsolved challenges and future research directions. Finally, we conclude this survey in section VII. II. PRELIMINARIES A. Large Language Models Large language models (LLMs) trained on an enormous corpus with rich language knowledge and prominent"
  },
  {
    "text": "generation capabilities are widely used in academia and industry. A popular approach to divide LLMs is based on the Transformer [67] architecture: encoder-only models (e.g. the BERT families [68]), decoder-only models (e.g. the GPT families [69]), and encoder-decoder models (e.g. T5 [70] and BART [71]). 1) Encoder-only Models: The encoder-only model is rep- resented by BERT [68], which has the main feature of bidi- rectional encoding. BERT performs contextual bidirectional encoding on the input data and captures multiple semantic information of each word in different contexts, enabling BERT and its variants to showcase outstanding performance in down- stream tasks such"
  },
  {
    "text": "as sentiment classification [10], [20], [72] named entity recognition [35], [5], and question-answering systems [24], [33], [4]. 2) Decoder-only Models: Compared with encoder-only models, decoder-only models include ChatGPT [73] that only use the Transformer decoder structure and demonstrate power- ful text generation abilities. They predict text word by word through unidirectional decoding and produce coherent and"
  },
  {
    "text": "Figure 2: Recent studies on four categories of data augmentation techniques. As mentioned in section III, Hybrid Augmentation technique combines superior few-shot learning capabilities similar to prompt engineering and a retriever to obtain external knowledge. Using only the prompt portion of the RAG itself, we categorise it as the Retrieval-based Augmentation technique. natural responses, allowing them to generate diverse and high- quality training data that effectively improve models robust- ness and generalisation ability [3], [37], [40]. 3) Encoder-decoder Models: Encoder-decoder models com- bine the advantages of Transformer s encoder and decoder. Representative models are T5 [70] and BART [71]. The"
  },
  {
    "text": "dual- module structure empowers them not only to understand the deep semantics of the input content but also to generate semantically augmented data. Many researches train encoder- decoder models for diverse NLP tasks [26], [29], [34], [36], [39] and utilize them for text generation without fine-tuning [4], [11], [25], [33]. B. Data Augmentation Aspects Data augmentation (DA) is one of the critical techniques to improve the performance of pre-trained language models by transforming existing data to obtain diverse and augmented data. Instead of limiting one DA aspect, combining multiple aspects could generate an extensive synthetic dataset. The common aspects of"
  },
  {
    "text": "data augmentation include the following ways and are illustrated in Table I. 1) Data Generation: Data generation is one of the most basic and crucial aspects of data augmentation [74]. A growing trend in generation tasks is to employ a Transformer-based language model with a decoder component to create new datasets [11]."
  },
  {
    "text": "Table I: Data Augmentation Aspects. Methods Generation Paraphrasing Translation Labeling Retrieval Editing Simple Augmentation TransformersDA [10] DAGAM [11] GenAug [12] AuGPT [13] COCA [14] Selection-DA [15] LAMBADA [16] LeCA [17] G-DAUGc [18] MRC-QA [19] Prompt-based Augmentation GPT3Mix [20] DA-intent [21] WANLI [22] FlipDA [4] AugESC [23] AugGPT [2] Read-Com [24] DAIL [25] DA-NMT [26] EPA [27] ZeroShotDataAug [28] Dialogue-Convert [29] HiPSTG [30] SUNGEN [31] LLM-powered [1] LLM-PTM [32] Generative-DA [33] ICLEF [34] LLM-DA [35] Synthetic-DA [36] LLM Assisted [3] LLM2LLM [37] PromptMix [38] Unnatural-instructions [39] GENIUS [5] TAPP [40] X-GEAR [41] InPars [42] ConvAug [43] Promptagator [44] DAPDR [45] UDAPDR"
  },
  {
    "text": "[46] Retrieval-based Augmentation AugmentedSBERT [47] zicl [48] RetGen [7] Internet-Aug [49] DialogGen [50] ChatPLUG [51] EDGE [52] RGQA [53] CGRG [54] IM-RAG [55] EAE-RAG [56] SeeKeR [57] Efficient-RAG [58] LAPDOG [59] Personae-DA [60] Hybrid Augmentation DAICL [61] KAPING [62] ALCE [63] RADA [64] UniMS-RAG [65] QA-Internet [8] ReAct [66]"
  },
  {
    "text": "For decoder-only models, such as Read-Com [24] leverages GPT-4 [75] to obtain synthesised datasets similar to the original data s style and semantics. LLM Assisted [3] adopts Llama2 [76] to produce three different level augmentations based on a provided sample in the training set. For encoder-decoder models, DAGAM [11] summarises a vast set of sentences from original datasets and utilizes T5 [70] to generate synthetic data with a similar representation distribution to original datasets. TransformersDA [10] first employs two word-level masking strategies to reconstruct the original sequence. Then, it applies BART [71] to decode the masked sequence and produce high-quality"
  },
  {
    "text": "output text. 2) Data Paraphrasing: Data paraphrasing refers to express- ing the original training data differently within a reasonable deviation. The original training data is rephrased by changing the sentence s structure, wording or grammar. EPA [27] pro- poses to paraphrase the available demonstrations on the source and target sides to acquire generated demonstrations for in- context learning. DAIL [25] expands the data by using the LLM itself to paraphrase the original test samples and obtain multiple candidate texts. The final labels are created by not only considering the original text but also taking into account all paraphrased candidates. Unnatural-instructions"
  },
  {
    "text": "[39] first use structured instruction format and some filtering heuristic methods to collect a core sample set. Then, it rephrases struc- tured instructions to extend the core dataset by prompting the language model to use manually constructed samples. Similarly, AugGPT [2] designs prompts to guide LLMs to rephrase the original data and obtain more data. ConvAug [43] introduces to mimic the variety of user expressions for similar intentions by utilising the LLM to extend the language s diversity by paraphrasing the whole context, reducing the model to overfit specific phrases or patterns. 3) Data Translation: Data translation aims to translate"
  },
  {
    "text": "text from one language to another language and then translate it back to the original language. DA-NMT [26] generates n translations with the same meaning but different languages for each original source sentence s, then maps a source sentence to n target sentences, and finally generates n parallel data. AuGPT [13] utilizes the advantages of back-translation [77] and employs a well-trained multilingual machine translation model to rephrase the data. It uses ten intermediate languages to obtain distinct new data for each input utterance. 4) Data Editing: Data editing refers to performing simple transformations on the text, which includes insertion[12], re-"
  },
  {
    "text": "ordering [14], deletion [12], rewriting [35] and shuffling [11]. DAGAM [11] implements a character order change strategy by fixing the first and last characters in a word and shuffling the rest to augment the data. LLM-DA [35] creates entities that are semantically similar to the sentence but diverse by replacing entities in the sentence with other entities of the same type. ConvAug [43] proposes Entity Replacing as one of its augmentation strategies. It uses the LLM to recognise and replace entities in a context similar to the original but differing in significant details. The model could focus more closely on"
  },
  {
    "text": "crucial information rather than trivial aspects through contrastive learning. Many tasks assume a strict sequence order. However, COCA [14] observes that the sequence of users search behaviour is more flexible. It proposes a behavioural reordering strategy to avoid relying too much on sequential information. 5) Data labeling: Data labeling refers to assigning labels to synthetic samples or unlabeled data [74]. In practice, adopting manual annotation to obtain more ground-truth labels is ex- pensive and cannot be performed on a considerable scale [36]. Manual annotation by experts may require additional implicit knowledge that is difficult to capture through statistical methods [15]."
  },
  {
    "text": "A common way to automatically label synthesised examples is using a language model [25]. In the domain adaptation data augmentation strategy, AugmentedSBERT [47] first fine- tune BERT on the source domain comprising paired anno- tations. After fine-tuning, the fine-tuned BERT is applied to annotate the target domain. Finally, training SBERT on the annotated target domain sentence pairs. Once the retrieved sentences are obtained, zicl [48] pairs each sentence with a label by a synonym labeling strategy to reduce the copy effect phenomenon. DAIL [25] considers candidate labels from the original sample and all paraphrased texts and determines the final label"
  },
  {
    "text": "by majority voting method. Synthetic-DA [36] attempts two scenarios: one is to annotate the set of unlabeled instances when there is a significant amount of unlabeled text, but the cost of obtaining annotations is high; the other is to generate the entire input-output pair without available annotated instances. PromptMix [38] first directs an LLM to produce samples that mix two classes and then uses relabeling prompts to enhance the faithfulness of the synthesised examples. 6) Data Retrieval: Data retrieval is an efficient way to use external data to retrieve samples related to the current task from a large-scale text corpus"
  },
  {
    "text": "and concatenate them into the training set [9]. AugmentedSBERT [47] and zicl [48] expand the data volume by incorporating data retrieved from the external source with the original data. DAICL [61] combines each input from the source domain with relevant retrieved texts from the target domain to enrich the dataset. Then, the model learns task discrimination based on the source content and target texts. Rather than incorporating the retrieved content with existing data for training, more studies have incorporated relevant in- formation and ground truth retrieved from the external corpus into LLM s input context, thereby enhancing LLM s factual"
  },
  {
    "text": "response [64]. KAPING [62] and ALCE [63] employ the retrieved information as additional knowledge to guide the model to produce more accurate responses. Also, recent researches explore utilising search engines [50] and various APIs [66], [8], [57] to conduct retrieval and access up-to-date knowledge from a more extensive source. C. Data Augmentation Granularity In addition to data augmentation aspects, data augmenta- tion granularity significantly affects the augmented content. A"
  },
  {
    "text": "Table II: Data Augmentation Granularity. Methods Token Level Token-span Level Sentence Level Passage Level Context Level Document Level Simple Augmentation TransformersDA [10] DAGAM [11] GenAug [12] AuGPT [13] COCA [14] Selection-DA [15] LAMBADA [16] LeCA [17] G-DAUGc [18] MRC-QA [19] Prompt-based Augmentation GPT3Mix [20] DA-intent [21] WANLI [22] FlipDA [4] AugESC [23] AugGPT [2] Read- Com [24] DAIL [25] DA-NMT [26] EPA [27] ZeroShotDataAug [28] Dialogue-Convert [29] HiPSTG [30] SUNGEN [31] LLM-powered [1] LLM-PTM [32] Generative-DA [33] ICLEF [34] LLM-DA [35] Synthetic-DA [36] LLM Assisted [3] LLM2LLM [37] PromptMix [38] Unnatural-instructions [39] GENIUS [5] TAPP [40] X-GEAR [41] InPars [42]"
  },
  {
    "text": "ConvAug [43] Promptagator [44] DAPDR [45] UDAPDR [46] Retrieval-based Augmentation AugmentedSBERT [47] zicl [48] RetGen [7] Internet-Aug [49] DialogGen [50] ChatPLUG [51] EDGE [52] RGQA [53] CGRG [54] IM-RAG [55] EAE-RAG [56] SeeKeR [57] Efficient-RAG [58] LAPDOG [59] Personae-DA [60] Hybrid Augmentation DAICL [61] KAPING [62] ALCE [63] RADA [64] UniMS-RAG [65] QA-Internet [8] ReAct [66]"
  },
  {
    "text": "coarser granularity typically less perturbs the original data and retains more initial information, while a finer granularity is more likely to produce more significant perturbations [9]. Dif- ferent data augmentation granularity could control the created data s diversity and preserve the original data s characteristics. Detailed information is depicted in Table II to present diverse granularity. 1) Token Level: Token level data augmentation is the most fine-grained granularity, mainly performed on single words. FlipDA [4] first combines text x and labels y into a sequence using a cloze pattern [78] and then randomly masks a fixed per- centage of the"
  },
  {
    "text": "input tokens. DAGAM [11] proposes a character order change (COC) strategy to achieve token insertion, token deletion and token replacement effects. RGQA [53] introduces three new tokens to demonstrate each component of the input sequence in the prompt and instantiate all portions to construct the final input sequence. ConvAug [43] treats a context as a sequence of tokens C and introduces two token level approaches on C, which are Token Masking and Entity Replacing, to help the model learn subtle information differences. Inspired by the word mask technique, COCA [14] proposes to mask terms randomly over user behavioural sequences to"
  },
  {
    "text": "prevent model overfitting and enhance the robustness of behaviour sentence representations. 2) Token-span Level: Compared with single token level augmentation, the token-span level involves a set of con- tinuous tokens that could obtain more semantic information between continuous tokens [79]. TransformersDA [10] applies two masking strategies to fine-tune BART: one is replacing a single word with a mask token <mask>, and the other is masking a continuous chunk of words with a single <mask>. Motivated by the outstanding performance of predicting masked words [79] and contiguous spans [70] as self-supervised train- ing objectives, Dialogue-Convert [29] concatenates gap text spans into"
  },
  {
    "text": "a pseudo-summary in the pre-training stage for the generation task in the medical field. KAPING [62] proposes to retrieve triples that are question-only related. One triple comprises (s, r, o), where s and o represent the subject and object entities, respectively, and r is a specific relation type between them. LLM-DA [35] adopts entity-level augmentation to tackle the NER task, which is necessary to understand the syntactic structure and semantic information of each token in the sentence to identify entities precisely. MRC-QA [19] firstly conducts probability sampling on which answers need to be employed for data augmentation. Secondly, once the"
  },
  {
    "text": "question-answer pairs are selected, a set of fuzzy answer spans are generated by shifting the correct answer span by certain characters to the left or right. 3) Sentence Level: Sentence level is the most widely used granularity in data augmentation, mainly focusing on the whole sentence, such as sentence generation and translation. Given an input sentence and its related entities, LLM-DA [35] aims to generate a collection of sentence variants. These variants include different aspects such as vocabulary usage, clauses, and expression styles. New sentences with rewritten context are obtained throughout the process while keeping the entities from the initial"
  },
  {
    "text": "sentences. ZeroShotDataAug [28] leverages the LLM to produce a significant number of sentences for each task in a zero-shot setting, improving the model s performance in the low-resourced scenario. LAMBADA [16] fine-tunes the LLM to learn the training data patterns and produce synthesised sentences based on specific labels. Some studies paraphrase the original sentence to increase the training dataset s diversity [2], [25], [26], [27]. In addition to common NLP tasks, in information retrieval (IR), InPars [42] and Promptagator [44] adopt query generation for data augmentation and improve retrieval accuracy by gen- erating a query and retrieving documents related to"
  },
  {
    "text": "the query in the document set. UDAPDR [46] and DAPDR [45] prompt an LLM to generate an enormous number of synthesis queries for passages, improving retrieval accuracy. An innovative approach is to employ external datasets to achieve data augmentation. zicl [48] and AugmentedSBERT [47] augment the limited dataset by retrieving sentences from an external corpus related to the training sentences. Then, concatenate the retrieved and original sentences to expand the dataset. 4) Passage Level: Passage level is a coarser level of data augmentation granularity. It could be adjusting the order of passages or inserting new content between passages. Given a"
  },
  {
    "text": "question, ALCE [63] creates text while citing corresponding supporting passages from a large-scale retrieval corpus. For the event argument extraction task, X-GEAR [41] extracts information from the input passage to fill in the language- agnostic template and guides the model to create a target string based on the language-agnostic template. 5) Context Level: Context level data augmentation requires language models to generate responses that demand an un- derstanding of the input context. The most common applica- tions are question-answering and conversation-generation tasks. Synthetic-DA [36] create more training data by providing LLM with prompts to understand the text context. LLM2LLM [37]"
  },
  {
    "text": "devises different system prompts for each task, which enable users to utilize domain-specific knowledge in the dataset gener- ation process, and it leverages in-context learning to create more relevant examples. Given a context, RADA [64] aims to gener- ate an extracted question-answer pair and requires obtaining the answer to the question from the provided context. Unnatural- instructions [39] constructs the meta-prompt containing the in- context example s number and each example s constant. This method prompts the model to create an example based on the in-context demonstrations. 6) Document Level: Document level data augmentation is the coarsest-grained granularity, usually utilized"
  },
  {
    "text": "for document generation and retrieval tasks. AugESC [23] introduces the im- plementation of the LLM to accomplish the entire dialogue for diverse topics. HiPSTG [30] proposes a hierarchical discipline structure to generate the whole proposal by comprehending the structured prompt. COCA [14] proposes three augmentation strategies, one of which is document level. This strategy utilizes"
  },
  {
    "text": "contrastive learning for query/document deletion to enhance the learning of user behaviour sequence representations. As for document retrieval tasks, SeeKeR [57] employs the search engine to retrieve relevant documents and then keep only the top 5 documents to generate the knowledge response. RetGen [7] introduces a framework with a dense document re- triever component to obtain the most significant documents and a knowledge-grounded text generator component to produce the target output. III. DATA AUGMENTATION TECHNIQUES FOR TEXT IN LLMS Early traditional data augmentation methods include basic text transformation [12], [11], [19], and Back-translation [13]. The arrival of large language models"
  },
  {
    "text": "(LLMs) has given rise to an increasing trend of utilizing the power of generative models and prompts them to generate diverse datasets [80]. To address LLMs inevitable limitations, such as hallucina- tions, retrieving relevant facts from extensive external corpora effectively improves the generated response [81]. With the con- tinuous progress of research, combining up-to-date knowledge with LLM s prominent few-shot learning capability has jointly promoted the application of data augmentation in various tasks [64], [61], [63]. We classify data augmentation techniques into four categories based on the critical technologies used in recent years. Each technique has distinct characteristics, as Figure"
  },
  {
    "text": "3 illustrates. We classify four DA techniques based on Prompt Complexity and Retrieval Model Complexity. Simple Augmentation is a conservative augmentation method that does not construct prompts or utilize retrieval methods. The augmented data is only slightly modified based on the original data and has a high similarity with the exist- ing data [5]. Similar to Simple Augmentation, Prompt-based Augmentation does not employ a retrieval module, but the difference is that it is a widely used technique during the flourishing period of prompt engineering, which opened up guiding large-scale language models to generate an extensive set of data by designing"
  },
  {
    "text": "diverse prompts [20]. Based on the Prompt Complexity, we categorize the recent prompt-based studies into basic Single-step Prompting (such as Zero-shot Prompting), Multi-step Prompting (such as Chain-of-Thought Prompting), and advanced Structured Prompting. In contrast, Retrieval-based Augmentation does not utilize LLM s few- shot learning ability, but instead obtains external knowledge through a retrieval model. The Retrieval Model Complexity ranges from Sparse Retrieval and Dense Retrieval in the early information retrieval stage [82], to more complex Graph-based Retrieval, and now with the emergence of RAG [81] in the era of LLMs, retrieval methods can be more personalized and advanced through search"
  },
  {
    "text": "engines. Hybrid Augmentation has exceptional few-shot learning ability which is well-known in the prompt engineering era and includes a retrieval module that could retrieve additional knowledge, although standard RAG [81] methods also have a prompt portion, we classify them as Retrieval-based Augmentation since it does not exhibit the few- shot learning capacity. The detailed data augmentation methods for four techniques in recent studies are depicted in Figure 4. A. Simple Augmentation Simple Augmentation is an early data augmentation tech- nique, such as synonym replacement, word deletion [4] and back translation [77]. Many traditional data augmentation works have applied large language"
  },
  {
    "text": "models to create diverse datasets and alleviate data scarcity issues. 1) Text Transformation: A conventional approach for data augmentation is through simple text transformations [20]. GenAug [12] explores multiple text augmentation approaches, including randomly swapping two words positions, deleting one word, or performing character insertion. For keyword replacement, this method uses RAKE [83] to extract keywords and incorporates external information through WordNet [84] by replacing keywords. The replaced keywords are sorted by their RAKE scores. One of DAGAM [11] data augmentation schemes is to adopt a character order change strategy. This strategy involves fixing a word s first and last"
  },
  {
    "text": "characters and randomly changing the remaining characters. MRC-QA [19] creates augmented data by shifting the correct answer span a few characters to the left or right. Each synthetic answer span is used as additional training data. 2) Seed Selection Strategies: During the model training phase, using selected seed training data could enable the model to learn key features of the data [15]. Choosing a high- quality dataset for model training is crucial to reduce the number of training examples and lower computational costs [47]. Selection-DA [15] implements various seed selection strategies and highlights the impact of the seed training samples"
  },
  {
    "text": "selection process on the quality of synthetic data and classifier performance. Further, it proposes a human-in-the-loop method for selecting seed samples. 3) Back-translation: Back-translation [77] is a typical data augmentation technique used in machine translation, which generates parallel training data by translating monolingual data from the target language back to the source language. AuGPT [13] utilizes back-translation to paraphrase the original data to increase data diversity. 4) Sequence-based Methods: More recently, a growing num- ber of researches have employed pre-trained language models to obtain augmented datasets. COCA [14] randomly masks a fixed percentage of the input tokens to enhance the"
  },
  {
    "text": "robustness of the user s behaviour sentence representation and incorporates the user s behaviour information into the document ranking process. TransformersDA [10] conditions the pre-trained mod- els by prepending class labels in text sequences and produces one synthesized example for every example in the training set. LAMBADA [16] fine-tunes GPT-2 [85] using existing labelled data. It leverages a simple form, using [SEP] and [EOS] to concatenate multiple sentences. For any class of labels y, use the fine-tuned language model to predict the duration of the sequence y [SEP] until [EOS], allowing each class to synthesize any number of sentences. For"
  },
  {
    "text": "the"
  },
  {
    "text": "Figure 3: Data augmentation techniques in the No Prompt-Basic-Advanced spectrum according to Prompt Complexity and No Retrieval-Basic-Advanced spectrum according to Retrieval Model Complexity. question-answering task, G-DAUGc [18] fine-tunes a language model on the training question set Qi to train the question generator, where Qi is the word sequence representing the ith problem. Then, it generates new questions using the fine- tuned model and uses a similar method to create synthetic answers and distractors to obtain an augmented dataset. LeCA [17] designs the input representation to distinguish between the source sentence and each constraint and applies lexical constraints to augment data"
  },
  {
    "text": "in neural machine translation. B. Prompt-based Augmentation The success of prompt engineering has dramatically stim- ulated the capabilities of LLMs [86]. Providing LLMs with carefully designed prompts can make LLMs produce more human-like responses. Prompt-based Augmentation technique effectively improve many downstream tasks performance by providing task-related or cross-task prompts to guide LLMs in generating high-quality data. 1) Single-step Prompts: Single-step Prompts only devise one-step instructions. The most common form is a straight- forward question or a specific task requirement that does not require the model to perform multi-step reasoning. Zero-shot Prompting LLMs have been adjusted to follow instructions and are"
  },
  {
    "text": "trained on the extensive corpus. Their enormous knowledge enables them to perform specific tasks in a zero-shot manner [28]. DA-NMT [26] provides storytelling, paraphrasing and multi-target prompts without any examples to acquire synthetic parallel datasets by utilizing ChatGPT. ZeroShotDataAug [28] employs LLMs to directly produce data based on the prompt in a zero-shot setting. For information retrieval (IR) tasks, DAPDR [45] creates relevant queries for a given text document in a zero-shot setting, then leverages IR models to acquire top-ranked can- didate datasets for each synthetic query by calculating the cosine similarity between texts. UDAPDR [46] uses zero-shot prompting and"
  },
  {
    "text": "also experiments with few-shot prompting to guide an LLM to create a set of synthesized queries. The queries generated from each prompt are applied to train a separate reranker, which is distilled into a single dense retriever for the target domain. Few-shot Prompting Unlike Zero-shot Prompting, Few- shot Prompting provides specific examples or one example to prompt LLMs to generate desired responses [80]. AugGPT [2] uses ChatGPT [73] as a data augmentation tool in few-shot learning. In particular, ChatGPT is employed to paraphrase each input sentence into a collection of extra sentences, thereby expanding the training samples. EPA [27] and"
  },
  {
    "text": "DAIL [25] apply a one-shot example to prompt the LLM to rephrase the original example and create new paraphrased examples. LLM Assisted [3] leverages few-shot prompting to guide LLMs to paraphrase the original sentences or content. DA-intent [21] addresses the difficulty of obtaining a large set of example utterances with the same intent by feeding an intent with available K-shot examples to an LLM. In addition to common NLP tasks, many studies have ex-"
  },
  {
    "text": "Text Data Augmentation Techniques in LLMs Simple Augmentation Text Transformation GenAug [12]; DAGAM [11]; MRC-QA [19] Seed Selection Strategies Selection-DA [15] Back-translation AuGPT [13] Sequence-based Methods COCA [14]; TransformersDA [10]; LAMBADA [16]; LeCA [17]; G-DAUGc [18] Prompt-based Augmentation Single-step Prompts Cloze Prompting FlipDA[4]; GENIUS[5]; SUNGEN[31] Zero-shot Prompting DA-NMT[26]; ZeroShotDataAug[28]; UDAPDR[46]; DAPDR[45]; Generative-DA[33]; UniMS-RAG[65] Few-shot Prompting AugGPT[2]; EPA[27]; DAIL[25];Promptagator[44]; InPars[42]; DA-intent[21]; LLM-Assisted[3]; Read-Com[24] Dialogue-Convert[29]; UDAPDR[46]; Synthetic-DA[36]; RADA[64]; DAICL[61]; QA-Internet[8]; ALCE[63] Multi-step Prompts Chain-of-Thought Prompting LLM-PTM[32]; LLM Assisted[3]; ConvAug[43];ReAct[66] Structured Prompts Role Prompting AugESC[23]; ICLEF[34] Tuple Prompting GPT3Mix[20]; KAPING[62]; Template Prompting HiPSTG[30]; LLM2LLM[37]; LLM-DA[35]; TAPP[40]; PromptMix[38]; LLM-powered[1];WANLI[22]; Unnatural-instructions[39]; X-GEAR [41] Retrieval-based Augmentation"
  },
  {
    "text": "Sparse Retrieval BM25 AugmentedSBERT [47]; ALCE [63]; UniMS-RAG [65] TF-IDF CGRG [54]; UniMS-RAG [65] Dense Retrieval ANCE RetGen [7] Contriever LAPDOG [59] Poly-encoder EDGE [52] S-BERT EAE-RAG [56]; RGQA [53] RoBERTa Efficient-RAG [58] SimCSE zicl [48]; DAICL [61] DPR IM-RAG [55]; ALCE [63] TAS-B RADA [64] GTR ALCE [63] Graph-based Retrieval Personae-DA [60]; KAPING [62] Search Engine Retrieval Wikipedia Search DialogGen [50] APIs ChatPLUG [51]; Internet-Aug [49]; SeeKeR [57]; ReAct [66]; QA-Internet [8] Hybrid Augmentation DAICL [61]; KAPING [62]; QA-Internet [8]; SeeKeR [57]; ALCE [63]; KAPING [62]; RADA[64]; ReAct [66]; UniMS-RAG [65] Figure 4: Detailed data augmentation methods for four"
  },
  {
    "text": "techniques. For a better understanding, the grey font means that the paper is from Hybrid Augmentation, and we could see from the figure how Hybrid Augmentation designs the prompt and performs the retrieval. ploited query generation in information retrieval by providing few-shot examples for prompt-based LLMs to improve retrieval performance significantly, such as InPars [42] provides three examples of few-shot learning for the language model to gen- erate document-related queries, then acquires the most likely optimal documents for each query. Promptagator [44] directs LLMs with up to 8 examples in the prompt to produce a massive set of synthesized query-document"
  },
  {
    "text": "pairs. Cloze Prompting involves filling in blanks in a sentence or context and operates similarly to masked language models known as BERT [68]. Masking part of the input text and applying the model to infer the masked part based on the context. FlipDA [4] forms a new sample by randomly masking a fixed percentage of the input contents and then employing a pre-trained language model to fill in the cloze blanks. SUN- GEN [31] leverages prompts that are composed of <mask> and <mask> position will be replaced with the label word. GENIUS [5] produces new samples based on the sketch"
  },
  {
    "text": "of training samples. It fills in blanks [M] with several words or long spans while keeping the main parts of the sketch in the synthetic text, ensuring that the augmented text does not have significant semantic deviations from the initial text."
  },
  {
    "text": "2) Multi-step Prompts: To reduce the errors generated by the language model, unlike Single-step Prompts that ask the model to produce a direct response, Multi-step Prompts direct the generative model to provide a desired response step by step. Chain-of-Thought Prompting (CoT Prompting) guides LLMs to reason step by step to solve complex tasks [80]. A typical prompt contains a structure similar to let s think step by step. LLM-PTM [32] proposes implementing Chain-of- Thought Prompting to direct LLMs in creating additional data points while maintaining semantic consistency in the inclusion and exclusion criteria of the original trials. LLM Assisted [3]"
  },
  {
    "text": "employs CoT Prompting to rewrite the source input content and gradually instruct LLMs for dependency tree generation. ConvAug [43] introduces a three-step prompting approach motivated by the theory of human cognition to reduce halluci- nations and improve data quality. 3) Structured Prompts: Structured Prompts explicitly pro- vide the structure of the output content in the prompt template to adapt to more specific structured tasks. This ensures the model produces the expected content and complies with the standard framework. Role Prompting assigns specific roles or personas to the model, devising its response style and content based on prede- fined characteristics. AugESC"
  },
  {
    "text": "[23] uses a task description and role prompts of Human and AI to distinguish between seekers and supporters. In the following dialogue completion process, the model receives the task description and a starting phrase starting with Human. Then, it is fed with the next AI. Finally, create the following dialogue. Inspired by LLM s self-criticism ability, ICLEF [34] provides ChatGPT with a small amount of expert revision and asks it to behave as an annotator, criticizing its output results. Tuple Prompting structures prompt as tuples or triples in a structured data pairs format to guide the model better understanding user"
  },
  {
    "text": "s intents [87]. GPT3Mix [20] uses the triple consisted of Text Type T, Label Type L, and Label-token Verbalizer v: Y V to create the task specification S = (T, L, v). Each task has a task description and is used to construct prompts. Template Prompting utilizes structured templates that guide the model to respond according to a designed format or instruction. LLM-DA [35] provides LLMs with given sen- tences and entities and requires LLMs to construct the ex- pected format. After giving the definitions of age limit and atm support, PromptMix [38] requires the LLMs to produce utterances composed"
  },
  {
    "text": "of different proportions of the age limit and atm support. Unnatural-instructions [39] constructs well- defined input formats and applies them to the article summa- rization task. LLM2LLM [37] gives detailed requirements in the prompt and asks the model to return a structured response. Other studies, such as WANLI [22] and LLM-powered [1], also adopt a predefined template to instruct LLMs to generate new data. C. Retrieval-based Augmentation Although LLMs have shown satisfactory capabilities in many fields, they inevitably suffer from producing hallucinations and being unable to use external information [8]. Retrieval-based Augmentation effectively overcomes some existing limitations of LLMs and"
  },
  {
    "text": "provides an innovative way for data augmentation by retrieving enormous and dynamic knowledge from corpus bases or external documents [55]. Recently, many studies have used Retrieval-augmented Generation (RAG) [81] to obtain external timely updated information and achieved exceptional performance on different tasks. Retrieval-based Augmentation methods could be categorised into Sparse Retrieval, Dense Retrieval, Graph-based Retrieval and Search Engine Retrieval. 1) Sparse Retrieval: Sparse Retrieval is a traditional infor- mation retrieval method that relies on explicit word matching in documents. The most commonly used models include TF-IDF [88] and BM25 [89]. TF-IDF [88] computes text relevance by calculating the word s"
  },
  {
    "text": "frequency weights in the document and the whole corpus [82]. CGRG [54] employs IDF-weighted to rank grounding information from external knowledge sources to allow the model to create answers consistent with the facts. The main idea of using IDF-weighted is to measure word overlaps and select words with high overlaps and IDF weights. BM25 [89] is an improved version of TF-IDF, which intro- duces a document length normalisation factor [82]. Augment- edSBERT [47] uses BM25 to sample similar sentences from unlabeled sentence pairs to form a silver dataset and then merge the silver and gold datasets for prediction. 2) Dense"
  },
  {
    "text": "Retrieval: Dense Retrieval maps the query and documents into the same continuous vector space and calculates the distance between the vectors to measure the similarity. Compared to Sparse Retrieval, Dense Retrieval is superior at capturing semantic information [82]. DPR [90] uses a dual-tower structure to encode queries and documents separately. It obtains semantic representation based on BERT structure and optimises the model through comparative learning [90]. IM-RAG [55] implements DPR to embed documents and then uses the FAISS library [91] for fast semantic similarity retrieval. ANCE [92] is a dense retrieval model that uses an asyn- chronously updated pool of"
  },
  {
    "text": "negative samples for contrastive learning, which improves the ability to distinguish negative samples [92]. RetGen [7] adopts ANCE as the dense retrieval model and initialises the dense retriever with ANCE. SimCSE [93] is a sentence embedding model trained by contrast learning. It improves the semantic representation of sentences by maximising the similarity of positive sample pairs and minimising the similarity of negative sample pairs using both labelled and unlabelled data [93]. zicl [48] proposes to retrieve sentences with similar input distributions to the test input by calculating the cosine similarity between two different sentence embeddings using SimCSE. Poly-encoder [94] captures"
  },
  {
    "text": "the multilayer semantics of an input text by using multiple fixed numbers of context vectors,"
  },
  {
    "text": "Table III: Data Augmentation tasks, sub-tasks, and datasets. Tasks Sub-tasks Datasets Methods Text Classification Sentiment Classification CR, Yelp, Yelp5, Tweet-Eval, MR zicl [48] SST-2 TransformersDA [10],GPT3Mix [20],LLM2LLM [37], DAIL [25], ZeroShotDataAug [28], SUNGEN [31],GENIUS [5], zicl [48] IMDb DAGAM [11],SUNGEN [31],GENIUS [5] Amazon AugGPT [2],SUNGEN [31], DAICL [61], zicl [48] Twitter Complaints PromptMix [38] Intent Classification SNIPS TransformersDA [10],DA-intent [21], LLM2LLM [37],ZeroShotDataAug [28] Banking DA-intent [21],PromptMix [38] Question Classification TREC TransformersDA [10],GPT3Mix [20], DAGAM [11],LLM2LLM [37], LAMBADA [16] ZeroShotDataAug [28],PromptMix [38] Quora-QP AugmentedSBERT [47] Topic Classification AGnews DAGAM [11],DAIL [25],SUNGEN [31] Yahoo DAIL [25],GENIUS [5] Subjectivity Classification Subjectivity SUNGEN [31],PromptMix"
  },
  {
    "text": "[38] Text Generation Reviews Generation Yelp Reviews GenAug [12] Machine Translation AI-hub corpus DA-NMT [26] newstest2014 LeCA [17] FLORES-200 EPA [27] Paraphrasing Quora Question Pairs (QQP) EPA [27] Dialogue Generation Reddit CGRG [54], RetGen [7] Dailydialog, fraudulent e-mails EDGE [52] DuLeMon,KBP UniMS-RAG [65] Wizard of the Internet Internet-Aug [49] Wizard-of-Wikipedia DialogGen [50] Common Document Corpus, Social Media Data, Benchmark Tasks ChatPLUG [51] MultiWOZ 2.1 Efficient-RAG [58] CONVAI2 LAPDOG [59] ESConv AugESC [23] MultiWOZ AuGPT [13] PersonaChat,Empathetic Dialogues, Blended Skill Talk,Multi-Session Chat SeeKeR [57] Schema Guided Dialog Synthetic-DA [36] Dialogue Summarization MIMIC-III MTSamples Dialogue-Convert [29] SAMSum EPA [27] Information Extraction Event"
  },
  {
    "text": "Argument Extraction WikiEvents EAE-RAG [56], RGQA [53] ACE 2005 X-GEAR [41], RGQA [53] RAMS EAE-RAG [56] Named Entity Recognition OntoNotes 5.0,MIT-Movie,FEW-NERD LLM-DA [35] CoNLL03 GENIUS [5], LLM-DA [35], DAICL [61] WNUT, FIN, BC2GM , BioNLP09, BC5CDR DAICL [61] Question Answering Single-hop TechQA MRC-QA [19], Read-Com [24], RADA [24] PolicyQA MRC-QA [19], Read-Com [24], RADA [24] CovidQA Read-Com [24], RADA [64] BoolQ FlipDA [4] SQUAD Generative-DA [33], GENIUS [5] NewWiki,NYT,Amazon Generative-DA [33] WebQuestionsSP KAPING [62] Natural Questions QA-Internet [8] Multi-hop QA ASQA, QAMPARI, ELI5 ALCE [63] Reddit Generative-DA [33] Mintaka KAPING [62] HotPotQA IM-RAG [55], ReAct [66], QA-Internet [8] STRATEGYQA QA-Internet"
  },
  {
    "text": "[8] MultiRC, ReCoRD FlipDA [4] Natural Language Inference Multi-task NLI MultiNLI WANLI [22], EPA [27] Multilingual Commonsense Reasoning XCOPA, XWinograd, XStoryCloze LLM-powered [1] Multiple-choice Reasoning GSM8K, CaseHOLD LLM2LLM [37] Textual Entailment SNLI EPA [27], G-DAUGc [18] Word Relation Reasoning SUPERNI TAPP [40] Fact Verification FEVER QA-Internet [8] Commonsense Reasoning COMMONSENSEQA, WINOGRANDE, CODAH and HellaSwag G-DAUGc [18] Semeval-2012 FlipDA [4] Information Retrieval MARCO, TREC-DL, Robust04, Natural Questions, TREC-COVID InPars [42] BEIR (exclude MS MARCO, NQ and Quora) Promptagator [44] QReCC, TopiOCQA, CAsT-20 and CAsT-21 ConvAug [43] NTCIR and ACODAR DAPDR [45] AOL search log and Tiangong-ST COCA [14] se LoTTE, BEIR,"
  },
  {
    "text": "NQ, and SQuAD UDAPDR [46] Regression STS,BWS AugmentedSBERT [47]"
  },
  {
    "text": "which can efficiently represent different information in long texts [94]. EDGE [52] implements a poly-encoder model to perform retrieval and then choose the highest-ranked exemplar responses. S-BERT [95] is a BERT-based model specialised for sen- tence embedding generation. It captures the semantic similarity between sentences by applying a dual-encoder architecture [95]. Compared with traditional BERT, S-BERT significantly improves the efficiency of similarity computation tasks. EAE- RAG [56] leverages S-BERT to retrieve top-K ranked docu- ments with the highest relevance to the original input document from the training corpus and uses the retrieved documents as a set of discrete demonstrations. RGQA"
  },
  {
    "text": "[53] first introduces three additional tokens [demo], [tgr], and [sep_arg] to obtain distinct parts of the final input sequence. [demo] represents the most significant demonstration of the question and input examples. Then, it utilizes S-BERT and computes the similarity scores to retrieve the demonstration. Contriever [96] is a retrieval model pre-trained with un- supervised data using a dual encoder architecture. It encodes query and document independently and obtains the similarity between query and document usually by calculating the dot product of their corresponding outputs [96]. LAPDOG [59] implements Contriever as the retriever to embed query and story corpus separately. Then,"
  },
  {
    "text": "the retriever computes the dot product similarity between the query and each story to obtain stories with the top-K ranked similarity scores. RoBERTa [97] could be used as an encoder to encode queries and documents. It converts text into vector represen- tations and calculates the similarity between these vectors to find the most relevant documents to the query from a large-scale document collection. Efficient-RAG [58] proposes to utilize a siamese network structure with two encoders to encode dialogue context and knowledge snippets separately and establish a suitable ranking function using the distance between them. It uses pre-trained RoBERTa [97] models"
  },
  {
    "text": "as the encoders. TAS-B [98] combines BERT dense representation and sparse features to optimise long text retrieval through multi-level semantic aggregation and contrastive learning [98]. Detailed methods will be introduced in Section III-D2. GTR [99] is a dense retrieval method based on the T5 [70]. It converts queries and documents into dense vectors and identifies candidate documents by calculating the similarity to achieve efficient information retrieval [99]. In Section III-D2, we will introduce how the recent paper uses GTR for retrieval. 3) Graph-based Retrieval: Graph-based Retrieval typically utilizes graph structures to retrieve documents and queries. To improve LLM s performance,"
  },
  {
    "text": "Personae-DA [60] introduces a new memory framework in the information retrieval model to reflect how humans dynamically access memory during interactive processes and use a memory model with an adaptive graph-based architecture to obtain relevant retrieved data. 4) Search Engine Retrieval: Search Engine Retrieval is a technique for retrieval-related tasks through external knowledge bases. The model s output may be out of control or inaccurate without providing external knowledge [54]. Retrieving external documents and injecting grounding knowledge into the model helps the model generate faithful and context-aware responses. Wikipedia Search is a system designed for searching within the Wikipedia database."
  },
  {
    "text": "DialogGen [50] employs the query generator to generate queries based on the conversation context. This method then implements Wikipedia Search 1 as the search engine to acquire significant articles. The response generator generates responses based on the retrieved articles and the conversation context. APIs return search results by calling interfaces provided by a particular platform or service (e.g. Google, Bing, and Wikipedia). ChatPLUG [51] accesses the internet-relevant knowledge by Quark Search API 2 based on the conversation context. Internet-Aug [49] first employs the Bing Search API to generate a URL list for each query, then retrieves up-to- the-minute relevant information"
  },
  {
    "text": "based on these URLs, and the retrieved knowledge is prepended to the conversation history. ReAct [66] performs retrieval by the Wikipedia web API to search for an entity and return the first five sentences of the corresponding entity s wiki page if it exists. Given the synthesised query, SeeKeR [57] adopts the Bing Web Search API 3 to retrieve documents. The retrieved document is used to produce a knowledge response composed of relevant sentences or phrases. D. Hybrid Augmentation The combination of constructing prompts and retrieval com- ponents not only stimulates LLMs to produce diverse datasets but also exploits retrievers"
  },
  {
    "text": "to obtain timely updated information. Hybrid Augmentation technique is shown in a grey font in Figure 4. There could be many different combinations to construct prompts and perform retrieval. The following will introduce the various combinations of the papers covered in this survey. 1) Single-step Prompts and Sparse Retrieval: UniMS-RAG [65] adopts existing methods such as TF-IDF and BM25 to obtain similarity scores between dialogue context and evidence. Motivated by recent studies that use LLMs to predict similarity relationships between queries and evidence, it provides dialogue context and multiple pieces of evidence to prompt ChatGPT [73] to predict similarity scores"
  },
  {
    "text": "in a zero-shot setting. ALCE [63] employs BM25 to retrieve relevant passages from an exten- sive collection of documents and returns candidate passages. It constructs two in-context demonstrations in input prompts and applies them to an LLM to interact with the sparse retriever. 2) Single-step Prompts and Dense Retrieval: For the senti- ment analysis task, DAICL [61] leverages SimCSE Roberta as the retrieval model to retrieve examples semantically similar to the input sentence from the target unlabeled corpus as the con- text of the source query. Then, it concatenates the source query and the retrieved context as input few-shot demonstrations"
  },
  {
    "text": "for domain-adaptive in-context learning. To acquire examples 1https://en.wikipedia.org/wiki/Special:Search 2https://quark.sm.cn/ 3www.microsoft.com/en-us/bing/apis/bing-web-search-api"
  },
  {
    "text": "similar to the original samples, RADA [64] implements Distil- Bert TAS-B to retrieve examples from external resources and leverages the retrieved samples to create new input-output pairs in the few-shot in-context setting. Given a question, ALCE [63] first employs GTR [99] and DPR [90] to generate content while providing related cited passages from a vast retrieval corpus. Followed by prompting an LLM to create and cite related evidence without fine-tuning the model s internal parameters. 3) Single-step Prompts and Search Engine Retrieval: QA- Internet [8] applies the question verbatim as a query and leverages Google Search API 4 to extend"
  },
  {
    "text": "knowledge of the model by retrieving a set of documents for each query. Followed by adopting k-shot prompting and the retrieved paragraphs for question-answering scenarios. 4) Multi-step Prompts and Search Engine Retrieval: ReAct [66] randomly selects cases from the training set and manually constructs trajectories that compose diverse thought-action- observation processes. It performs retrieval by the Wikipedia web API to search for an entity and return the first five sentences of the corresponding entity s wiki page if it exists. 5) Structured Prompts and Graph-based Retrieval: KAP- ING [62] uses MPNet [100] as the symmetric retriever and applies TAS-B [98]"
  },
  {
    "text": "as the asymmetric retriever to retrieve only the related facts from the external Knowledge Graph. Then, it augments them to the prompt composed of relevant fact triples to produce a grounded response. IV. POST-PROCESSING APPROACHES The augmented data by LLMs is not fully guaranteed to be valid for training, and it is crucial to refine the generated data and further ensure data quality and relevance [1]. We catego- rize the post-processing approaches into consistency measures, filtering techniques, heuristic methods, and human involvement. A. Consistency Measures Consistency measures ensure that the generated data is logically and semantically consistent with the original"
  },
  {
    "text": "data. LAMBADA [16] ranks the generated sentences for the same label by calculating the confidence score and keeping the top- ranked sentences. After developing a set of new examples, LLM-powered [1] performs post-processing by only adding efficient and consistent output samples to the training set. Promptagator [44] and Generative-DA [33] clean the synthetic data by employing round-trip consistency. Many studies intro- duce multiple reranking strategies to refine the response. ALCE [63] utilizes the citation recall store as a rerank strategy to further improve the output s quality. Aiming to get the top- ranked supporting knowledge, IM-RAG [55] leverages a refiner"
  },
  {
    "text": "to retain significant information of the retriever s output. 4https://developers.google.com/custom-search B. Filtering Techniques Filtering techniques are used to retain generated data that is helpful to the model and to filter redundant or invalid data. G- DAUGc [18] applies influence functions to filter out detrimental training examples. LLM-DA [35], Unnatural-instructions [39] and RGQA [53] remove non-qualified augmented content by implementing specific filtering metrics. To select high-quality synthetic queries, UDAPDR [46] applies a filter to refine generated queries that could return its gold passage among the first 20 of the retrieved results. To further enhance the quality of generated data, Read-Com [24]"
  },
  {
    "text": "harnesses a round-trip filtration strategy. LLM2LLM [37] and DAPDR [45] harness similarity- based filtering approaches, which specifically use ROUGE filter and compute cosine similarity, respectively. Aiming to ensure the LLM could generate examples for the majority class, PromptMix [38] selects top-5 classes based on the similarity between the synthetic examples and current examples. MRC- QA [19] only retains the documents related to answer spans that obtain the top-K highest scores. To reduce the number of retrieved factual triples, KAPING [62] only keeps the top-K relevant triples to the question. C. Heuristic Methods Heuristic methods avoid common problems by adopting specific"
  },
  {
    "text": "rules or principles. AugESC [23] discards undesirable generated content based on designed heuristics in the last post- processing stage. WANLI [22] firstly filters failure examples by applying heuristics and then calculates the estimated max variability for the rest of the examples. D. Human Involvement Human revision plays a critical role in refining augmented datasets. WANLI [22] recruits crowd-workers to review unla- beled examples and remove offensive examples. The practical application of post-processing approaches im- proves the quality of the augmented datasets and enhances the overall performance of language models [46], making them more robust and reliable. V. TASKS AND EVALUATION"
  },
  {
    "text": "A. Tasks More recently, the parameters of deep neural network models have significantly increased, and the quantity and quality of data have become an essential role in the model s training process and received significant attention. Large-scale language models alleviate data scarcity by generating more augmented datasets with similar distribution to the original data [72]. Various data augmentation techniques extend existing text data and significantly improve the performance of natural language processing (NLP) tasks. Common NLP tasks include Informa- tion Extraction, Question Answering, and Text Classification [101]."
  },
  {
    "text": "Table IV: Evaluation Metrics of four Data Augmentation Techniques. DA techniques Evaluation Metrics Methods Simple Augmentation Automatic Evaluation Accuracy DAGAM [11], MRC-QA [19] Exact Match score MRC-QA [19] Recall AuGPT [13], MRC-QA [19] F1 Score AuGPT [13], Selection-DA [15], MRC-QA [19] Perplexity GenAug [12] BLEU GenAug [12], AuGPT [13], LeCA [17] Prompt-based Augmentation Automatic Evaluation Accuracy WANLI [22],LLM2LLM [37],FlipDA [4],GPT3Mix [20],DA-intent [21] DAIL [25],EPA [27],SUNGEN [31],PromptMix [38] ROUGE Dialogue-Convert [29],Synthetic-DA [36],EPA [27],TAPP [40] Exact Match score Read-Com [24],TAPP [40],Generative-DA [33] Recall DA-intent [21],GENIUS [5] F1 Score Dialogue-Convert [29],LLM-DA [35],X-GEAR [41],Read-Com [24] Generative-DA [33],EPA [27],FlipDA [4] Perplexity GENIUS [5] BLEU DA-NMT"
  },
  {
    "text": "[26],EPA [27] Human Evaluation Consistency AugESC [23] Coherence Informativeness Safety Retrieval-based Augmentation Automatic Evaluation Accuracy zicl [48] ROUGE LAPDOG [59] Exact Match score IM-RAG[55] Recall DialogGen [50] F1 Score EAE-RAG [56],X-GEAR [41],IM-RAG [55],AugmentedSBERT [47] Internet-Aug [49],DialogGen [50],Efficient-RAG [58],LAPDOG [59] Perplexity Internet-Aug [49],DialogGen [50] BLEU RetGen [7],Efficient-RAG [58],LAPDOG [59] Human Evaluation Consistency SeeKeR [57] Coherence ChatPLUG [51],RetGen [7] Informativeness ChatPLUG [51],RetGen [7] Safety ChatPLUG [51] Hallucination ChatPLUG [51] Knowledgeable Internet-Aug [49], SeeKeR [57] Engaging Internet-Aug [49] Hybrid Augmentation Automatic Evaluation Accuracy KAPING [62],ReAct[66],QA-Internet [8],DAICL [61] ROUGE UniMS-RAG[65] Exact Match score RGQA [53],ReAct [66],QA-Internet [8] Recall RGQA [53],ConvAug [43] F1 Score RGQA"
  },
  {
    "text": "[53],RADA [64],DAICL [61],UniMS-RAG [65] BLEU CGRG [54],UniMS-RAG [65] Human Evaluation Consistency SeeKeR [57] Coherence UniMS-RAG [65] Fluency ALCE [63] Factually Correct ALCE [63] In addition to typical NLP tasks, LLMs have been employed to improve the accuracy and new domain adaptability of infor- mation retrieval (IR) tasks [46]. In information retrieval, given a query, candidate documents related to the query are retrieved in the document set. A widely used method in the context of LLMs for IR tasks is query generation for data augmentation, which applies LLMs to generate queries or query-document pairs [45]. The synthetic queries could help the"
  },
  {
    "text": "retrieval model adapt to targeted domains in multiple tasks. InPars [42], Promptagator [44], UDAPDR [46] and DAPDR [45] improve the performance of passage and document retrieval tasks by enhancing queries. Besides, COCA [14] and ConvAug [43] utilize contrastive learning to obtain augmented data in document ranking and conversational dense retrieval tasks, respectively. Moreover, With the powerful generation capability of LLMs, incorporating a retrieval module to retrieve relevant information from large-scale corpora demonstrates excellent superiority in the question answering (QA) and dialogue generation tasks [81]. The broad application of data augmentation techniques ef- fectively alleviates data scarcity issues and promotes the"
  },
  {
    "text": "break- through and development of LLMs in diverse NLP tasks. Table III shows the popular tasks of data augmentation in recent research. B. Evaluation Metrics Automatic and human evaluation are two main perspec- tives on a model s performance. Automatic evaluation is more suitable when the task contains a numerous dataset. Typical automatic evaluation includes accuracy [22], [37], [4], exact match score [33], [19] and F1 Score [35], [41], [24]. In contrast to automatic evaluation, human evaluation could deeply analyse the model s output through the involvement of crowd workers or experts. Common human evaluation criteria"
  },
  {
    "text": "include consistency [23], [65], [54], [57], coherence [65], [51] and informativeness [23], [51]. Human evaluation could provide a more comprehensive understanding of the model s perfor- mance, but it could be time-consuming and labour-intensive on massive datasets. The detailed evaluation of data augmentation studies is illustrated in table IV. VI. CHALLENGES AND OPPORTUNITIES Although data augmentation has showcased excellent perfor- mance in many studies and tasks, it still faces challenges and limitations. This chapter underscores some directions that could be investigated in greater depth for future research. A. The Quality and Diversity of Generated Data The prerequisite for effective data"
  },
  {
    "text": "augmentation is the valid- ity of the generated data. Even though generating semantically rich data that is highly relevant to seed training data [30], [31], [1] and employing various filtering strategies to filter out repet- itive and irrelevant content generated by LLMs could address the limitations of the low-quality augmented datasets [18], [35], [53]. However, there is no exact solution to determine how much synthetic data is efficient and how to ensure that the synthesised data is helpful to improve the performance of the model [44]. B. Tasks Adaptation Current experiments mainly focused on a single task, such as classification"
  },
  {
    "text": "tasks. Inputs and test inputs need more flexible options for sharing between diverse tasks [48]. Specifically, the issues of adapting the model to different tasks and correctly evaluating the model s output on distinct tasks are unresolved [56]. C. Reducing Hallucination LLMs transformative generative ability has brought enor- mous benefits to both academic and industrial, but LLMs unavoidably produce factually incorrect responses and con- tradictory content [57]. Many studies propose to mitigate the hallucination by retrieving external and up-to-date knowledge and improving the accuracy of the generated content by retriev- ing relevant articles and providing citations for LLMs [63]. Another"
  },
  {
    "text": "popular approach is to incorporate grounded truths through multiple APIs [66], [57], [51], search engines [50], and modulars [57], thereby alleviating hallucination. D. Retrieval Dependencies During the retrieval process, the model s performance de- pends to some extent on the quality of the retrieved data and the relevance between external and existing data [64]. If the retriever fails to retrieve related grounded facts, the model may produce unfaithful answers [62], [64]. E. Large Number of Parameters and High Training Cost LLMs have numerous training parameters, which take up a significant amount of GPU resources during the training process. The inference"
  },
  {
    "text": "procedure also requires high computa- tional and storage costs [53], [64]. Future work could explore innovative methods to better transfer knowledge from large- scale language models to smaller language models. Investigate how to effectively integrate various domain knowledge into the model to improve its adaptability to complex tasks rather than increasing the model parameters. F. Ethics and Potential Risks LLMs may pose uncontrolled risks [54], as their synthesised content may contain sensitive and private information [43]. In addition, LLMs have inherited biases regarding specific topics, which may produce harmful content [46]. Applying text data augmentation techniques in large lan- guage"
  },
  {
    "text": "models still faces unresolved challenges. Future research could focus on optimising the quality of the generated data, improving the model s adaptability to different tasks, and reducing the training cost to further promote the effectiveness and development of data augmentation. VII. CONCLUSION This survey classifies text data augmentation into four tech- niques: Simple Augmentation, Prompt-based Augmentation, Retrieval-based Augmentation, and Hybrid Augmentation. Each technique is further categorised according to its unique charac- teristics. Providing crafted prompt templates to large language models has shown prominent performance in data augmen- tation. Combining external data with existing data through a retriever offers more possibilities"
  },
  {
    "text": "for cross-domain tasks. Data augmentation indeed contributes to expanding the dataset and generating diverse content, but more techniques are needed to test the validity and factuality of the generated data. The continuous development and improvement of LLMs makes them increasingly effective in data augmentation and deserves continued exploration in the future. REFERENCES [1] C. Whitehouse, M. Choudhury, and A. F. Aji, Llm-powered data augmentation for enhanced cross-lingual performance, in Proceedings of the 2023 Conference on Empirical Methods in Natural Language Pro- cessing, EMNLP 2023, Singapore, December 6-10, 2023. Association for Computational Linguistics, 2023, pp. 671 686. [2] H. Dai, Z."
  },
  {
    "text": "Liu, W. Liao, X. Huang, Y. Cao, Z. Wu, L. Zhao, S. Xu, W. Liu, N. Liu et al., Auggpt: Leveraging chatgpt for text data augmentation, arXiv preprint arXiv:2302.13007, 2023. [3] M. Zhang, G. Jiang, S. Liu, J. Chen, and M. Zhang, LLM Assisted Data Augmentation for Chinese Dialogue Level Dependency Parsing, 2024. [4] J. Zhou, Y. Zheng, J. Tang, L. Jian, and Z. Yang, Flipda: Effective and robust data augmentation for few-shot learning, in Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022. Association for"
  },
  {
    "text": "Computational Linguistics, 2022, pp. 8646 8665."
  },
  {
    "text": "[5] B. Guo, Y. Gong, Y. Shen, S. Han, H. Huang, N. Duan, and W. Chen, GENIUS: sketch-based language model pre-training via extreme and selective masking for text generation and augmentation, CoRR, vol. abs/2211.10330, 2022. [6] A. Patel, B. Li, M. S. Rasooli, N. Constant, C. Raffel, and C. Callison- Burch, Bidirectional language models are also few-shot learners, in The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. [7] Y. Zhang, S. Sun, X. Gao, Y. Fang, C. Brockett, M. Galley, J. Gao, and B. Dolan, Retgen: A joint framework for retrieval and"
  },
  {
    "text": "grounded text generation modeling, in Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, Thirty-Fourth Conference on Innovative Appli- cations of Artificial Intelligence, IAAI 2022, The Twelveth Symposium on Educational Advances in Artificial Intelligence, EAAI 2022 Virtual Event, February 22 - March 1, 2022. AAAI Press, 2022, pp. 11 739 11 747. [8] A. Lazaridou, E. Gribovskaya, W. Stokowiec, and N. Grigorev, Internet- augmented language models through few-shot prompting for open- domain question answering, CoRR, vol. abs/2203.05115, 2022. [9] Y. Gao, Y. Xiong, X. Gao, K. Jia, J. Pan, Y. Bi, Y. Dai, J. Sun, M. Wang, and H. Wang, Retrieval-Augmented"
  },
  {
    "text": "Generation for Large Language Models: A Survey, Mar. 2024. [10] V. Kumar, A. Choudhary, and E. Cho, Data augmentation using pre- trained transformer models, CoRR, vol. abs/2003.02245, 2020. [11] B. Jo, T. Heo, Y. Park, Y. Yoo, W. Cho, and K. Kim, DAGAM: data augmentation with generation and modification, CoRR, vol. abs/2204.02633, 2022. [12] S. Y. Feng, V. Gangal, D. Kang, T. Mitamura, and E. H. Hovy, Genaug: Data augmentation for finetuning text generators, CoRR, vol. abs/2010.01794, 2020. [13] J. Kulh anek, V. Hude cek, T. Nekvinda, and O. Du sek, Augpt: Auxiliary tasks and data augmentation for end-to-end dialogue"
  },
  {
    "text": "with pre-trained language models, arXiv preprint arXiv:2102.05126, 2021. [14] Y. Zhu, J. Nie, Z. Dou, Z. Ma, X. Zhang, P. Du, X. Zuo, and H. Jiang, Contrastive learning of user behavior sequence for context- aware document ranking, in CIKM 21: The 30th ACM International Conference on Information and Knowledge Management, Virtual Event, Queensland, Australia, November 1 - 5, 2021. ACM, 2021, pp. 2780 2791. [15] A. Edwards, A. Ushio, J. Camacho-Collados, H. de Ribaupierre, and A. D. Preece, Guiding generative language models for data augmenta- tion in few-shot text classification, CoRR, vol. abs/2111.09064, 2021. [16] A. Anaby-Tavor, B. Carmeli, E."
  },
  {
    "text": "Goldbraich, A. Kantor, G. Kour, S. Shlomov, N. Tepper, and N. Zwerdling, Do not have enough data? deep learning to the rescue! in The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020. AAAI Press, 2020, pp. 7383 7390. [17] G. Chen, Y. Chen, Y. Wang, and V. O. K. Li, Lexical-constraint-aware neural machine translation via data augmentation, in Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI 2020. ijcai.org,"
  },
  {
    "text": "2020, pp. 3587 3593. [18] Y. Yang, C. Malaviya, J. Fernandez, S. Swayamdipta, R. L. Bras, J. Wang, C. Bhagavatula, Y. Choi, and D. Downey, G-daug: Generative data augmentation for commonsense reasoning, in Findings of the Association for Computational Linguistics: EMNLP 2020, Online Event, 16-20 November 2020, ser. Findings of ACL, vol. EMNLP 2020. Association for Computational Linguistics, 2020, pp. 1008 1025. [19] H. Van, V. Yadav, and M. Surdeanu, Cheap and good? simple and effective data augmentation for low resource machine reading, in SIGIR 21: The 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, Virtual"
  },
  {
    "text": "Event, Canada, July 11- 15, 2021. ACM, 2021, pp. 2116 2120. [20] K. M. Yoo, D. Park, J. Kang, S. Lee, and W. Park, Gpt3mix: Leveraging large-scale language models for text augmentation, in Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 16-20 November, 2021. Association for Computational Linguistics, 2021, pp. 2225 2239. [21] G. Sahu, P. Rodr guez, I. H. Laradji, P. Atighehchian, D. V azquez, and D. Bahdanau, Data augmentation for intent classification with off-the- shelf large language models, in Proceedings of the 4th Workshop on NLP for Conversational AI, ConvAI@ACL"
  },
  {
    "text": "2022, Dublin, Ireland, May 27, 2022. Association for Computational Linguistics, 2022, pp. 47 57. [22] A. Liu, S. Swayamdipta, N. A. Smith, and Y. Choi, WANLI: worker and AI collaboration for natural language inference dataset creation, in Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022. As- sociation for Computational Linguistics, 2022, pp. 6826 6847. [23] C. Zheng, S. Sabour, J. Wen, Z. Zhang, and M. Huang, Augesc: Dialogue augmentation with large language models for emotional sup- port conversation, in Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July"
  },
  {
    "text": "9-14, 2023, A. Rogers, J. L. Boyd-Graber, and N. Okazaki, Eds. Association for Computational Linguistics, 2023, pp. 1552 1568. [24] V. Samuel, H. Aynaou, A. G. Chowdhury, K. V. Ramanan, and A. Chadha, Can llms augment low-resource reading comprehension datasets? opportunities and challenges, in Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics, ACL 2024 - Student Research Workshop, Bangkok, Thailand, August 11-16, 2024. Association for Computational Linguistics, 2024, pp. 411 421. [25] D. Li, Y. Li, D. Mekala, S. Li, Y. Wang, X. Wang, W. Hogan, and J. Shang, DAIL: data augmentation for in-context learning via"
  },
  {
    "text": "self- paraphrase, CoRR, vol. abs/2311.03319, 2023. [26] S. Oh, S. A. Lee, and W. Jung, Data augmentation for neural machine translation using generative language model, CoRR, vol. abs/2307.16833, 2023. [27] H. Lu and W. Lam, EPA: easy prompt augmentation on large lan- guage models via multiple sources and multiple targets, CoRR, vol. abs/2309.04725, 2023. [28] S. Ubani, S. O. Polat, and R. Nielsen, Zeroshotdataaug: Generating and augmenting training data with chatgpt, CoRR, vol. abs/2304.14334, 2023. [29] V. Schlegel, H. Li, Y. Wu, A. Subramanian, T. Nguyen, A. R. Kashyap, D. Beck, X. Zeng, R. T. Batista-Navarro, S. Winkler, and G."
  },
  {
    "text": "Nenadic, PULSAR at mediqa-sum 2023: Large language models augmented by synthetic dialogue convert patient dialogues to medical records, in Working Notes of the Conference and Labs of the Evaluation Forum (CLEF 2023), Thessaloniki, Greece, September 18th to 21st, 2023, ser. CEUR Workshop Proceedings, vol. 3497. CEUR-WS.org, 2023, pp. 1668 1679. [30] X. Cai, M. Xiao, Z. Ning, and Y. Zhou, Resolving the imbalance issue in hierarchical disciplinary topic inference via llm-based data augmentation, in IEEE International Conference on Data Mining, ICDM 2023, Shanghai, China, December 1-4, 2023. IEEE, 2023, pp. 956 961. [31] J. Gao, R. Pi, Y. Lin, H."
  },
  {
    "text": "Xu, J. Ye, Z. Wu, W. Zhang, X. Liang, Z. Li, and L. Kong, Self-guided noise-free data generation for efficient zero- shot learning, in The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. Open- Review.net, 2023. [32] J. Yuan, R. Tang, X. Jiang, and X. Hu, Large Language Models for Healthcare Data Augmentation: An Example on Patient-Trial Matching, Aug. 2023. [33] A. G. Chowdhury and A. Chadha, Generative data augmentation using llms improves distributional robustness in question answering, in Proceedings of the 18th Conference of the European Chapter of the As- sociation for Computational Linguistics,"
  },
  {
    "text": "EACL 2024: Student Research Workshop, St. Julian s, Malta, March 21-22, 2024. Association for Computational Linguistics, 2024, pp. 258 265. [34] A. Saakyan and S. Muresan, ICLEF: in-context learning with expert feedback for explainable style transfer, in Proceedings of the 62nd An- nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024. Association for Computational Linguistics, 2024, pp. 16 141 16 163. [35] J. Ye, N. Xu, Y. Wang, J. Zhou, Q. Zhang, T. Gui, and X. Huang, LLM- DA: data augmentation via large language models for few-shot named entity recognition,"
  },
  {
    "text": "CoRR, vol. abs/2402.14568, 2024. [36] J. Kaddour and Q. Liu, Synthetic Data Generation in Low- Resource Settings via Fine-Tuning of Large Language Models, no. arXiv:2310.01119, Jan. 2024. [37] N. Lee, T. Wattanawong, S. Kim, K. Mangalam, S. Shen, G. Anu- manchipalli, M. W. Mahoney, K. Keutzer, and A. Gholami, LLM2LLM: boosting llms with novel iterative data enhancement, in Findings of the Association for Computational Linguistics, ACL 2024, Bangkok,"
  },
  {
    "text": "Thailand and virtual meeting, August 11-16, 2024. Association for Computational Linguistics, 2024, pp. 6498 6526. [38] G. Sahu, O. Vechtomova, D. Bahdanau, and I. H. Laradji, Promptmix: A class boundary augmentation method for large language model distil- lation, in Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023. Association for Computational Linguistics, 2023, pp. 5316 5327. [39] O. Honovich, T. Scialom, O. Levy, and T. Schick, Unnatural in- structions: Tuning language models with (almost) no human labor, in Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume"
  },
  {
    "text": "1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023. Association for Computational Linguistics, 2023, pp. 14 409 14 428. [40] S. Ye, H. Hwang, S. Yang, H. Yun, Y. Kim, and M. Seo, Investi- gating the effectiveness of task-agnostic prefix prompt for instruction following, in Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver, Canada. AAAI Press, 2024, pp. 19 386 19 394. [41] K. Huang, I. Hsu, P. Natarajan, K. Chang, and N. Peng, Multilingual"
  },
  {
    "text": "generative language models for zero-shot cross-lingual event argument extraction, in Proceedings of the 60th Annual Meeting of the Asso- ciation for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022. Association for Computational Linguistics, 2022, pp. 4633 4646. [42] L. H. Bonifacio, H. Q. Abonizio, M. Fadaee, and R. F. Nogueira, In- pars: Data augmentation for information retrieval using large language models, CoRR, vol. abs/2202.05144, 2022. [43] H. Chen, Z. Dou, K. Mao, J. Liu, and Z. Zhao, Generalizing conversa- tional dense retrieval via llm-cognition data augmentation, in Proceed- ings of the 62nd Annual Meeting of"
  },
  {
    "text": "the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024. Association for Computational Linguistics, 2024, pp. 2700 2718. [44] Z. Dai, V. Y. Zhao, J. Ma, Y. Luan, J. Ni, J. Lu, A. Bakalov, K. Guu, K. B. Hall, and M. Chang, Promptagator: Few-shot dense retrieval from 8 examples, in The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. [45] L. de Souza Silva and L. Barbosa, Improving dense retrieval models with LLM augmented data for dataset search, Knowl. Based Syst., vol. 294, p. 111740, 2024. [46]"
  },
  {
    "text": "J. Saad-Falcon, O. Khattab, K. Santhanam, R. Florian, M. Franz, S. Roukos, A. Sil, M. A. Sultan, and C. Potts, UDAPDR: unsupervised domain adaptation via LLM prompting and distillation of rerankers, in Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023. Association for Computational Linguistics, 2023, pp. 11 265 11 279. [47] N. Thakur, N. Reimers, J. Daxenberger, and I. Gurevych, Augmented SBERT: data augmentation method for improving bi-encoders for pair- wise sentence scoring tasks, in Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational"
  },
  {
    "text": "Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021. Association for Computational Linguistics, 2021, pp. 296 310. [48] X. Lyu, S. Min, I. Beltagy, L. Zettlemoyer, and H. Hajishirzi, Z-ICL: zero-shot in-context learning with pseudo-demonstrations, in Proceed- ings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023. Association for Computational Linguistics, 2023, pp. 2304 2317. [49] M. Komeili, K. Shuster, and J. Weston, Internet-augmented dialogue generation, in Proceedings of the 60th Annual Meeting of the Asso- ciation for Computational Linguistics (Volume 1: Long Papers), ACL 2022,"
  },
  {
    "text": "Dublin, Ireland, May 22-27, 2022. Association for Computational Linguistics, 2022, pp. 8460 8478. [50] A. Wang, L. Song, Q. Liu, H. Mi, L. Wang, Z. Tu, J. Su, and D. Yu, Search-engine-augmented dialogue response generation with cheaply supervised query production, Artif. Intell., vol. 319, p. 103874, 2023. [51] J. Tian, H. Chen, G. Xu, M. Yan, X. Gao, J. Zhang, C. Li, J. Liu, W. Xu, H. Xu, Q. Qian, W. Wang, Q. Ye, J. Zhang, J. Zhang, F. Huang, and J. Zhou, Chatplug: Open-domain generative dialogue system with internet-augmented instruction tuning for digital human, CoRR, vol. abs/2304.07849, 2023. [52]"
  },
  {
    "text": "P. Gupta, J. P. Bigham, Y. Tsvetkov, and A. Pavel, Controlling dialogue generation with semantic exemplars, in Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL- HLT 2021, Online, June 6-11, 2021. Association for Computational Linguistics, 2021, pp. 3018 3029. [53] X. Du and H. Ji, Retrieval-augmented generative question answering for event argument extraction, in Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022. Association for Computational Linguistics, 2022, pp. 4649 4666. [54] Z. Wu, M."
  },
  {
    "text": "Galley, C. Brockett, Y. Zhang, X. Gao, C. Quirk, R. Koncel- Kedziorski, J. Gao, H. Hajishirzi, M. Ostendorf, and B. Dolan, A controllable model of grounded response generation, in Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021. AAAI Press, 2021, pp. 14 085 14 093. [55] D. Yang, J. Rao, K. Chen, X. Guo, Y. Zhang, J. Yang, and Y. Zhang, IM-RAG: multi-round retrieval-augmented generation through learning inner monologues, in Proceedings of the 47th"
  },
  {
    "text": "International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2024, Washington DC, USA, July 14-18, 2024. ACM, 2024, pp. 730 740. [56] Y. Ren, Y. Cao, P. Guo, F. Fang, W. Ma, and Z. Lin, Retrieve-and- sample: Document-level event argument extraction via hybrid retrieval augmentation, in Proceedings of the 61st Annual Meeting of the As- sociation for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023. Association for Computational Linguistics, 2023, pp. 293 306. [57] K. Shuster, M. Komeili, L. Adolphs, S. Roller, A. Szlam, and J. Weston, Language models that seek for"
  },
  {
    "text": "knowledge: Modular search & genera- tion for dialogue and prompt completion, in Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022. Association for Computational Linguistics, 2022, pp. 373 393. [58] D. Thulke, N. Daheim, C. Dugast, and H. Ney, Efficient retrieval augmented generation from unstructured knowledge for task-oriented dialog, CoRR, vol. abs/2102.04643, 2021. [59] Q. Huang, S. Fu, X. Liu, W. Wang, T. Ko, Y. Zhang, and L. Tang, Learning retrieval augmentation for personalized dialogue generation, in Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore,"
  },
  {
    "text": "December 6-10, 2023. Association for Computational Linguistics, 2023, pp. 2523 2540. [60] R. A. Gonzalez and S. DiPaola, Exploring augmentation and cognitive strategies for AI based synthetic personae, CoRR, vol. abs/2404.10890, 2024. [61] Q. Long, W. Wang, and S. J. Pan, Adapt in contexts: Retrieval- augmented domain adaptation via in-context learning, in Proceedings of the 2023 Conference on Empirical Methods in Natural Language Pro- cessing, EMNLP 2023, Singapore, December 6-10, 2023. Association for Computational Linguistics, 2023, pp. 6525 6542. [62] J. Baek, A. F. Aji, and A. Saffari, Knowledge-augmented language model prompting for zero-shot knowledge graph question answering, CoRR, vol."
  },
  {
    "text": "abs/2306.04136, 2023. [63] T. Gao, H. Yen, J. Yu, and D. Chen, Enabling large language models to generate text with citations, in Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023. Association for Computational Linguistics, 2023, pp. 6465 6488. [64] M. Seo, J. Baek, J. Thorne, and S. J. Hwang, Retrieval-augmented data augmentation for low-resource domain tasks, CoRR, vol. abs/2402.13482, 2024. [65] H. Wang, W. Huang, Y. Deng, R. Wang, Z. Wang, Y. Wang, F. Mi, J. Z. Pan, and K. Wong, Unims-rag: A unified multi-source retrieval- augmented generation for personalized"
  },
  {
    "text": "dialogue systems, CoRR, vol. abs/2401.13256, 2024. [66] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. R. Narasimhan, and Y. Cao, React: Synergizing reasoning and acting in language models, in The"
  },
  {
    "text": "Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. [67] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, Attention is all you need, in Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, 2017, pp. 5998 6008. [68] J. Devlin, M. Chang, K. Lee, and K. Toutanova, BERT: pre-training of deep bidirectional transformers for language understanding, in Pro- ceedings of the 2019 Conference of the North American Chapter of the Association"
  },
  {
    "text": "for Computational Linguistics: Human Language Technolo- gies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers). Association for Computational Linguistics, 2019, pp. 4171 4186. [69] A. Radford, Improving language understanding by generative pre- training, 2018. [70] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu, Exploring the limits of transfer learning with a unified text-to-text transformer, J. Mach. Learn. Res., vol. 21, pp. 140:1 140:67, 2020. [71] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov, and L. Zettlemoyer,"
  },
  {
    "text": "BART: denoising sequence-to- sequence pre-training for natural language generation, translation, and comprehension, in Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020. Association for Computational Linguistics, 2020, pp. 7871 7880. [72] L. Xu, H. Xie, S. J. Qin, F. L. Wang, and X. Tao, Exploring ChatGPT- based Augmentation Strategies for Contrastive Aspect-based Sentiment Analysis, Sep. 2024. [73] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P."
  },
  {
    "text": "Welinder, P. F. Christiano, J. Leike, and R. Lowe, Training language models to follow instructions with human feedback, in Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. [74] B. Ding, C. Qin, R. Zhao, T. Luo, X. Li, G. Chen, W. Xia, J. Hu, A. T. Luu, and S. Joty, Data Augmentation using LLMs: Data Perspectives, Learning Paradigms and Challenges, Mar. 2024. [75] OpenAI, GPT-4 technical report, CoRR, vol. abs/2303.08774, 2023. [76] H. Touvron, L. Martin, K. Stone, P. Albert,"
  },
  {
    "text": "A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. Canton-Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra, I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E."
  },
  {
    "text": "Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and T. Scialom, Llama 2: Open foundation and fine-tuned chat models, CoRR, vol. abs/2307.09288, 2023. [77] R. Sennrich, Improving neural machine translation models with mono- lingual data, arXiv preprint arXiv:1511.06709, 2015. [78] T. Schick and H. Sch utze, It s not just size that matters: Small language models are also few-shot learners, in Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human"
  },
  {
    "text": "Language Technologies, NAACL- HLT 2021, Online, June 6-11, 2021. Association for Computational Linguistics, 2021, pp. 2339 2352. [79] M. Joshi, D. Chen, Y. Liu, D. S. Weld, L. Zettlemoyer, and O. Levy, Spanbert: Improving pre-training by representing and predicting spans, Trans. Assoc. Comput. Linguistics, vol. 8, pp. 64 77, 2020. [80] P. Sahoo, A. K. Singh, S. Saha, V. Jain, S. Mondal, and A. Chadha, A systematic survey of prompt engineering in large language models: Techniques and applications, arXiv preprint arXiv:2402.07927, 2024. [81] P. S. H. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. K uttler,"
  },
  {
    "text": "M. Lewis, W. Yih, T. Rockt aschel, S. Riedel, and D. Kiela, Retrieval-augmented generation for knowledge-intensive NLP tasks, in Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. [82] X. Liu and W. B. Croft, Statistical language modeling for information retrieval, Annu. Rev. Inf. Sci. Technol., vol. 39, no. 1, pp. 1 31, 2005. [83] S. Rose, D. Engel, N. Cramer, and W. Cowley, Automatic keyword extraction from individual documents, Text mining: applications and theory, pp. 1 20, 2010. [84] G. A. Miller, Wordnet: a lexical database"
  },
  {
    "text": "for english, Communications of the ACM, vol. 38, no. 11, pp. 39 41, 1995. [85] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al., Language models are unsupervised multitask learners, OpenAI Blog, vol. 1, no. 8, p. 9, 2019. [86] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig, Pre- train, prompt, and predict: A systematic survey of prompting methods in natural language processing, ACM Comput. Surv., vol. 55, no. 9, pp. 195:1 195:35, 2023. [87] B. Chen, Z. Zhang, N. Langren e, and S. Zhu, Unleashing the potential of prompt"
  },
  {
    "text": "engineering in large language models: a comprehensive review, CoRR, vol. abs/2310.14735, 2023. [88] G. Salton and C.-S. Yang, On the specification of term values in automatic indexing, Journal of documentation, vol. 29, no. 4, pp. 351 372, 1973. [89] S. E. Robertson, S. Walker, S. Jones, M. M. Hancock-Beaulieu, M. Gat- ford et al., Okapi at trec-3, Nist Special Publication Sp, vol. 109, p. 109, 1995. [90] V. Karpukhin, B. Oguz, S. Min, P. S. H. Lewis, L. Wu, S. Edunov, D. Chen, and W. Yih, Dense passage retrieval for open-domain question answering, in Proceedings of the 2020 Conference on"
  },
  {
    "text": "Empirical Meth- ods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020. Association for Computational Linguistics, 2020, pp. 6769 6781. [91] J. Johnson, M. Douze, and H. J egou, Billion-scale similarity search with gpus, IEEE Trans. Big Data, vol. 7, no. 3, pp. 535 547, 2021. [92] L. Xiong, C. Xiong, Y. Li, K. Tang, J. Liu, P. N. Bennett, J. Ahmed, and A. Overwijk, Approximate nearest neighbor negative contrastive learning for dense text retrieval, in 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. [93] T. Gao, X. Yao, and D."
  },
  {
    "text": "Chen, Simcse: Simple contrastive learning of sentence embeddings, in Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021. Association for Computational Linguistics, 2021, pp. 6894 6910. [94] S. Humeau, K. Shuster, M.-A. Lachaux, and J. Weston, Poly-encoders: Architectures and pre-training strategies for fast and accurate multi- sentence scoring, in International Conference on Learning Representa- tions, 2019. [95] N. Reimers and I. Gurevych, Sentence-bert: Sentence embeddings using siamese bert-networks, in Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Inter-"
  },
  {
    "text": "national Joint Conference on Natural Language Processing, EMNLP- IJCNLP 2019, Hong Kong, China, November 3-7, 2019. Association for Computational Linguistics, 2019, pp. 3980 3990. [96] G. Izacard, M. Caron, L. Hosseini, S. Riedel, P. Bojanowski, A. Joulin, and E. Grave, Unsupervised Dense Information Retrieval with Con- trastive Learning, Aug. 2022. [97] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov, Roberta: A robustly optimized BERT pretraining approach, CoRR, vol. abs/1907.11692, 2019. [98] S. Hofst atter, S. Lin, J. Yang, J. Lin, and A. Hanbury, Efficiently teach- ing an"
  },
  {
    "text": "effective dense retriever with balanced topic aware sampling, in SIGIR 21: The 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, Virtual Event, Canada, July 11-15, 2021. ACM, 2021, pp. 113 122. [99] J. Ni, C. Qu, J. Lu, Z. Dai, G. H. Abrego, J. Ma, V. Y. Zhao, Y. Luan, K. B. Hall, M. Chang, and Y. Yang, Large dual encoders are generalizable retrievers, in Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022. Association for Computational Linguistics, 2022, pp. 9844 9855. [100]"
  },
  {
    "text": "K. Song, X. Tan, T. Qin, J. Lu, and T. Liu, Mpnet: Masked and permuted pre-training for language understanding, in Advances in Neu- ral Information Processing Systems 33: Annual Conference on Neural"
  },
  {
    "text": "Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. [101] X. Chen, H. Xie, S. J. Qin, Y. Chai, X. Tao, and F. L. Wang, Cognitive-inspired deep learning models for aspect-based sentiment analysis: A retrospective overview and bibliometric analysis, Cognitive Computation, pp. 1 39, 2024."
  }
]