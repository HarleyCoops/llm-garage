{
  "page_1": "2025-03-12\nGemma 3 Technical Report\nGemma Team, Google DeepMind1\nWe introduce Gemma 3, a multimodal addition to the Gemma family of lightweight open models, ranging\nin scale from 1 to 27 billion parameters. This version introduces vision understanding abilities, a wider\ncoverage of languages and longer context – at least 128K tokens. We also change the architecture of\nthe model to reduce the KV-cache memory that tends to explode with long context. This is achieved by\nincreasing the ratio of local to global attention layers, and keeping the span on local attention short.\nThe Gemma 3 models are trained with distillation and achieve superior performance to Gemma 2\nfor both pre-trained and instruction finetuned versions. In particular, our novel post-training recipe\nsignificantly improves the math, chat, instruction-following and multilingual abilities, making Gemma3-\n4B-IT competitive with Gemma2-27B-IT and Gemma3-27B-IT comparable to Gemini-1.5-Pro across\nbenchmarks. We release all our models to the community.\n1. Introduction\nWe present the newest version of Gemma open\nlanguage models (Gemma Team, 2024a), co-\ndesigned with the family of Gemini frontier mod-\nels (Gemini Team, 2023).\nThis new version\ncomes in sizes comparable to Gemma 2 (Gemma\nTeam, 2024b), with the addition of a 1B model.\nThese models are designed to run on standard\nconsumer-grade hardware such as phones, lap-\ntops, and high-end GPUs. This version comes\nwith several new abilities to the Gemma family;\nnamely, multimodality, long context, and mul-\ntilinguality, while preserving or surpassing the\nperformance of prior versions.\nIn terms of multimodality, most Gemma 3 mod-\nels are compatible with a tailored version of the\nSigLIP vision encoder (Zhai et al., 2023). The\nlanguage models treat images as a sequence of\nsoft tokens encoded by SigLIP. We reduce the in-\nference cost of image processing by condensing\nthe vision embeddings into a fixed size of 256\nvectors. The encoder works at a fixed resolution\nand we take inspiration from LLaVA (Liu et al.,\n2024) to enable flexible resolutions with a Pan\nand Scan (P&S) method.\nThe second main architectural improvement is\nan increase in context size to 128K tokens, with-\nout reducing performance. A challenge with long\ncontext is the memory explosion of the KV cache\nduring inference. To reduce this issue, we inter-\nleave multiple local layers between each global\nlayer, and assign a smaller span of only 1024\ntokens to the local layers. Therefore, only the\nglobal layers attend to long context, and we have\n1 global for every 5 local layers.\nThe pre-training optimization recipe is similar\nto Gemma 2, with some modifications in the ar-\nchitecture design. We use the same tokenizer as\nGemini 2.0, and we also revisit our data mixture\nto improve the multilingual capabilities of the\nmodels, while introducing image understanding.\nAll Gemma 3 models are trained with knowledge\ndistillation (Hinton et al., 2015).\nIn post-training, we focus our efforts on im-\nproving mathematics, reasoning, and chat abili-\nties, as well as integrating the new capabilities of\nGemma 3, long-context, and image inputs. We\nuse a novel post-training approach that brings\ngains across all capabilities, including math, cod-\ning, chat, instruction following, and multilingual.\nThe resulting Gemma 3 instruction-tuned models\nare both powerful and versatile, outperforming\ntheir predecessors by a wide margin.\nIn the following sections, we provide a brief\noverview of our models, including the architec-\nture and pre- and post-training recipes. We also\nprovide detailed evaluations across a wide vari-\nety of quantitative and qualitative benchmarks.\nWe discuss our approach to safe and responsible\ndeployment and outline the broader implications\nof Gemma 3, its limitations, and advantages.\n1See Contributions and Acknowledgments section for full author list. Please send correspondence to gemma-3-report@google.com.\n© 2025 Google DeepMind. All rights reserved\n",
  "page_2": "Gemma 3 Technical Report\nFigure 1 | Example of visual interaction with\nGemma 3 27B IT model.\n2. Model Architecture\nGemma 3 models follow the same general\ndecoder-only transformer architecture as previ-\nous iterations (Vaswani et al., 2017), with most\narchitecture elements similar to the first two\nGemma versions. We use a Grouped-Query Atten-\ntion (GQA) (Ainslie et al., 2023) with post-norm\nand pre-norm with RMSNorm (Zhang and Sen-\nnrich, 2019). Inspired by Dehghani et al. (2023),\nWortsman et al. (2023) and Chameleon Team\n(2024), we replace the soft-capping of Gemma 2\nwith QK-norm. In this section, we focus on some\nkey differences from previous versions below.\n5:1 interleaving of local/global layers.\nWe\nalternate between a local sliding window self-\nattention (Beltagy et al., 2020) and global self-\nModel\nVision\nEncoder\nEmbedding\nParameters\nNon-embedding\nParameters\n1B\n0\n302M\n698M\n4B\n417M\n675M\n3,209M\n12B\n417M\n1,012M\n10,759M\n27B\n417M\n1,416M\n25,600M\nTable 1 | Parameter counts for the Gemma 3 mod-\nels. Our vocabulary has 256k entries.\nattention (Luong et al., 2015), with a pattern of\n5 local layers for every global layer, starting with\na local layer as the first layer of the model.\nLong context. Gemma 3 models support context\nlength of 128K tokens, with the exception of the\n1B model that has 32K. We increase RoPE base\nfrequency from 10k to 1M on global self-attention\nlayers, and keep the frequency of the local lay-\ners at 10k. We follow a process similar to the\npositional interpolation of Chen et al. (2023) to\nextend the span of the global self-attention layers.\n2.1. Vision modality\nVision encoder. We use a 400M variant of the\nSigLIP encoder (Zhai et al., 2023), a Vision Trans-\nformer (Dosovitskiy, 2020) trained with a varia-\ntion of the CLIP loss (Radford et al., 2021). The\nGemma vision encoder takes as input square im-\nages resized to 896 x 896, and is finetuned on\ndata from visual assistant tasks. For simplicity, we\nshare the vision encoder across our 4B, 12B, and\n27B models, keeping it frozen during training.\nPan & Scan (P&S). The Gemma vision encoder\noperates at a fixed resolution of 896 × 896. This\nresults in artifacts when processing non-square\naspect ratios and high-resolution images, leading\nto unreadable text, or small object disappeared.\nWe address this issue with an adaptive windowing\nalgorithm during inference. This algorithm seg-\nments images into non-overlapping crops of equal\nsize, covering the whole image, and resize them\nto 896×896 pixels to pass them to the encoder.\nThis windowing is applied only when necessary,\nand control for the maximum number of crops.\nIt is an inference-time only optimization and can\nbe disabled for faster inference.\n2\n",
  "page_3": "Gemma 3 Technical Report\nShards\nModel\nType\n#Chips\nData\nSeq.\nReplica\n1B\nTPUv5e\n512\n16\n16\n2\n4B\nTPUv5e\n2048\n16\n16\n8\n12B\nTPUv4\n6144\n16\n16\n24\n27B\nTPUv5p\n6144\n24\n8\n32\nTable 2 | Training infrastructure with sharding by\ndata, sequence (Seq.), and replica.\n2.2. Pre-training\nWe follow a similar recipe as in Gemma 2 for\npre-training with knowledge distillation.\nTraining data. We pre-train our models on a\nslightly larger token budget than Gemma 2, i.e.,\nwe train on 14T tokens for Gemma 3 27B, 12T\nfor the 12B version, 4T for the 4B, and 2T to-\nkens for the 1B. The increase in tokens accounts\nfor the mix of images and text used during pre-\ntraining. We also increase the amount of multi-\nlingual data to improve language coverage. We\nadd both monolingual and parallel data, and we\nhandle the imbalance in language representation\nusing a strategy inspired by Chung et al. (2023).\nTokenizer. We use the same tokenizer as Gem-\nini 2.0: a SentencePiece tokenizer with split dig-\nits, preserved whitespace, and byte-level encod-\nings (Kudo and Richardson, 2018). The resulting\nvocabulary has 262k entries. This tokenizer is\nmore balanced for non-English languages.\nFiltering. We use filtering techniques that reduce\nthe risk of unwanted or unsafe utterances and\nremove certain personal information and other\nsensitive data. We decontaminate evaluation sets\nfrom our pre-training data mixture, and reduce\nthe risk of recitation by minimizing the prolifer-\nation of sensitive outputs. We also apply a qual-\nity reweighing step inspired by Sachdeva et al.\n(2024) to reduce occurrences of low quality data.\nDistillation. We sample 256 logits per token,\nweighted by teacher probabilities. The student\nlearns the teacher’s distribution within these sam-\nples via cross-entropy loss. The teacher’s target\ndistribution is set to zero probability for non-\nsampled logits, and renormalized.\nRaw (GB)\nQuantized (GB)\nModel\nbf16\nInt4\nInt4blocks=32\nSFP8\n1B\n2.0\n0.5\n0.7\n1.0\n+KV\n2.9\n1.4\n1.6\n1.9\n4B\n8.0\n2.6\n2.9\n4.4\n+KV\n12.7\n7.3\n7.6\n9.1\n12B\n24.0\n6.6\n7.1\n12.4\n+KV\n38.9\n21.5\n22.0\n27.3\n27B\n54.0\n14.1\n15.3\n27.4\n+KV\n72.7\n32.8\n34.0\n46.1\nTable 3 | Memory footprints (in GB) comparison\nbetween raw (bfloat16) and quantized check-\npoints for weights and KV caching (+KV) at\n32,768 context size, quantized in 8 bits.\n2.3. Quantization Aware Training\nAlong with the raw checkpoints, we also provide\nquantized versions of our models in different stan-\ndard formats. These versions are obtained by fine-\ntuning each model for a small number of steps,\ntypically 5,000, using Quantization Aware Train-\ning (QAT) (Jacob et al., 2018). We use prob-\nabilities from the non-quantized checkpoint as\ntargets, and adapt the data to match the pre-\ntraining and post-training distributions. Based\non the most popular open source quantization\ninference engines (e.g. llama.cpp), we focus on\nthree weight representations: per-channel int4,\nper-block int4, and switched fp8. In Table 3, we\nreport the memory filled by raw and quantized\nmodels for each weight representation with and\nwithout a KV-cache for a sequence of 32k tokens.\n2.4. Compute Infrastructure\nWe train our models with TPUv4, TPUv5e, and\nTPUv5p as outlined in Table 2. Each model con-\nfiguration is optimized to minimize training step\ntime. For the vision encoder, we pre-compute\nthe embeddings for each image and directly train\nwith the embeddings, adding no cost to the train-\ning of the language models.\nThe optimizer state is sharded using an im-\nplementation of ZeRO-3 (Ren et al., 2021). For\nmulti-pod training, we perform a data replica re-\n3\n",
  "page_4": "Gemma 3 Technical Report\nContext\nFormatting\nUser turn\n<start_of_turn>user\nModel turn\n<start_of_turn>model\nEnd of turn\n<end_of_turn>\nExample of discussion:\nUser: Who are you?\nModel: My name is Gemma!\nUser: What is 2+2?\nModel: 2+2=4.\nModel input:\n[BOS]<start_of_turn>user\nWho are you?<end_of_turn>\n<start_of_turn>model\nMy name is Gemma!<end_of_turn>\n<start_of_turn>user\nWhat is 2+2?<end_of_turn>\n<start_of_turn>model\nModel output:\n2+2=4.<end_of_turn>\nTable 4 | Formatting for Gemma IT models. Explic-\nitly add the [BOS] token after tokenization, or\nuse the add_bos=True option in the tokenizer.\nDo not tokenize the text \"[BOS]\".\nduction over the data center network, using the\nPathways approach of Barham et al. (2022). We\nuse the ‘single controller’ programming paradigm\nof Jax (Roberts et al., 2023) and Pathways\n(Barham et al., 2022), along with the GSPMD\npartitioner (Xu et al., 2021) and the MegaScale\nXLA compiler (XLA, 2019).\n3. Instruction-Tuning\nPre-trained models are turned into instruction-\ntuned models with an improved post-training ap-\nproach compared to our prior recipe (see Table 6).\nTechniques. Our post-training approach relies\non an improved version of knowledge distilla-\ntion (Agarwal et al., 2024; Anil et al., 2018; Hin-\nton et al., 2015) from a large IT teacher, along\nwith a RL finetuning phase based on improved ver-\nsions of BOND (Sessa et al., 2024), WARM (Ramé\net al., 2024b), and WARP (Ramé et al., 2024a).\nReinforcement learning objectives.\nWe use\na variety of reward functions to improve help-\nfulness, math, coding, reasoning, instruction-\nfollowing, and multilingual abilities, while mini-\nmizing model harmfulness. This includes learn-\ning from weight averaged reward models (Ramé\net al., 2024b) trained with human feedback data,\ncode execution feedback (Gehring et al., 2024),\nand ground-truth rewards for solving math prob-\nlems (DeepSeek-AI, 2025; Lambert et al., 2024).\nData filtering. We carefully optimize the data\nused in post-training to maximize model perfor-\nmance. We filter examples that show certain per-\nsonal information, unsafe or toxic model outputs,\nmistaken self-identification data, and duplicated\nexamples. Including subsets of data that encour-\nage better in-context attribution, hedging, and\nrefusals to minimize hallucinations also improves\nperformance on factuality metrics, without de-\ngrading model performance on other metrics.\n[BOS] token. For both PT and IT models, text\nstarts with a [BOS] token, that needs to be added\nexplicitly since the text “[BOS]” does not map to\nthe [BOS] token. For instance, Flax has an option,\nadd_bos=True, to add this token automatically\nwhen tokenizing. An example of the formatting\nfor an IT model is shown in Table 4,\nPT versus IT Formatting. All models share the\nsame tokenizer, with some control tokens dedi-\ncated to IT formatting. A key difference is that PT\nmodels output a <eos> token at the end of gener-\nation, while IT models output a <end_of_turn>\nat the end of the generation, as shown for IT in\nTable 4. Fine-tuning either model type thus also\nrequires to add their respective end token.\n4. Evaluation of final models\nIn this section, we evaluate the IT models over\na series of automated benchmarks and human\nevaluations across a variety of domains, as well\nas static benchmarks such as MMLU.\n4.1. LMSYS Chatbot Arena\nIn this section, we report the performance of our\nIT 27B model on LMSys Chatbot Arena (Chiang\net al., 2024) in blind side-by-side evaluations by\nhuman raters against other state-of-the-art mod-\nels. We report Elo scores in Table 5. Gemma 3 27B\n4\n",
  "page_5": "Gemma 3 Technical Report\nRank\nModel\nElo\n95% CI\nOpen\nType\n#params/#activated\n1\nGrok-3-Preview-02-24\n1412\n+8/-10\n-\n-\n-\n1\nGPT-4.5-Preview\n1411\n+11/-11\n-\n-\n-\n3\nGemini-2.0-Flash-Thinking-Exp-01-21\n1384\n+6/-5\n-\n-\n-\n3\nGemini-2.0-Pro-Exp-02-05\n1380\n+5/-6\n-\n-\n-\n3\nChatGPT-4o-latest (2025-01-29)\n1377\n+5/-4\n-\n-\n-\n6\nDeepSeek-R1\n1363\n+8/-6\nyes\nMoE\n671B/37B\n6\nGemini-2.0-Flash-001\n1357\n+6/-5\n-\n-\n-\n8\no1-2024-12-17\n1352\n+4/-6\n-\n-\n-\n9\nGemma-3-27B-IT\n1338\n+8/-9\nyes\nDense\n27B\n9\nQwen2.5-Max\n1336\n+7/-5\n-\n-\n-\n9\no1-preview\n1335\n+4/-3\n-\n-\n-\n9\no3-mini-high\n1329\n+8/-6\n-\n-\n-\n13\nDeepSeek-V3\n1318\n+8/-6\nyes\nMoE\n671B/37B\n14\nGLM-4-Plus-0111\n1311\n+8/-8\n-\n-\n-\n14\nQwen-Plus-0125\n1310\n+7/-5\n-\n-\n-\n14\nClaude 3.7 Sonnet\n1309\n+9/-11\n-\n-\n-\n14\nGemini-2.0-Flash-Lite\n1308\n+5/-5\n-\n-\n-\n18\nStep-2-16K-Exp\n1305\n+7/-6\n-\n-\n-\n18\no3-mini\n1304\n+5/-4\n-\n-\n-\n18\no1-mini\n1304\n+4/-3\n-\n-\n-\n18\nGemini-1.5-Pro-002\n1302\n+3/-3\n-\n-\n-\n...\n28\nMeta-Llama-3.1-405B-Instruct-bf16\n1269\n+4/-3\nyes\nDense\n405B\n...\n38\nLlama-3.3-70B-Instruct\n1257\n+5/-3\nyes\nDense\n70B\n...\n39\nQwen2.5-72B-Instruct\n1257\n+3/-3\nyes\nDense\n72B\n...\n59\nGemma-2-27B-it\n1220\n+3/-2\nyes\nDense\n27B\nTable 5 | Evaluation of Gemma 3 27B IT model in the Chatbot Arena (Chiang et al., 2024). All the\nmodels are evaluated against each other through blind side-by-side evaluations by human raters. Each\nmodel is attributed a score, based on the Elo rating system. Gemma-3-27B-IT numbers are preliminary\nresults received on March 8, 2025.\nIT (1338) is among the top 10 best models, with a\nscore above other non-thinking open models, such\nas DeepSeek-V3 (1318), LLaMA 3 405B (1257),\nand Qwen2.5-70B (1257), which are much larger\nmodels. Finally, the Elo of Gemma 3 is signifi-\ncantly higher than Gemma 2, at 1220. Note that\nElo scores do not take into account visual abilities,\nwhich none of the aforementioned models have.\n4.2. Standard benchmarks\nIn Table 6, we show the performance of our final\nmodels across a variety of benchmarks compared\nto our previous model iteration, and Gemini 1.5.\nWe do not compare directly with external mod-\nels that often report their own evaluation set-\ntings, since running them in our setting does not\nguarantee a fair comparison. We encourage the\nreader to follow third-party static leaderboards\nfor a fairer comparisons across models. We in-\nclude additional evaluations of our models on\nother benchmarks in the appendix.\n5. Ablations\nIn this section, we focus on the impact of our\narchitecture changes, as well as some of the vision\nabilities new to this model.\n5.1. Pre-training ability probing\nWe use several standard benchmarks as probes\nduring pre-training to ensure our models capture\ngeneral abilities, and in Figure 2, we compare the\nquality of pre-trained models from Gemma 2 and\n3 across these general abilities, namely, science,\n5\n",
  "page_6": "Gemma 3 Technical Report\nGemini 1.5\nGemini 2.0\nGemma 2\nGemma 3\nFlash\nPro\nFlash\nPro\n2B\n9B\n27B\n1B\n4B\n12B\n27B\nMMLU-Pro\n67.3\n75.8\n77.6\n79.1\n15.6\n46.8\n56.9\n14.7\n43.6\n60.6\n67.5\nLiveCodeBench\n30.7\n34.2\n34.5\n36.0\n1.2\n10.8\n20.4\n1.9\n12.6\n24.6\n29.7\nBird-SQL (dev)\n45.6\n54.4\n58.7\n59.3\n12.2\n33.8\n46.7\n6.4\n36.3\n47.9\n54.4\nGPQA Diamond\n51.0\n59.1\n60.1\n64.7\n24.7\n28.8\n34.3\n19.2\n30.8\n40.9\n42.4\nSimpleQA\n8.6\n24.9\n29.9\n44.3\n2.8\n5.3\n9.2\n2.2\n4.0\n6.3\n10.0\nFACTS Grounding\n82.9\n80.0\n84.6\n82.8\n43.8\n62.0\n62.4\n36.4\n70.1\n75.8\n74.9\nGlobal MMLU-Lite\n73.7\n80.8\n83.4\n86.5\n41.9\n64.8\n68.6\n34.2\n54.5\n69.5\n75.1\nMATH\n77.9\n86.5\n90.9\n91.8\n27.2\n49.4\n55.6\n48.0\n75.6\n83.8\n89.0\nHiddenMath\n47.2\n52.0\n63.5\n65.2\n1.8\n10.4\n14.8\n15.8\n43.0\n54.5\n60.3\nMMMU (val)\n62.3\n65.9\n71.7\n72.7\n-\n-\n-\n-\n48.8\n59.6\n64.9\nTable 6 | Performance of instruction fine-tuned (IT) models compared to Gemini 1.5, Gemini 2.0, and\nGemma 2 on zero-shot benchmarks across different abilities.\nFigure 2 | Summary of the performance of different pre-trained models from Gemma 2 and 3 across\ngeneral abilities. This plots are meant to give an simplified summary and details are in the appendix.\ncode, factuality, multilinguality, reasoning, and\nvision. The details of the performance across the\ndifferent public benchmarks used in these plots\nare summarized in the appendix. Overall, we see\nthat the new versions improve in most categories,\ndespite the addition of vision. We particularly\nfocus on multilinguality in this version, and this\ndirectly impacts the quality of our models. How-\never, despite the use of decontamination tech-\nniques, there is always a risk of contamination\nof these probes (Mirzadeh et al., 2024), making\nmore definitive conclusions harder to assess.\n5.2. Local:Global attention layers\nWe measure the impact of changes to local and\nglobal self-attention layers on performance and\nmemory consumption during inference.\nLocal:Global ratio. In Fig. 3, we compare differ-\n1:1\n3:1\n5:1\n7:1\nLocal:Global\n0.1\n0.0\n0.1\n Perplexity\n2B\n9B\nFigure 3 | Impact of Local:Global ratio on the\nperplexity on a validation set. The impact is mini-\nmal, even with 7-to-1 local to global. This ablation\nis run with text-only models.\nent ratios of local to global attention layers. 1:1\nis used in Gemma 2 models, and 5:1 is used in\nGemma 3. We observe minimal impact on per-\nplexity when changing this ratio.\nSliding window size. In Fig. 4, we compare\ndifferent sliding window sizes for the local at-\n6\n",
  "page_7": "Gemma 3 Technical Report\ntention layers in different global:local ratio con-\nfigurations. The sliding window can be reduced\nsignificantly without impacting perplexity.\n512\n1024\n2048\n4096\nSliding Window\n0.02\n0.01\n0.00\n0.01\n Perplexity\n2B L:G=1:1\n2B L:G=3:1\nFigure 4 | Impact of Sliding Window size on per-\nplexity measured on a validation set. We consider\n2 2B models, with 1:1 and 1:3 local to global layer\nratios. This ablation is run with text-only models.\nImpact on KV cache memory. In Fig. 5, we show\nthe balance between the memory used by the\nmodel and the KV cache during inference with a\ncontext of 32k tokens. The “global only” configu-\nration is the standard configuration used across\nmost dense models. The “1:1, sw=4096” is used\nin Gemma 2. We observe that the “global only”\nconfiguration results in a memory overhead of\n60%, while this is reduced to less than 15% with\n1:3 and sliding window of 1024 (“sw=1024”).\nIn Fig. 6, we compute the memory used by the\nKV cache as a function of the context length with\neither our 2B architecture (L:G=5:1, sw=1024)\nversus a “global only” 2B model.\nglobal only\n1:1, sw=4096\n1:1 sw=1024\n1:3 sw=4096\n1:3 sw=1024\n0\n1000\n2000\n3000\n4000\n5000\nInference memory (MB)\nmodel\nkv cache\nFigure 5 | Model versus KV cache memory dur-\ning inference with a pre-fill KV cache of size 32k.\nWe consider a 2B model with different local to\nglobal ratios and sliding window sizes (sw). We\ncompare to global only, which is the standard\nused in Gemma 1 and Llama. This ablation is run\nwith a text-only model.\n5.3. Enabling long context\nInstead of training with 128K sequences from\nscratch, we pre-train our models with 32K se-\n1K\n4K\n8K\n16K\n32K\n64K 128K\nContext length\n0\n2000\n4000\n6000\nKV Cache memory (MB)\n2B L:G=5:1, sw=1024\n2B global only\nFigure 6 | KV cache memory versus context\nlength. We show the memory usage of the KV\ncache for our architecture (L:G=5:1, sw=1024)\nand a transformer with global attention only – as\nused in LLaMa or Gemma 1.\nquences and then scale the 4B, 12B, and 27B mod-\nels up to 128K tokens at the end of pre-training\nwhile rescaling RoPE (Chen et al., 2023). We\nfind a scaling factor of 8 to work well in practice.\nNote that compared to Gemma 2, we have also\nincreased the RoPE base frequency of global self-\nattention layers from 10k to 1M, while keeping\n10k for the local self-attention layers. In Figure 7,\nwe show the impact on perplexity for different\ncontext lengths. Our models generalize to 128K,\nbut rapidly degrade as we continue to scale.\nFigure 7 | Long context performance of pre-\ntrained models before and after RoPE rescaling.\n5.4. Small versus large teacher\nA common finding is that, to train a small model,\nit is preferable to distill from a smaller teacher.\n7\n",
  "page_8": "Gemma 3 Technical Report\n101\n102\nTotal training tokens (B)\n0.006\n0.004\n0.002\n0.000\n0.002\n Perplexity\nFigure 8 | Small versus large teacher. Relative\ndifference of perplexity when using a small and\nlarge teacher as a function of the token size of\ntraining. Smaller numbers means distilling from\na larger teacher is better.\nWe suspect this is because these studies are often\nperformed in settings where the regularization ef-\nfect of using a worse teacher surpasses the benefit\nof using a better teacher. We train a student with\n2 teachers of different sizes, one large and one\nsmall, for different training horizons. In Fig. 8,\nwe observe that for short training horizons, the\nsmaller teacher is better, but the trend is reversed\nfor longer training.\n5.5. Vision encoder\nResolution\nDocVQA\nInfoVQA\nTextVQA\n256\n31.9\n23.1\n44.1\n448\n45.4\n31.6\n53.5\n896\n59.8\n33.7\n58.0\nTable 7 | Impact of image encoder input reso-\nlution. We measure performance using a short\nschedule 2B Gemma model on a few evaluation\nbenchmarks to observe the effect of input image\nresolution on vision encoder pre-training.\nImpact of image resolution. We use a vision\nencoder based on SigLIP (Zhai et al., 2023). The\nvision encoder is frozen, and only the language\nmodel is trained. Each image in this multimodal\ndata is represented by 256 image tokens from\nthe respective vision encoder. The higher resolu-\ntion encoders thus use average pooling to reduce\ntheir output to 256 tokens. For instance, the 896\nresolution encoder has a 4x4 average pooling on\nits output. As shown in Table 7, higher resolution\nencoders perform than smaller ones.\nDocVQA\nInfoVQA\nTextVQA\n4B\n72.8\n44.1\n58.9\n4B w/ P&S\n81.0\n57.0\n60.8\nΔ\n(+8.2)\n(+12.9)\n(+1.9)\n27B\n85.6\n59.4\n68.6\n27B w/ P&S\n90.4\n76.4\n70.2\nΔ\n(+4.8)\n(+17.0)\n(+1.6)\nTable 8 | Impact of P&S. 4-shot evaluation re-\nsults on the valid set, with and without P&S on a\npre-trained checkpoint. Boosts are on tasks asso-\nciated with images with varying aspect ratios, or\ninvolving reading text on images.\nPan & Scan. P&S enables capturing images at\nclose to their native aspect ratio and image reso-\nlution. In Table 8, we compare our 27B IT model\nwith and without P&S. As expected, the ability\nto treat images with close to native resolution\ngreatly helps with tasks that require some form\nof reading text on images, which is particularly\nimportant for visual language models.\n6. Memorization and Privacy\nLarge language models may produce near-copies\nof some text used in training (Biderman et al.,\n2023; Carlini et al., 2021, 2022; Ippolito et al.,\n2022; Nasr et al., 2023). Several prior reports\nhave released audits that quantify this risk by\nmeasuring the memorization rate (Anil et al.,\n2023; Chowdhery et al., 2022; Gemini Team,\n2023, 2024; Gemma Team, 2024a,b; LLaMa\nTeam, 2024). This “memorization rate”1 is de-\nfined as the ratio of generations from the model\nthat match its training data compared to all model\ngenerations using the following setup. We fol-\nlow the methodology described in Gemma Team\n1\"We do not state or imply [here] that a model \"contains\"\nits training data in the sense that there is a copy of that data\nin the model. Rather, a model memorizes attributes of its\ntraining data such that in certain cases it is statistically able\nto generate such training data when following rules and\nusing information about features of its training data that it\ndoes contain.\"\n8\n",
  "page_9": "Gemma 3 Technical Report\nGemma 3\n1B\nGemma 3\n4B\nGemma 3\n12B\nGemma 3\n27B\nGemma 2\n 2B\nGemma 2\n 9B\nGemma 2\n 27B\nGemini 1.5\nFlash\nGemma\n2B\nGemma\n7B\nPaLM\nSmall\nModel\n0.0001\n0.001\n0.01\n0.1\n1\n10\n% Memorized\nTotal Memorization Rate\nMemorization Type\nExact\nApproximate\nFigure 9 | Total memorization rates for both ex-\nact and approximate memorization. Gemma 3\nmodels memorize significantly less than all prior\nmodels. *No results for approximate memoriza-\ntion on these models.\n(2024b) to measure it. Specifically, we subsam-\nple a large portion of training data distributed\nuniformly across different corpora and test for\ndiscoverable extraction (Nasr et al., 2023) of this\ncontent using a prefix of length 50 and a suffix of\nlength 50. We denote text as either “exactly mem-\norized” if all tokens in the continuation match\nthe source suffix or “approximately memorized”\nif they match up to an edit distance of 10%.\nFigure 9 compares the memorization rates\nacross Gemma and Gemini models; these models\nare ordered in reverse chronological order, with\nthe newest Gemma 3 models on the left. We find\nthat Gemma 3 models memorize long-form text\nat a much lower rate than prior models (note the\nlog y-axis). We observe only a marginal differ-\nence in the memorization rates between the 4B,\n12B, and 27B models, with 1B memorizing less\nthan these larger models. Further, we find that a\nlarger proportion of text is characterized as ap-\nproximately memorized, with a relative increase\nin approximate memorization compared to exact\nmemorization of roughly 24x on average.\nWe also study the rate at which the generations\nmay contain personal information. To identify po-\ntentially personal information, we use the Google\nCloud Sensitive Data Protection (SDP) service.2\nSDP uses broad detection rules to identify text\nthat may contain personal information. SDP is\n2https://cloud.google.com/sensitive-data-protection\ndesigned to have high recall and does not con-\nsider the context in which the information may\nappear, which leads to many false positives. Thus,\nwe are likely overestimating the true amount of\npotentially personal information contained in the\noutputs classified as memorized. SDP also pro-\nvides broad severity levels: low, medium, and\nhigh. We classify text as personal if SDP clas-\nsifies it as personal information at any severity\nlevel. We observed no personal information in\nthe outputs characterized as memorization for all\nGemma 3 models. This indicates a low rate of\npersonal data, below our detection thresholds, in\noutputs classified as memorization.\n7. Responsibility, Safety, Security\nResponsibility, safety, and security are of utmost\nimportance in the development of Gemma mod-\nels. To reduce risks to Gemma 3 users, we have\ncontinued to integrate enhanced internal safety\nprocesses that span the development workflow,\nin line with recent Google AI models (Gemini\nTeam, 2024). This focuses on safety mitigation at\ntraining time, and robust and transparent model\nevaluations for the new image-to-text capabilities\nwe have introduced.\n7.1. Governance & Assessment\nOur approach to assessing the benefits and risks\nof Gemma is reflective of that outlined for Gemma\n1 (Gemma Team, 2024a), taking into account the\nchanges in supported modalities. We continue to\nbelieve that openness in AI can spread the bene-\nfits of these technologies across society, but must\nbe evaluated against the risk of malicious uses\nthat can cause harm on both individual and in-\nstitutional levels (Weidinger et al., 2021). Since\nthe inaugural Gemma launch, we have seen these\nmodels drive a number of socially beneficial ap-\nplications, such as our own ShieldGemma 2, a 4B\nimage safety classifier built with Gemma 3, which\nprovides a ready-made solution for image safety,\noutputting safety labels across dangerous content,\nsexually explicit, and violence categories.\nReleasing Gemma 3 models required specific\nattention to changes in model capabilities and\n9\n",
  "page_10": "Gemma 3 Technical Report\nclose monitoring of the evolving risks of existing\nmultimodal LLMs (Lin et al., 2024), as well as an\nunderstanding of the ways in which models are\nbeing used in the wild. Although we are yet to\nreceive any reports of malicious use for Gemma,\nwe remain committed to investigating any such\nreporting, and work with the academic and de-\nveloper communities, as well as conduct our own\nmonitoring, to flag such cases.\nDespite advancements in capabilities, we be-\nlieve that, given the number of larger powerful\nopen models available, this release will have a\nnegligible effect on the overall risk landscape.\n7.2. Safety policies and train-time mitigations\nA key pillar of Gemma’s approach to safety is to\nalign fine-tuned models with Google’s safety poli-\ncies, in line with Gemini models (Gemini Team,\n2023). They are designed to help prevent our\nmodels from generating harmful content, i.e.,\n• Child sexual abuse and exploitation\n• Revealing personally identifiable information\nthat can lead to harm (e.g., Social Security\nnumbers)\n• Hate speech and harassment\n• Dangerous or malicious content (including\npromoting self-harm or instructing in harm-\nful activities)\n• Sexually explicit content\n• Medical advice that runs contrary to scientific\nor medical consensus\nWe undertook considerable safety filtering of our\npre-training data to reduce the likelihood of our\npre-trained and fine-tuned checkpoints producing\nharmful content. For fine-tuned models, we also\nuse both SFT and RLHF to steer the model away\nfrom undesirable behavior.\n7.3. Assurance Evaluations\nWe also run our IT models through a set of base-\nline assurance evaluations to understand the po-\ntential harms that our models can cause. As we\nchampion open models, we also recognize that\nthe irreversible nature of weight releases requires\nrigorous risk assessment. Our internal safety pro-\ncesses are designed accordingly, and for previ-\nous Gemma models we have also undertaken\nevaluations of capabilities relevant to extreme\nrisks (Phuong et al., 2024; Shevlane et al., 2023).\nAs we continue to develop and share open mod-\nels, we will follow the heuristic that thoroughly\nevaluating a more capable model often provides\nsufficient assurance for less capable ones. As such,\nwe prioritised a streamlined set of evaluations for\nGemma 3, reserving in-depth dangerous capabil-\nity assessments for cases where a specific model\nmay present a potentially heightened risk (as de-\nscribed below on CBRN evaluations). We balance\ndevelopment speed with targeted safety testing,\nensuring our evaluations are well-focused and\nefficient, while upholding the commitments laid\nout in our Frontier Safety Framework.\nBaseline Evaluations\nBaseline assurance captures the model violation\nrate for safety policies, using a large number of\nsynthetic adversarial user queries, and human\nraters to label the answers as policy violating or\nnot. Overall, Gemma 3 violation rate is signifi-\ncantly low overall on these safety policies.\nChemical, Biological, Radiological and Nuclear\n(CBRN) knowledge\nOwing to enhanced performance on STEM-\nrelated tasks, we evaluated knowledge relevant\nto biological, radiological, and nuclear risks using\nan internal dataset of closed-ended, knowledge-\nbased multiple choice questions. For evaluations\nof chemical knowledge, we employed a closed-\nended knowledge-based approach on chemical\nhazards developed by Macknight et al. Our eval-\nuation suggests that the knowledge of Gemma 3\nmodels in these domains is low.\n7.4. Our approach to responsible open models\nDesigning safe, secure, and responsible applica-\ntions requires a system-level approach, working\nto mitigate risks associated with each specific use\ncase and environment. We will continue to adopt\nassessments and safety mitigations proportion-\nate to the potential risks from our models, and\n10\n",
  "page_11": "Gemma 3 Technical Report\nwill only share these with the community when\nwe are confident that the benefits significantly\noutweigh the foreseeable risks.\n8. Discussion and Conclusion\nIn this work, we have presented Gemma 3, the\nlatest addition to the Gemma family of open lan-\nguage models for text, image, and code. In this\nversion, we focus on adding image understanding\nand long context while improving multilinguality\nand STEM-related abilities. Our model sizes and\narchitectures are designed to be compatible with\nstandard hardware, and most of our architecture\nimprovements are tailored to fit this hardware\nwhile maintaining performance.\nReferences\nRealworldqa.\nhttps://x.ai/news/grok-1.\n5v.\nM. Acharya, K. Kafle, and C. Kanan. Tallyqa: An-\nswering complex counting questions. In AAAI,\n2018.\nR. Agarwal, N. Vieillard, Y. Zhou, P. Stanczyk, S. R.\nGarea, M. Geist, and O. Bachem. On-policy\ndistillation of language models: Learning from\nself-generated mistakes. In ICLR, 2024.\nJ. Ainslie, J. Lee-Thorp, M. de Jong, Y. Zemlyan-\nskiy, F. Lebrón, and S. Sanghai. Gqa: Training\ngeneralized multi-query transformer models\nfrom multi-head checkpoints. arXiv preprint\narXiv:2305.13245, 2023.\nR. Anil, G. Pereyra, A. Passos, R. Ormandi, G. E.\nDahl, and G. E. Hinton. Large scale distributed\nneural network training through online distil-\nlation. arXiv preprint arXiv:1804.03235, 2018.\nR. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lep-\nikhin, A. Passos, S. Shakeri, E. Taropa, P. Bailey,\nZ. Chen, et al. Palm 2 technical report. arXiv\npreprint arXiv:2305.10403, 2023.\nM. Artetxe, S. Ruder, and D. Yogatama. On the\ncross-lingual transferability of monolingual rep-\nresentations. In ACL, 2020.\nA. Asai, J. Kasai, J. H. Clark, K. Lee, E. Choi,\nand H. Hajishirzi. Xor qa: Cross-lingual open-\nretrieval question answering. arXiv preprint\narXiv:2010.11856, 2020.\nJ. Austin, A. Odena, M. I. Nye, M. Bosma,\nH. Michalewski, D. Dohan, E. Jiang, C. J. Cai,\nM. Terry, Q. V. Le, and C. Sutton. Program\nsynthesis with large language models. CoRR,\nabs/2108.07732, 2021.\nP. Barham, A. Chowdhery, J. Dean, S. Ghemawat,\nS. Hand, D. Hurt, M. Isard, H. Lim, R. Pang,\nS. Roy, B. Saeta, P. Schuh, R. Sepassi, L. E.\nShafey, C. A. Thekkath, and Y. Wu.\nPath-\nways: Asynchronous distributed dataflow for\nml, 2022.\nI. Beltagy, M. E. Peters, and A. Cohan. Long-\nformer: The long-document transformer. arXiv\npreprint arXiv:2004.05150, 2020.\nS.\nBiderman,\nU.\nPrashanth,\nL.\nSutawika,\nH. Schoelkopf, Q. Anthony, S. Purohit, and\nE. Raff. Emergent and predictable memoriza-\ntion in large language models. NeurIPS, 36:\n28072–28090, 2023.\nY. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi.\nPIQA: reasoning about physical commonsense\nin natural language. CoRR, abs/1911.11641,\n2019.\nN. Carlini, F. Tramer, E. Wallace, M. Jagielski,\nA. Herbert-Voss, K. Lee, A. Roberts, T. Brown,\nD. Song, U. Erlingsson, et al. Extracting train-\ning data from large language models.\nIn\nUSENIX, 2021.\nN. Carlini, D. Ippolito, M. Jagielski, K. Lee,\nF. Tramer, and C. Zhang. Quantifying memo-\nrization across neural language models. arXiv\npreprint arXiv:2202.07646, 2022.\nChameleon Team.\nChameleon: Mixed-modal\nearly-fusion foundation models. arXiv preprint\narXiv:2405.09818, 2024.\nM. Chen, J. Tworek, H. Jun, Q. Yuan, H. P.\nde Oliveira Pinto, J. Kaplan, H. Edwards,\nY. Burda, N. Joseph, G. Brockman, A. Ray,\nR. Puri, G. Krueger, M. Petrov, H. Khlaaf,\n11\n",
  "page_12": "Gemma 3 Technical Report\nG. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ry-\nder, M. Pavlov, A. Power, L. Kaiser, M. Bavar-\nian, C. Winter, P. Tillet, F. P. Such, D. Cum-\nmings, M. Plappert, F. Chantzis, E. Barnes,\nA. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino,\nN. Tezak, J. Tang, I. Babuschkin, S. Balaji,\nS. Jain, W. Saunders, C. Hesse, A. N. Carr,\nJ. Leike, J. Achiam, V. Misra, E. Morikawa,\nA. Radford, M. Knight, M. Brundage, M. Murati,\nK. Mayer, P. Welinder, B. McGrew, D. Amodei,\nS. McCandlish, I. Sutskever, and W. Zaremba.\nEvaluating large language models trained on\ncode. CoRR, abs/2107.03374, 2021.\nS. Chen, S. Wong, L. Chen, and Y. Tian. Extend-\ning context window of large language mod-\nels via positional interpolation. arXiv preprint\narXiv:2306.15595, 2023.\nX. Chen, H. Fang, T.-Y. Lin, R. Vedantam, S. Gupta,\nP. Dollár, and C. L. Zitnick.\nMicrosoft coco\ncaptions: Data collection and evaluation server.\nArXiv, abs/1504.00325, 2015.\nW.-L. Chiang, L. Zheng, Y. Sheng, A. N. An-\ngelopoulos, T. Li, D. Li, H. Zhang, B. Zhu,\nM. Jordan, J. E. Gonzalez, and I. Stoica. Chat-\nbot arena: An open platform for evaluating\nllms by human preference, 2024.\nF. Chollet. On the measure of intelligence. arXiv\npreprint arXiv:1911.01547, 2019.\nA. Chowdhery, S. Narang, J. Devlin, M. Bosma,\nG. Mishra, A. Roberts, P. Barham, H. W.\nChung, C. Sutton, S. Gehrmann, P. Schuh,\nK. Shi, S. Tsvyashchenko, J. Maynez, A. Rao,\nP. Barnes, Y. Tay, N. Shazeer, V. Prabhakaran,\nE. Reif, N. Du, B. Hutchinson, R. Pope, J. Brad-\nbury, J. Austin, M. Isard, G. Gur-Ari, P. Yin,\nT. Duke, A. Levskaya, S. Ghemawat, S. Dev,\nH. Michalewski, X. Garcia, V. Misra, K. Robin-\nson, L. Fedus, D. Zhou, D. Ippolito, D. Luan,\nH. Lim, B. Zoph, A. Spiridonov, R. Sepassi,\nD. Dohan, S. Agrawal, M. Omernick, A. M. Dai,\nT. S. Pillai, M. Pellat, A. Lewkowycz, E. Moreira,\nR. Child, O. Polozov, K. Lee, Z. Zhou, X. Wang,\nB. Saeta, M. Diaz, O. Firat, M. Catasta, J. Wei,\nK. Meier-Hellstern, D. Eck, J. Dean, S. Petrov,\nand N. Fiedel. Palm: Scaling language model-\ning with pathways, 2022.\nH. W. Chung, N. Constant, X. Garcia, A. Roberts,\nY. Tay, S. Narang, and O. Firat. Unimax: Fairer\nand more effective language sampling for large-\nscale multilingual pretraining, 2023.\nC. Clark, K. Lee, M. Chang, T. Kwiatkowski,\nM. Collins, and K. Toutanova. Boolq: Explor-\ning the surprising difficulty of natural yes/no\nquestions. CoRR, abs/1905.10044, 2019.\nK. Cobbe, V. Kosaraju, M. Bavarian, M. Chen,\nH. Jun, L. Kaiser, M. Plappert, J. Tworek,\nJ. Hilton, R. Nakano, C. Hesse, and J. Schul-\nman. Training verifiers to solve math word\nproblems. CoRR, abs/2110.14168, 2021.\nDeepSeek-AI. Deepseek-r1: Incentivizing reason-\ningt learning, 2025.\nM.\nDehghani,\nJ.\nDjolonga,\nB.\nMustafa,\nP. Padlewski, J. Heek, J. Gilmer, A. P. Steiner,\nM. Caron, R. Geirhos, I. Alabdulmohsin, et al.\nScaling vision transformers to 22 billion\nparameters. In ICML, 2023.\nD. Deutsch, E. Briakou, I. Caswell, M. Finkelstein,\nR. Galor, J. Juraska, G. Kovacs, A. Lui, R. Rei,\nJ. Riesa, S. Rijhwani, P. Riley, E. Salesky, F. Tra-\nbelsi, S. Winkler, B. Zhang, and M. Freitag.\nWmt24++: Expanding the language coverage\nof wmt24 to 55 languages & dialects, 2025.\nA. Dosovitskiy. An image is worth 16x16 words:\nTransformers for image recognition at scale.\narXiv preprint arXiv:2010.11929, 2020.\nD. Dua, Y. Wang, P. Dasigi, G. Stanovsky, S. Singh,\nand M. Gardner. DROP: A reading comprehen-\nsion benchmark requiring discrete reasoning\nover paragraphs. In ACL, 2019.\nB. Fatemi, M. Kazemi, A. Tsitsulin, K. Malkan,\nJ. Yim, J. Palowitch, S. Seo, J. Halcrow, and\nB. Perozzi.\nTest of time: A benchmark for\nevaluating llms on temporal reasoning. arXiv\npreprint arXiv:2406.09170, 2024.\nX. Fu, Y. Hu, B. Li, Y. Feng, H. Wang, X. Lin,\nD. Roth, N. A. Smith, W.-C. Ma, and R. Krishna.\nBlink: Multimodal large language models can\nsee but not perceive. ArXiv, abs/2404.12390,\n2024.\n12\n",
  "page_13": "Gemma 3 Technical Report\nJ. Gehring, K. Zheng, J. Copet, V. Mella, T. Cohen,\nand G. Synnaeve. Rlef: Grounding code llms in\nexecution feedback with reinforcement learn-\ning. arXiv preprint arXiv:2410.02089, 2024.\nGemini Team. Gemini: A family of highly capable\nmultimodal models, 2023.\nGemini Team. Gemini 1.5: Unlocking multimodal\nunderstanding across millions of tokens of con-\ntext, 2024.\nGemma Team. Gemma: Open models based on\ngemini research and technology, 2024a.\nGemma Team. Gemma 2: Improving open lan-\nguage models at a practical size. arXiv preprint\narXiv:2408.00118, 2024b.\nO. Goldman, U. Shaham, D. Malkin, S. Eiger,\nA. Hassidim, Y. Matias, J. Maynez, A. M. Gi-\nlady, J. Riesa, S. Rijhwani, L. Rimell, I. Szpektor,\nR. Tsarfaty, and M. Eyal. Eclektic: a novel chal-\nlenge set for evaluation of cross-lingual knowl-\nedge transfer, 2025.\nN. Goyal, C. Gao, V. Chaudhary, P.-J. Chen,\nG. Wenzek, D. Ju, S. Krishnan, M. Ranzato,\nF. Guzmán, and A. Fan. The flores-101 evalua-\ntion benchmark for low-resource and multilin-\ngual machine translation. ACL, 2022.\nY. Goyal, T. Khot, D. Summers-Stay, D. Batra, and\nD. Parikh. Making the V in VQA matter: Elevat-\ning the role of image understanding in Visual\nQuestion Answering. In CVPR, 2017.\nD. Hendrycks, C. Burns, S. Basart, A. Zou,\nM. Mazeika, D. Song, and J. Steinhardt. Mea-\nsuring massive multitask language understand-\ning. CoRR, abs/2009.03300, 2020.\nD. Hendrycks, C. Burns, S. Kadavath, A. Arora,\nS. Basart, E. Tang, D. Song, and J. Steinhardt.\nMeasuring mathematical problem solving with\nthe math dataset. NeurIPS, 2021.\nJ. Hessel, A. Marasović, J. D. Hwang, L. Lee, J. Da,\nR. Zellers, R. Mankoff, and Y. Choi. Do an-\ndroids laugh at electric sheep? humor\" under-\nstanding\" benchmarks from the new yorker cap-\ntion contest. arXiv preprint arXiv:2209.06293,\n2022.\nG. Hinton, O. Vinyals, and J. Dean. Distilling the\nknowledge in a neural network. arXiv preprint\narXiv:1503.02531, 2015.\nC.-P. Hsieh, S. Sun, S. Kriman, S. Acharya,\nD. Rekesh, F. Jia, Y. Zhang, and B. Ginsburg.\nRuler: What’s the real context size of your\nlong-context language models? arXiv preprint\narXiv:2404.06654, 2024.\nD. Ippolito, F. Tramèr, M. Nasr, C. Zhang,\nM. Jagielski, K. Lee, C. A. Choquette-Choo, and\nN. Carlini. Preventing verbatim memorization\nin language models gives a false sense of pri-\nvacy. arXiv preprint arXiv:2210.17546, 2022.\nB. Jacob, S. Kligys, B. Chen, M. Zhu, M. Tang,\nA. Howard, H. Adam, and D. Kalenichenko.\nQuantization and training of neural networks\nfor efficient integer-arithmetic-only inference.\nIn CVPR, 2018.\nM. Joshi, E. Choi, D. S. Weld, and L. Zettlemoyer.\nTriviaqa: A large scale distantly supervised\nchallenge dataset for reading comprehension.\nCoRR, abs/1705.03551, 2017.\nM. Kazemi, H. Alvari, A. Anand, J. Wu, X. Chen,\nand R. Soricut. Geomverse: A systematic eval-\nuation of large models for geometric reasoning.\narXiv preprint arXiv:2312.12241, 2023.\nM. Kazemi, N. Dikkala, A. Anand, P. Dević, I. Das-\ngupta, F. Liu, B. Fatemi, P. Awasthi, D. Guo,\nS. Gollapudi, and A. Qureshi. Remi: A dataset\nfor reasoning with multiple images.\nArXiv,\nabs/2406.09175, 2024a.\nM. Kazemi,\nQ. Yuan,\nD. Bhatia,\nN. Kim,\nX. Xu, V. Imbrasaite, and D. Ramachandran.\nBoardgameqa:\nA dataset for natural lan-\nguage reasoning with contradictory informa-\ntion. NeurIPS, 36, 2024b.\nM. Kazemi, B. Fatemi, H. Bansal, J. Palowitch,\nC. Anastasiou, S. V. Mehta, L. K. Jain, V. Aglietti,\nD. Jindal, P. Chen, et al. Big-bench extra hard.\narXiv preprint arXiv:2502.19187, 2025.\nA. Kembhavi, M. Salvato, E. Kolve, M. Seo, H. Ha-\njishirzi, and A. Farhadi. A diagram is worth a\ndozen images. ArXiv, abs/1603.07396, 2016.\n13\n",
  "page_14": "Gemma 3 Technical Report\nE. Kıcıman, R. Ness, A. Sharma, and C. Tan.\nCausal reasoning and large language models:\nOpening a new frontier for causality.\narXiv\npreprint arXiv:2305.00050, 2023.\nT. Kudo and J. Richardson. SentencePiece: A\nsimple and language independent subword to-\nkenizer and detokenizer for neural text pro-\ncessing. 2018.\nT.\nKwiatkowski,\nJ.\nPalomaki,\nO.\nRedfield,\nM. Collins, A. Parikh, C. Alberti, D. Epstein,\nI. Polosukhin, J. Devlin, K. Lee, K. Toutanova,\nL. Jones, M. Kelcey, M.-W. Chang, A. M. Dai,\nJ. Uszkoreit, Q. Le, and S. Petrov. Natural ques-\ntions: A benchmark for question answering re-\nsearch. ACL, 2019.\nN. Lambert, J. Morrison, V. Pyatkin, S. Huang,\nH. Ivison, F. Brahman, L. J. V. Miranda, A. Liu,\nN. Dziri, S. Lyu, et al.\nT\\\" ulu 3: Pushing\nfrontiers in open language model post-training.\narXiv preprint arXiv:2411.15124, 2024.\nZ. Lin, J. Cui, X. Liao, and X. Wang. Malla: De-\nmystifying real-world large language model\nintegrated malicious services, 2024.\nH. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruc-\ntion tuning. NeurIPS, 36, 2024.\nLLaMa Team. The llama 3 herd of models. arXiv\npreprint arXiv:2407.21783, 2024.\nM. Luong, H. Pham, and C. D. Manning. Effective\napproaches to attention-based neural machine\ntranslation. 2015.\nMacknight, Aung, and Gomes. Personal Commu-\nnication.\nK. Marino, M. Rastegari, A. Farhadi, and R. Mot-\ntaghi. Ok-vqa: A visual question answering\nbenchmark requiring external knowledge. In\nCVPR, 2019.\nA. Masry, X. L. Do, J. Q. Tan, S. Joty, and E. Hoque.\nChartQA: A benchmark for question answering\nabout charts with visual and logical reasoning.\nACL, 2022.\nM. Mathew, D. Karatzas, R. Manmatha, and C. V.\nJawahar. Docvqa: A dataset for vqa on docu-\nment images. WACV, 2020.\nM. Mathew, V. Bagal, R. Tito, D. Karatzas, E. Val-\nveny, and C. Jawahar. Infographicvqa. In WACV,\n2022.\nI. Mirzadeh, K. Alizadeh, H. Shahrokhi, O. Tuzel,\nS. Bengio, and M. Farajtabar. Gsm-symbolic:\nUnderstanding the limitations of mathemati-\ncal reasoning in large language models. arXiv\npreprint arXiv:2410.05229, 2024.\nM. Nasr, N. Carlini, J. Hayase, M. Jagielski, A. F.\nCooper, D. Ippolito, C. A. Choquette-Choo,\nE. Wallace, F. Tramèr, and K. Lee.\nScal-\nable extraction of training data from (pro-\nduction) language models.\narXiv preprint\narXiv:2311.17035, 2023.\nA. Nie, Y. Zhang, A. S. Amdekar, C. Piech, T. B.\nHashimoto, and T. Gerstenberg. Moca: Mea-\nsuring human-language model alignment on\ncausal and moral judgment tasks. NeurIPS, 36,\n2024.\nR. Paiss, A. Ephrat, O. Tov, S. Zada, I. Mosseri,\nM. Irani, and T. Dekel. Teaching clip to count\nto ten. ICCV, 2023.\nM. Phuong,\nM. Aitchison,\nE. Catt,\nS. Co-\ngan, A. Kaskasoli, V. Krakovna, D. Lindner,\nM. Rahtz, Y. Assael, S. Hodkinson, H. Howard,\nT. Lieberum, R. Kumar, M. A. Raad, A. Webson,\nL. Ho, S. Lin, S. Farquhar, M. Hutter, G. Dele-\ntang, A. Ruoss, S. El-Sayed, S. Brown, A. Dra-\ngan, R. Shah, A. Dafoe, and T. Shevlane. Evalu-\nating frontier models for dangerous capabilities,\n2024.\nA. Radford, J. W. Kim, C. Hallacy, A. Ramesh,\nG. Goh, S. Agarwal, G. Sastry, A. Askell,\nP. Mishkin, J. Clark, et al. Learning transferable\nvisual models from natural language supervi-\nsion. In ICML, pages 8748–8763. PMLR, 2021.\nA. Ramé, J. Ferret, N. Vieillard, R. Dadashi,\nL. Hussenot, P.-L. Cedoz, P. G. Sessa, S. Girgin,\nA. Douillard, and O. Bachem. WARP: On the\nbenefits of weight averaged rewarded policies,\n2024a.\nA. Ramé, N. Vieillard, L. Hussenot, R. Dadashi,\nG. Cideron, O. Bachem, and J. Ferret. WARM:\nOn the benefits of weight averaged reward mod-\nels. In ICML, 2024b.\n14\n",
  "page_15": "Gemma 3 Technical Report\nD. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y.\nPang, J. Dirani, J. Michael, and S. R. Bow-\nman. Gpqa: A graduate-level google-proof q&a\nbenchmark. ArXiv, abs/2311.12022, 2023.\nJ. Ren,\nS. Rajbhandari,\nR. Y. Aminabadi,\nO. Ruwase, S. Yang, M. Zhang, D. Li, and\nY. He.\nZero-offload: Democratizing billion-\nscale model training. In USENIX, 2021.\nA. Roberts, H. W. Chung, G. Mishra, A. Levskaya,\nJ. Bradbury, D. Andor, S. Narang, B. Lester,\nC. Gaffney, A. Mohiuddin, et al. Scaling up\nmodels and data with t5x and seqio. JMLR,\n2023.\nN. Sachdeva, B. Coleman, W.-C. Kang, J. Ni,\nL. Hong, E. H. Chi, J. Caverlee, J. McAuley, and\nD. Z. Cheng. How to train data-efficient llms.\narXiv preprint arXiv:2402.09668, 2024.\nK. Sakaguchi, R. L. Bras, C. Bhagavatula, and\nY. Choi.\nWINOGRANDE: an adversarial\nwinograd schema challenge at scale. CoRR,\nabs/1907.10641, 2019.\nE. Sánchez, B. Alastruey, C. Ropers, P. Stenetorp,\nM. Artetxe, and M. R. Costa-jussà. Linguini:\nA benchmark for language-agnostic linguistic\nreasoning. arXiv preprint arXiv:2409.12126,\n2024.\nM. Sap, H. Rashkin, D. Chen, R. L. Bras,\nand Y. Choi.\nSocialiqa:\nCommonsense\nreasoning about social interactions.\nCoRR,\nabs/1904.09728, 2019.\nP. G. Sessa, R. Dadashi, L. Hussenot, J. Ferret,\nN. Vieillard, A. Ramé, B. Shariari, S. Perrin,\nA. Friesen, G. Cideron, S. Girgin, P. Stanczyk,\nA. Michi, D. Sinopalnikov, S. Ramos, A. Héliou,\nA. Severyn, M. Hoffman, N. Momchev, and\nO. Bachem. Bond: Aligning llms with best-of-n\ndistillation, 2024.\nK. Shah, N. Dikkala, X. Wang, and R. Panigrahy.\nCausal language modeling can elicit search and\nreasoning capabilities on logic puzzles. arXiv\npreprint arXiv:2409.10502, 2024.\nT. Shevlane, S. Farquhar, B. Garfinkel, M. Phuong,\nJ. Whittlestone, J. Leung, D. Kokotajlo, N. Mar-\nchal, M. Anderljung, N. Kolt, L. Ho, D. Sid-\ndarth, S. Avin, W. Hawkins, B. Kim, I. Gabriel,\nV. Bolina, J. Clark, Y. Bengio, P. Christiano, and\nA. Dafoe. Model evaluation for extreme risks,\n2023.\nF. Shi, M. Suzgun, M. Freitag, X. Wang, S. Sri-\nvats, S. Vosoughi, H. W. Chung, Y. Tay, S. Ruder,\nD. Zhou, D. Das, and J. Wei. Language models\nare multilingual chain-of-thought reasoners. In\nICLR, 2023.\nA. Singh, V. Natarjan, M. Shah, Y. Jiang, X. Chen,\nD. Parikh, and M. Rohrbach. Towards vqa mod-\nels that can read. In CVPR, 2019.\nH. Singh, N. Gupta, S. Bharadwaj, D. Tewari,\nand P. Talukdar. Indicgenbench: a multilin-\ngual benchmark to evaluate generation capabil-\nities of llms on indic languages. arXiv preprint\narXiv:2404.16816, 2024a.\nS. Singh, A. Romanou, C. Fourrier, D. I. Adelani,\nJ. G. Ngui, D. Vila-Suero, P. Limkonchotiwat,\nK. Marchisio, W. Q. Leong, Y. Susanto, R. Ng,\nS. Longpre, W.-Y. Ko, M. Smith, A. Bosselut,\nA. Oh, A. F. T. Martins, L. Choshen, D. Ippolito,\nE. Ferrante, M. Fadaee, B. Ermis, and S. Hooker.\nGlobal mmlu: Understanding and addressing\ncultural and linguistic biases in multilingual\nevaluation, 2024b.\nA. Steiner, A. S. Pinto, M. Tschannen, D. Key-\nsers, X. Wang, Y. Bitton, A. Gritsenko, M. Min-\nderer, A. Sherbondy, S. Long, S. Qin, R. In-\ngle, E. Bugliarello, S. Kazemzadeh, T. Mes-\nnard, I. Alabdulmohsin, L. Beyer, and X. Zhai.\nPaliGemma 2: A Family of Versatile VLMs\nfor Transfer. arXiv preprint arXiv:2412.03555,\n2024.\nM. Suzgun, N. Scales, N. Schärli, S. Gehrmann,\nY. Tay, H. W. Chung, A. Chowdhery, Q. V. Le,\nE. H. Chi, D. Zhou, and J. Wei. Challenging\nbig-bench tasks and whether chain-of-thought\ncan solve them, 2022.\nG. Tyen, H. Mansoor, P. Chen, T. Mak, and\nV. Cărbune.\nLlms cannot find reasoning er-\nrors, but can correct them!\narXiv preprint\narXiv:2311.08516, 2023.\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,\nL. Jones, A. N. Gomez, L. Kaiser, and I. Polo-\nsukhin. Attention is all you need. 2017.\n15\n",
  "page_16": "Gemma 3 Technical Report\nK. Vodrahalli, S. Ontanon, N. Tripuraneni, K. Xu,\nS. Jain, R. Shivanna, J. Hui, N. Dikkala,\nM. Kazemi, B. Fatemi, et al.\nMichelangelo:\nLong context evaluations beyond haystacks\nvia latent structure queries.\narXiv preprint\narXiv:2409.12640, 2024.\nY. Wang, X. Ma, G. Zhang, Y. Ni, A. Chandra,\nS. Guo, W. Ren, A. Arulraj, X. He, Z. Jiang,\net al. Mmlu-pro: A more robust and challenging\nmulti-task language understanding benchmark.\nIn NeurIPS, 2024.\nL. Weidinger, J. Mellor, M. Rauh, C. Griffin,\nJ. Uesato, P.-S. Huang, M. Cheng, M. Glaese,\nB. Balle, A. Kasirzadeh, Z. Kenton, S. Brown,\nW. Hawkins, T. Stepleton, C. Biles, A. Birhane,\nJ. Haas, L. Rimell, L. A. Hendricks, W. Isaac,\nS. Legassick, G. Irving, and I. Gabriel. Ethical\nand social risks of harm from language models,\n2021.\nC. White, S. Dooley, M. Roberts, A. Pal, B. Feuer,\nS. Jain, R. Shwartz-Ziv, N. Jain, K. Saiful-\nlah, S. Naidu, et al. Livebench: A challeng-\ning, contamination-free llm benchmark. arXiv\npreprint arXiv:2406.19314, 2024.\nM. Wortsman, P. J. Liu, L. Xiao, K. Everett,\nA. Alemi, B. Adlam, J. D. Co-Reyes, I. Gur, A. Ku-\nmar, R. Novak, et al. Small-scale proxies for\nlarge-scale transformer training instabilities.\narXiv preprint arXiv:2309.14322, 2023.\nXLA.\nXla:\nOptimizing compiler for tensor-\nflow, 2019. URL https://www.tensorflow.\norg/xla.\nY. Xu, H. Lee, D. Chen, B. A. Hechtman, Y. Huang,\nR. Joshi, M. Krikun, D. Lepikhin, A. Ly, M. Mag-\ngioni, R. Pang, N. Shazeer, S. Wang, T. Wang,\nY. Wu, and Z. Chen. GSPMD: general and scal-\nable parallelization for ML computation graphs.\n2021.\nY. Yamada, Y. Bao, A. K. Lampinen, J. Kasai,\nand I. Yildirim. Evaluating spatial understand-\ning of large language models. arXiv preprint\narXiv:2310.14540, 2023.\nK. Yang, O. Russakovsky, and J. Deng.\nSpa-\ntialsense:\nAn adversarially crowdsourced\nbenchmark for spatial relation recognition.\nICCV, 2019.\nX. Yue, Y. Ni, K. Zhang, T. Zheng, R. Liu, G. Zhang,\nS. Stevens, D. Jiang, W. Ren, Y. Sun, C. Wei,\nB. Yu, R. Yuan, R. Sun, M. Yin, B. Zheng,\nZ. Yang, Y. Liu, W. Huang, H. Sun, Y. Su,\nand W. Chen.\nMmmu:\nA massive multi-\ndiscipline multimodal understanding and rea-\nsoning benchmark for expert agi. CVPR, 2023.\nR. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and\nY. Choi. HellaSwag: Can a machine really finish\nyour sentence? In ACL, 2019.\nX. Zhai, B. Mustafa, A. Kolesnikov, and L. Beyer.\nSigmoid loss for language image pre-training.\nIn CVPR, 2023.\nB. Zhang and R. Sennrich. Root mean square\nlayer normalization. 2019.\nJ. Zhang, L. Jain, Y. Guo, J. Chen, K. L. Zhou,\nS. Suresh, A. Wagenmaker, S. Sievert, T. Rogers,\nK. Jamieson, et al.\nHumor in ai: Massive\nscale crowd-sourced preferences and bench-\nmarks for cartoon captioning. arXiv preprint\narXiv:2406.10522, 2024.\nW. Zhong, R. Cui, Y. Guo, Y. Liang, S. Lu, Y. Wang,\nA. Saied, W. Chen, and N. Duan. Agieval: A\nhuman-centric benchmark for evaluating foun-\ndation models, 2023.\n16\n",
  "page_17": "Gemma 3 Technical Report\nCore contributors\nAishwarya Kamath∗\nJohan Ferret∗\nShreya Pathak∗\nNino Vieillard∗\nRamona Merhej∗\nSarah Perrin∗\nTatiana Matejovicova∗\nAlexandre Ramé∗\nMorgane Rivière∗\nLouis Rouillard∗\nThomas Mesnard∗\nGeoffrey Cideron∗\nJean-bastien Grill∗\nSabela Ramos∗\nEdouard Yvinec∗\nMichelle Casbon∗\nEtienne Pot\nIvo Penchev\nGaël Liu\nFrancesco Visin\nKathleen Kenealy\nLucas Beyer\nXiaohai Zhai\nAnton Tsitsulin\nRobert Busa-Fekete\nAlex Feng\nNoveen Sachdeva\nBenjamin Coleman\nYi Gao\nBasil Mustafa\nIain Barr\nEmilio Parisotto\nDavid Tian\nMatan Eyal\nColin Cherry\nJan-Thorsten Peter\nDanila Sinopalnikov\nSurya Bhupatiraju\nRishabh Agarwal\nMehran Kazemi\nDan Malkin\nDavid Vilar\nIdan Brusilovsky\nJiaming Luo\nAndreas Steiner\n∗co-first authors.\nContributors (alphabetical order)\nAbe Friesen\nAbhanshu Sharma\nAbheesht Sharma\nAdi Mayrav Gilady\nAdrian Goedeckemeyer\nAlex Feng\nAlexander Kolesnikov\nAlexei Bendebury\nAlvin Abdagic\nAmit Vadi\nAndré Susano Pinto\nAnil Das\nAnkur Bapna\nAntoine Miech\nAntoine Yang\nAntonia Paterson\nAshish Shenoy\nAyan Chakrabarti\nBilal Piot\nBo Wu\nBobak Shahriari\nBryce Petrini\nCharlie Chen\nCharline Le Lan\nChristopher A. Choquette-Choo\nCJ Carey\nCormac Brick\nDaniel Deutsch\nDanielle Eisenbud\nDee Cattle\nDerek Cheng\nDimitris Paparas\nDivyashree Shivakumar Sreepathihalli\nDoug Reid\nDustin Tran\nDustin Zelle\nEric Noland\nErwin Huizenga\nEugene Kharitonov\nFrederick Liu\nGagik Amirkhanyan\nGlenn Cameron\nHadi Hashemi\nHanna Klimczak-Plucińska\nHarman Singh\nHarsh Mehta\nHarshal Tushar Lehri\nHussein Hazimeh\n17\n",
  "page_18": "Gemma 3 Technical Report\nIan Ballantyne\nIdan Szpektor\nIvan Nardini\nJean Pouget-Abadie\nJetha Chan\nJoe Stanton\nJohn Wieting\nJonathan Lai\nJordi Orbay\nJoseph Fernandez\nJosh Newlan\nJu-yeong Ji\nJyotinder Singh\nKat Black\nKathy Yu\nKevin Hui\nKiran Vodrahalli\nKlaus Greff\nLinhai Qiu\nMarcella Valentine\nMarina Coelho\nMarvin Ritter\nMatt Hoffman\nMatthew Watson\nMayank Chaturvedi\nMichael Moynihan\nMin Ma\nNabila Babar\nNatasha Noy\nNathan Byrd\nNick Roy\nNikola Momchev\nNilay Chauhan\nNoveen Sachdeva\nOskar Bunyan\nPankil Botarda\nPaul Kishan Rubenstein\nPhil Culliton\nPhilipp Schmid\nPier Giuseppe Sessa\nPingmei Xu\nPiotr Stanczyk\nPouya Tafti\nRakesh Shivanna\nRavin Kumar\nRenjie Wu\nRenke Pan\nReza Rokni\nRob Willoughby\nRohith Vallu\nRyan Mullins\nSammy Jerome\nSara Smoot\nSertan Girgin\nShariq Iqbal\nShashir Reddy\nShruti Sheth\nSiim Põder\nSijal Bhatnagar\nSindhu Raghuram Panyam\nSivan Eiger\nSusan Zhang\nTianqi Liu\nTrevor Yacovone\nTyler Liechty\nUday Kalra\nUtku Evci\nVedant Misra\nVincent Roseberry\nVlad Feinberg\nVlad Kolesnikov\nWoohyun Han\nWoosuk Kwon\nYinlam Chow\nZichuan Wei\nZoltan Egyed\nSupport\nVictor Cotruta\nMinh Giang\nPhoebe Kirk\nAnand Rao\nKat Black\nNabila Babar\nJessica Lo\nErica Moreira\nLuiz Gustavo Martins\nOmar Sanseviero\nLucas Gonzalez\nZach Gleicher\nTris Warkentin\nSponsors\nVahab Mirrokni\nEvan Senter\nEli Collins\nJoelle Barral\n18\n",
  "page_19": "Gemma 3 Technical Report\nZoubin Ghahramani\nRaia Hadsell\nD. Sculley\nSlav Petrov\nNoah Fiedel\nNoam Shazeer\nOriol Vinyals\nJeff Dean\nDemis Hassabis\nKoray Kavukcuoglu\nClement Farabet\nTechnical advisors\nElena Buchatskaya\nJean-Baptiste Alayrac\nRohan Anil\nDmitry (Dima) Lepikhin\nSebastian Borgeaud\nOlivier Bachem\nLead\nArmand Joulin\nTechnical leads\nAlek Andreev\nCassidy Hardin\nRobert Dadashi\nLéonard Hussenot\n19\n",
  "page_20": "Gemma 3 Technical Report\nAppendix\nDetails of pre-trained performances.\nGemma 2\nGemma 3\n2B\n9B\n27B\n1B\n4B\n12B 27B\nHellaS 72.9 81.9 86.4\n62.3 77.2 84.2 85.6\nBoolQ 75.6 77.5 76.2\n63.2 72.3 78.8 82.4\nPIQA\n78.1 81.9 83.5\n73.8 79.6 81.8 83.3\nSIQA\n51.8 53.3 53.8\n48.9 51.9 53.4 54.9\nTQA\n60.2 76.5 83.8\n39.8 65.8 78.2 85.5\nNQ\n17.2 29.2 34.7\n9.48 20.0 31.4 36.1\nARC-C 55.8 69.1 71.4\n38.4 56.2 68.9 70.6\nARC-E 80.6 88.3 88.6\n73.0 82.4 88.3 89.0\nWinoG 65.4 73.9 79.4\n58.2 64.7 74.3 78.8\nBBH\n42.4 69.4 74.8\n28.4 50.9 72.6 77.7\nDrop\n53.2 71.5 75.2\n42.4 60.1 72.2 77.2\nTable 9 | Factuality, common-sense performance\nand reasoning after pre-training phase.\nFactuality and common-sense. In Table 9, we\nreport the performance of our new pre-trained\nbenchmarks compared to previous versions. We\nconsider several standard benchmarks, namely\nHellaSwag (Zellers et al., 2019), BoolQ (Clark\net al., 2019), PIQA (Bisk et al., 2019), SIQA (Sap\net al., 2019), TriviaQA (Joshi et al., 2017), Natu-\nral Questions (Kwiatkowski et al., 2019), ARC-C\nand ARC-E (Chollet, 2019), WinoGrande (Sak-\naguchi et al., 2019), BBH (Suzgun et al., 2022),\nDROP (Dua et al., 2019). Evaluation details are\ndescribed in Table 19. Overall, our models are in\nthe same ballpark as Gemma 2, which is encour-\naging since these abilities are not the focus of the\nimprovements brought in this version.\nSTEM and code.\nThe details of our per-\nformance on STEM and Code are in Ta-\nble 10.\nWe consider several standard bench-\nmarks, namely MMLU (Hendrycks et al., 2020),\nMMLU-Pro (Wang et al., 2024), AGIEval (Zhong\net al., 2023), MATH (Hendrycks et al., 2021),\nGSM8K (Cobbe et al., 2021), GPQA (Rein\net al., 2023), MBPP (Austin et al., 2021), Hu-\nmanEval (Chen et al., 2021). Evaluation details\nare described in Table 19. Overall we see a consis-\ntent improvement over STEM abilities across our\nGemma 2\nGemma 3\n2B\n9B\n27B\n4B\n12B 27B\nMMLU\n52.2 71.2 75.2\n59.6 74.5 78.6\nMMLUpro 22.2 43.7 49.4\n29.2 45.3 52.2\nAGIE\n31.6 53.1 55.1\n42.1 57.4 66.2\nMATH\n16.4 36.4 42.1\n24.2 43.3 50.0\nGSM8K\n25.0 70.2 74.6\n38.4 71.0 82.6\nGPQA\n12.5 24.8 26.3\n15.0 25.4 24.3\nMBPP\n31.0 51.2 60.8\n46.0 60.4 65.6\nHumanE\n19.5 40.2 51.2\n36.0 45.7 48.8\nTable 10 | STEM and code performance after pre-\ntraining phase.\npre-trained models. On code, we see a similar\nimprovement for the 4B and 12B models but not\non the 27B.\n4B\n12B\n27B\nCOCO caption\n102\n111\n116\nDocVQA\n72.8\n82.3\n85.6\nInfoVQA\n44.1\n54.8\n59.4\nMMMU\n39.2\n50.3\n56.1\nTextVQA\n58.9\n66.5\n68.6\nRealWorldQA\n45.5\n52.2\n53.9\nReMI\n27.3\n38.5\n44.8\nAI2D\n63.2\n75.2\n79.0\nChartQA\n63.6\n74.7\n76.3\nVQAv2\n63.9\n71.2\n72.9\nBLINK\n38.0\n35.9\n39.6\nOK-VQA\n51.0\n58.7\n60.2\nTallyQA\n42.5\n51.8\n54.3\nSpatialSense VQA\n50.9\n60.0\n59.4\nCountBench VQA\n26.1\n17.8\n68.0\nTable 11 | Multimodal performance after pre-\ntraining phase. The scores are on the val split\nof each dataset without P&S.\nImage understanding.\nIn Table 11, we re-\nport performance across a variety of visual\nquestion answer benchmarks for the different\nmodels that were trained with a vision en-\ncoder, namely COCO Caption (Chen et al.,\n2015), DocVQA (Mathew et al., 2020), Info-\ngraphicVQA (Mathew et al., 2022), MMMU (Yue\net al., 2023), TextVQA (Singh et al., 2019), Re-\nalWorldQA (Rea), ReMI (Kazemi et al., 2024a),\n20\n",
  "page_21": "Gemma 3 Technical Report\nAI2D (Kembhavi et al., 2016), ChartQA (Masry\net al., 2022), VQA v2 (Goyal et al., 2017),\nBLINK (Fu et al., 2024), OK-VQA (Marino et al.,\n2019), TallyQA (Acharya et al., 2018), Spa-\ntialSense VQA (Yang et al., 2019), CountBench\nVQA (Paiss et al., 2023). Evaluation details are\ndescribed in Table 20.\nPaliGemma 2\nGemma 3\n2B\n9B\n27B\n4B\n12B 27B\nDocVQA\n81.6 86.3 85.1\n86.1 89.0 89.5\nInfoVQA\n41.4 53.1 50.2\n55.6 61.6 64.6\nTextVQA\n76.3 76.3 75.1\n79.1 81.6 83.2\nChartQA\n70.7 79.1 71.3\n79.8 83.5 83.4\nAI2D\n76.0 84.4 84.6\n80.9 85.6 86.5\nOKVQA\n64.1 68.6 70.6\n65.2 69.3 71.1\nCountBenchQA 82.0 85.3 87.4\n79.4 83.5 87.8\nCOCO caption\n143. 145. 145.\n143. 143. 144.\nVQAv2\n84.8 85.8 85.8\n84.1 84.9 85.1\nTally QA\n80.6 82.4 82.1\n79.0 81.3 81.7\nTable 12 | Performance of pre-trained checkpoints\nafter fine-tuning on multi-modal benchmarks\n(without P&S). PaliGemma 2 was transfered at\n896x896 resolution for the first four benchmarks,\nand at 448x448 resolution for the others.\nComparison to PaliGemma 2. We fine-tune mul-\ntimodal Gemma 3 pre-trained checkpoints fol-\nlowing the protocol from Steiner et al. (2024) –\nonly learning rate is swept, otherwise same trans-\nfer settings are used. The results in Table 12\nshow that Gemma 3 excels at benchmarks in-\nvolving document understanding, even outper-\nforming the larger PaliGemma 2 variant. Note\nthat due to average pooling in the vision en-\ncoder the Gemma 3 4B and 12B models are\nabout 10x cheaper to transfer compared with the\nPaliGemma 2 9B and 27B models at the same 896\nx 896 resolution. Gemma 3 also performs better\non AI2D and OKVQA, but PaliGemma 2 performs\nslightly better on VQAv2 and COCO caption.\nMultilinguality. In Table 13 we report the per-\nformance of the pre-trained models on multilin-\ngual tasks. We apply in-context learning with\nmulti-shot prompting and present results on\nthe following benchmarks: MGSM (Shi et al.,\n2023), Global-MMLU-Lite (Singh et al., 2024b),\nWMT24++ (Deutsch et al., 2025), FLoRes (Goyal\nGemma 2\nGemma 3\n2B\n9B\n27B\n1B\n4B\n12B 27B\nMGSM\n18.7 57.3 68.0\n2.04 34.7 64.3 74.3\nGMMLU\n43.3 64.0 69.4\n24.9 57.0 69.4 75.7\nWMT24++ 38.8 50.3 53.0\n36.7 48.4 53.9 55.7\nFlores\n30.2 41.3 44.3\n29.5 39.2 46.0 48.8\nXQuAD\n53.7 72.2 73.9\n43.9 68.0 74.5 76.8\nECLeKTic 8.29 14.0 17.1\n4.69 11.0 17.2 24.4\nIndicGB\n47.4 59.3 62.1\n41.4 57.2 61.7 63.4\nTable 13 | Multilingual performance after the pre-\ntraining phase. IndicGenBench is an average over\nbenchmarks reported in Table 14.\net al., 2022), XQuAD (Artetxe et al., 2020),\nECLeKTic (Goldman et al., 2025), IndicGen-\nBench (Singh et al., 2024a), XOR QA (Asai et al.,\n2020). Evaluation details are described in Ta-\nble 19.\nGemma 2\nGemma 3\n2B\n9B\n27B\n1B\n4B\n12B 27B\nXQuAD Indic 54.3 73.1 74.9\n43.1 68.3 75.2 77.8\nXORQA in-en 66.2 69.3 72.5\n56.3 68.3 69.8 70.4\nXORQA in-xx 31.2 40.8 44.3\n27.1 39.8 43.8 46.0\nFlores Indic\n38.1 54.0 56.9\n39.0 52.3 58.0 59.5\nTable 14 | Detailed IndicGenBench performance\nafter the pre-training phase.\nLong context. In Table 15 we report the per-\nformance of pre-trained and fine-tuned mod-\nels on long context benchmarks.\nWe include\nRULER (Hsieh et al., 2024) and MRCR (Vodra-\nhalli et al., 2024) benchmarks evaluating at 32K\nand 128K sequence lengths.\n8.1. Performance of IT models\nWe report in Table 18, additional benchmarks\non our IT models.\nNote that N2C refers to\nNatural2Code, the Gemini 1.0 internal held-out\ndataset, which uses author-generated sources in-\nstead of web-based information. BBEH refers to\nBIG-Bench Extra Hard (Kazemi et al., 2025), a\nchallenging LLM reasoning benchmark that aggre-\ngates several reasoning tasks (Fatemi et al., 2024;\n21\n",
  "page_22": "Gemma 3 Technical Report\nGemma 3 PT\nGemma 3 IT\nContext 4B\n12B 27B\n4B\n12B 27B\nRULER\n32K\n67.1 90.6 85.9\n61.4 80.3 91.1\nRULER\n128K\n51.7 80.7 72.9\n46.8 57.1 66.0\nMRCR\n32K\n44.7 59.8 63.2 49.8 53.7 63.2\nMRCR\n128K\n40.6 56.9 60.0 44.6 49.8 59.3\nTable 15 | Performance of pre-trained (PT) and\ninstruction fine-tuned (IT) models on long context\nbenchmarks at different context lengths.\n4B\n12B\n27B\nMMMU (val)\n48.8\n59.6\n64.9\nDocVQA\n75.8\n87.1\n86.6\nInfoVQA\n50.0\n64.9\n70.6\nTextVQA\n57.8\n67.7\n65.1\nAI2D\n74.8\n84.2\n84.5\nChartQA\n68.8\n75.7\n78.0\nVQAv2 (val)\n62.4\n71.6\n71.0\nMathVista (testmini)\n50.0\n62.9\n67.6\nTable 16 | Performance of instruction fine-tuned\n(IT) models on multimodal benchmarks. If not\nmentioned, these results are on the final test set\nof each dataset with P&S applied.\nHessel et al., 2022; Kazemi et al., 2023, 2024b;\nKıcıman et al., 2023; Nie et al., 2024; Sánchez\net al., 2024; Shah et al., 2024; Tyen et al., 2023;\nWhite et al., 2024; Yamada et al., 2023; Zhang\net al., 2024). ECLeKTic refers to Goldman et al.\n(2025). We report the micro average score. More\nevaluation details are described in Table 21.\n8.2. Performance of IT models on video under-\nstanding\nAdditional multimodal evaluations. Gemma\n3 IT models were evaluated on common vision\nbenchmarks following the evaluation protocol of\nGemini 1.5 (Gemini Team, 2024). The results are\ngiven in Table 16 when P&S is activated.\n4B\n12B\n27B\nPerception Test MCVQA\n50.6\n54.9\n58.1\nActivityNet-QA\n46.3\n50.4\n52.8\nTable 17 | Performance of instruction fine-tuned\n(IT) models on vision understanding benchmarks\nusing 0 shot with 16 frames linspace.\nPer-\nception Test consists of real-world videos de-\nsigned to show perceptually interesting situa-\ntions and we report results on the multiple choice\nvideo QA benchmark in terms of top-1 accuracy.\nActivityNet-QA reports standard gpt-evaluation.\n22\n",
  "page_23": "Gemma 3 Technical Report\nGemma 2\nGemma 3\n2B\n9B\n27B\n1B\n4B\n12B\n27B\nMMLU\n56.1\n71.3\n76.2\n38.8\n58.1\n71.9\n76.9\nMBPP\n36.6\n59.2\n67.4\n35.2\n63.2\n73.0\n74.4\nHumanEval\n20.1\n40.2\n51.8\n41.5\n71.3\n85.4\n87.8\nN2C\n46.8\n68.3\n77.3\n56.0\n70.3\n80.7\n84.5\nLiveCodeBench\n7.0\n20.0\n29.0\n5.0\n23.0\n32.0\n39.0\nGSM8K\n62.6\n88.1\n91.1\n62.8\n89.2\n94.4\n95.9\nMATH\n27.2\n49.4\n55.6\n48.0\n75.6\n83.8\n89.0\nHiddenMath\n2.0\n8.0\n12.0\n15.0\n42.0\n51.0\n56.0\nBBH\n41.4\n69.0\n74.9\n39.1\n72.2\n85.7\n87.6\nBBEH\n5.9\n9.8\n14.8\n7.2\n11.0\n16.3\n19.3\nIFEval\n80.4\n88.4\n91.1\n80.2\n90.2\n88.9\n90.4\nGMMLU-Lite\n41.9\n64.8\n68.6\n34.2\n54.5\n69.5\n75.1\nECLeKTic\n5.3\n11.8\n17.6\n1.4\n4.6\n10.3\n16.7\nWMT24++\n37.4\n48.7\n51.7\n35.9\n46.8\n51.6\n53.4\nTable 18 | Performance of instruction fine-tuned (IT) models of different sizes on more internal and\nexternal benchmarks.\n23\n",
  "page_24": "Gemma 3 Technical Report\nEvaluation\nMetric\nType\nn-shot\nCOT\nNorm\nMBPP\npass@1\nsampling\n3-shot\nHumanEval\npass@1\nsampling\n0-shot\nHellaSwag\nAccuracy\nscoring\n10-shot\nChar-Len\nBoolQ\nAccuracy\nscoring\n0-shot\nChar-Len\nPIQA\nAccuracy\nscoring\n0-shot\nChar-Len\nSIQA\nAccuracy\nscoring\n0-shot\nChar-Len\nTriviaQA\nAccuracy\nsampling\n5-shot\nNatural Questions\nAccuracy\nsampling\n5-shot\nARC-C\nAccuracy\nscoring\n25-shot\nChar-Len\nARC-E\nAccuracy\nscoring\n0-shot\nChar-Len\nWinoGrande\nAccuracy\nscoring\n5-shot\nChar-Len\nBBH\nAccuracy\nsampling\nfew-shot\nYes\nDROP\nToken F1 score\nsampling\n1-shot\nAGIEval\nAccuracy\nsampling\n3-5-shot\nMMLU\nAccuracy\nscoring\n5-shot\nChar-Len\nMATH\nAccuracy\nsampling\n4-shot\nYes\nGSM8K\nAccuracy\nsampling\n8-shot\nYes\nGPQA\nAccuracy\nsampling\n5-shot\nYes\nMMLU-Pro\nAccuracy\nsampling\n5-shot\nYes\nMGSM\nAccuracy\nsampling\n8-shot\nFLoRes\nCHaRacter-level F-score\nsampling\n1-shot\nGlobal-MMLU-Lite\nAccuracy\nscoring\n5-shot\nChar-Len\nXQuAD\nCHaRacter-level F-score\nsampling\n5-shot\nWMT24++\nCHaRacter-level F-score\nsampling\n5-shot\nECLeKTic\nECLeKTic score\nsampling\n2-shot\nFirst-line/strip\nXQuAD Indic\nCHaRacter-level F-score\nsampling\n5-shot\nXOR QA IN-EN\nCHaRacter-level F-score\nsampling\n5-shot\nXOR QA IN-XX\nCHaRacter-level F-score\nsampling\n5-shot\nFLoRes Indic\nCHaRacter-level F-score\nsampling\n5-shot\nRULER\nAccuracy\nsampling\n0-shot\nMRCR\nMRCR score\nsampling\nfew-shot\nTable 19 | Details on text benchmarks. Char-Len stands for Character Length Normalization and COT\nstands for Chain-Of-Thought prompting.\n24\n",
  "page_25": "Gemma 3 Technical Report\nEvaluation\nMetric\nType\nn-shot\nCOCO Caption\nCider score\nsampling\n4-shot\nDocVQA\nANLS score\nsampling\n4-shot\nInfographicVQA\nANLS score\nsampling\n4-shot\nMMMU\nAccuracy\nsampling\n3-shot text only\nTextVQA\nAccuracy\nsampling\n4-shot\nRealWorldQA\nAccuracy\nsampling\n4-shot text only\nReMI\nAccuracy\nsampling\n4-shot\nAI2D\nAccuracy\nsampling\n4-shot\nChartQA\nAccuracy\nsampling\n4-shot\nVQA v2\nAccuracy\nsampling\n4-shot\nBLINK\nAccuracy\nsampling\n0-shot\nOK-VQA\nAccuracy\nsampling\n4-shot\nTallyQA\nAccuracy\nsampling\n4-shot\nSpatialSense VQA\nAccuracy\nsampling\n4-shot\nCountBench VQA\nAccuracy\nsampling\n0-shot\nTable 20 | Details on vision benchmarks. No Chain-Of-Thought prompting nor normalization.\nEvaluation\nMetric\nType\nn-shot\nCOT\nMMLU\nAccuracy\nsampling\n0-shot\nMBPP\npass@1\nsampling\n3-shot\nHumanEval\npass@1\nsampling\n0-shot\nN2C\npass@1\nsampling\n0-shot\nLiveCodeBench\nAverage over 8 samples\nsampling\n0-shot\nYes\nGSM8K\nAccuracy\nsampling\n0-shot\nYes\nMATH\nAccuracy\nsampling\n0-shot\nHiddenMath\nAccuracy\nsampling\n0-shot\nBBH\nAccuracy\nsampling\n0-shot\nBBEH\nAccuracy\nsampling\n0-shot\nIFEval\nAccuracy\nsampling\n0-shot\nGlobal-MMLU-lite\nAccuracy\nsampling\n0-shot\nYes\nECLeKTic\nECLeKTic score\nsampling\n0-shot\nWMT24++\nCHaRacter-level F-score\nsampling\n0-shot\nTable 21 | Details on instruction fine-tuned (IT) benchmarks. No normalization.\n25\n"
}