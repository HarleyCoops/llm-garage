{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation from Raw PDF Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyMuPDF\n",
      "  Downloading pymupdf-1.25.4-cp39-abi3-win_amd64.whl.metadata (3.4 kB)\n",
      "Downloading pymupdf-1.25.4-cp39-abi3-win_amd64.whl (16.6 MB)\n",
      "   ---------------------------------------- 0.0/16.6 MB ? eta -:--:--\n",
      "   ----------------------------- ---------- 12.3/16.6 MB 71.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 16.6/16.6 MB 60.7 MB/s eta 0:00:00\n",
      "Installing collected packages: PyMuPDF\n",
      "Successfully installed PyMuPDF-1.25.4\n"
     ]
    }
   ],
   "source": [
    "!pip install PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text saved to ./data/extracted_pdf.json\n",
      "Processed fine-tuning dataset saved to ./data/fine_tuning_dataset.json\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import json\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def extract_text_from_pdf(pdf_path, output_json):\n",
    "    \"\"\"\n",
    "    Extracts text from each page of a PDF and saves it in a JSON file.\n",
    "    Each key is formatted as \"page_X\" where X is the page number.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    pdf_content = {}\n",
    "    for page_number in range(doc.page_count):\n",
    "        page = doc[page_number]\n",
    "        text = page.get_text()\n",
    "        pdf_content[f\"page_{page_number + 1}\"] = text\n",
    "    with open(output_json, 'w', encoding='utf-8') as f:\n",
    "        json.dump(pdf_content, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"Extracted text saved to {output_json}\")\n",
    "\n",
    "def remove_headers_footers(text):\n",
    "    \"\"\"\n",
    "    Remove lines that are likely headers or footers, such as page numbers\n",
    "    or lines that match \"Page X\" patterns.\n",
    "    \"\"\"\n",
    "    lines = text.splitlines()\n",
    "    cleaned_lines = []\n",
    "    for line in lines:\n",
    "        # Skip lines that contain only numbers\n",
    "        if re.match(r'^\\s*\\d+\\s*$', line):\n",
    "            continue\n",
    "        # Skip lines matching patterns like \"Page 1\" (case-insensitive)\n",
    "        if re.match(r'^\\s*Page\\s+\\d+\\s*$', line, re.IGNORECASE):\n",
    "            continue\n",
    "        cleaned_lines.append(line)\n",
    "    return \"\\n\".join(cleaned_lines).strip()\n",
    "\n",
    "def clean_special_characters(text):\n",
    "    \"\"\"\n",
    "    Normalize unicode characters and remove non-printable characters.\n",
    "    Also collapses multiple spaces/newlines.\n",
    "    \"\"\"\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    text = re.sub(r'[^\\x20-\\x7E]+', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def chunk_text(text, max_words=100):\n",
    "    \"\"\"\n",
    "    Split text into smaller chunks if it exceeds max_words.\n",
    "    Adjust max_words based on your model's input size limitations.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    if len(words) <= max_words:\n",
    "        return [text]\n",
    "    \n",
    "    chunks = []\n",
    "    for i in range(0, len(words), max_words):\n",
    "        chunk = \" \".join(words[i:i + max_words])\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "def process_pdf_json(input_file, output_file, max_words_per_chunk=100):\n",
    "    \"\"\"\n",
    "    Loads the extracted PDF JSON, cleans the text, and splits long pages into chunks.\n",
    "    Then, it formats each chunk as a dictionary with a \"text\" key and saves the data.\n",
    "    \"\"\"\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        pdf_data = json.load(f)\n",
    "    \n",
    "    training_examples = []\n",
    "    \n",
    "    for key, raw_text in pdf_data.items():\n",
    "        # Remove headers/footers\n",
    "        cleaned_text = remove_headers_footers(raw_text)\n",
    "        # Clean special characters and extra whitespace\n",
    "        cleaned_text = clean_special_characters(cleaned_text)\n",
    "        # Split the cleaned text into chunks if necessary\n",
    "        chunks = chunk_text(cleaned_text, max_words=max_words_per_chunk)\n",
    "        # Add each chunk as a separate training example\n",
    "        for chunk in chunks:\n",
    "            training_examples.append({\"text\": chunk})\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(training_examples, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"Processed fine-tuning dataset saved to {output_file}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # File paths (keep the file names as requested)\n",
    "    pdf_path = './data/augmentation.pdf'\n",
    "    extracted_json = './data/extracted_pdf.json'\n",
    "    fine_tuning_json = './data/fine_tuning_dataset.json'\n",
    "    \n",
    "    # Step 1: Extract text from the PDF and save it as JSON\n",
    "    extract_text_from_pdf(pdf_path, extracted_json)\n",
    "    \n",
    "    # Step 2: Process the extracted JSON and prepare fine-tuning data\n",
    "    process_pdf_json(extracted_json, fine_tuning_json, max_words_per_chunk=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers@ git+https://github.com/huggingface/transformers@46350f5eae87ac1d168ddfdc57a0b39b64b9a029\n",
      "  Cloning https://github.com/huggingface/transformers (to revision 46350f5eae87ac1d168ddfdc57a0b39b64b9a029) to c:\\users\\lucasmartins\\appdata\\local\\temp\\pip-install-u1498rba\\transformers_33d1cc310d5b4d8682ac396f2b1d4d77\n",
      "  Resolved https://github.com/huggingface/transformers to commit 46350f5eae87ac1d168ddfdc57a0b39b64b9a029\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: filelock in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from transformers@ git+https://github.com/huggingface/transformers@46350f5eae87ac1d168ddfdc57a0b39b64b9a029) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from transformers@ git+https://github.com/huggingface/transformers@46350f5eae87ac1d168ddfdc57a0b39b64b9a029) (0.29.3)\n",
      "Collecting numpy>=1.17 (from transformers@ git+https://github.com/huggingface/transformers@46350f5eae87ac1d168ddfdc57a0b39b64b9a029)\n",
      "  Downloading numpy-2.2.4-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from transformers@ git+https://github.com/huggingface/transformers@46350f5eae87ac1d168ddfdc57a0b39b64b9a029) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from transformers@ git+https://github.com/huggingface/transformers@46350f5eae87ac1d168ddfdc57a0b39b64b9a029) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers@ git+https://github.com/huggingface/transformers@46350f5eae87ac1d168ddfdc57a0b39b64b9a029)\n",
      "  Using cached regex-2024.11.6-cp313-cp313-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from transformers@ git+https://github.com/huggingface/transformers@46350f5eae87ac1d168ddfdc57a0b39b64b9a029) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers@ git+https://github.com/huggingface/transformers@46350f5eae87ac1d168ddfdc57a0b39b64b9a029)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers@ git+https://github.com/huggingface/transformers@46350f5eae87ac1d168ddfdc57a0b39b64b9a029)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from transformers@ git+https://github.com/huggingface/transformers@46350f5eae87ac1d168ddfdc57a0b39b64b9a029) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers@ git+https://github.com/huggingface/transformers@46350f5eae87ac1d168ddfdc57a0b39b64b9a029) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers@ git+https://github.com/huggingface/transformers@46350f5eae87ac1d168ddfdc57a0b39b64b9a029) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers@ git+https://github.com/huggingface/transformers@46350f5eae87ac1d168ddfdc57a0b39b64b9a029) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from requests->transformers@ git+https://github.com/huggingface/transformers@46350f5eae87ac1d168ddfdc57a0b39b64b9a029) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from requests->transformers@ git+https://github.com/huggingface/transformers@46350f5eae87ac1d168ddfdc57a0b39b64b9a029) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from requests->transformers@ git+https://github.com/huggingface/transformers@46350f5eae87ac1d168ddfdc57a0b39b64b9a029) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from requests->transformers@ git+https://github.com/huggingface/transformers@46350f5eae87ac1d168ddfdc57a0b39b64b9a029) (2025.1.31)\n",
      "Downloading numpy-2.2.4-cp313-cp313-win_amd64.whl (12.6 MB)\n",
      "   ---------------------------------------- 0.0/12.6 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 3.1/12.6 MB 19.1 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 7.9/12.6 MB 20.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.8/12.6 MB 20.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.6/12.6 MB 19.0 MB/s eta 0:00:00\n",
      "Using cached regex-2024.11.6-cp313-cp313-win_amd64.whl (273 kB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.4/2.4 MB 17.0 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (pyproject.toml): started\n",
      "  Building wheel for transformers (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for transformers: filename=transformers-4.50.0.dev0-py3-none-any.whl size=10936767 sha256=bf6fb3db9d966bf34f08e23266c741fa1cb96d01b20e4299aa2dc7f5eee4a9a7\n",
      "  Stored in directory: c:\\users\\lucasmartins\\appdata\\local\\pip\\cache\\wheels\\c3\\e4\\b1\\c918fc6f31e83369415e4e3016f7638615818889ea75964cc0\n",
      "Successfully built transformers\n",
      "Installing collected packages: safetensors, regex, numpy, tokenizers, transformers\n",
      "Successfully installed numpy-2.2.4 regex-2024.11.6 safetensors-0.5.3 tokenizers-0.21.1 transformers-4.50.0.dev0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers 'C:\\Users\\LucasMartins\\AppData\\Local\\Temp\\pip-install-u1498rba\\transformers_33d1cc310d5b4d8682ac396f2b1d4d77'\n",
      "  Running command git rev-parse -q --verify 'sha^46350f5eae87ac1d168ddfdc57a0b39b64b9a029'\n",
      "  Running command git fetch -q https://github.com/huggingface/transformers 46350f5eae87ac1d168ddfdc57a0b39b64b9a029\n",
      "  Running command git checkout -q 46350f5eae87ac1d168ddfdc57a0b39b64b9a029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface-hub==0.29.3 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (0.29.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from huggingface-hub==0.29.3) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from huggingface-hub==0.29.3) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from huggingface-hub==0.29.3) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from huggingface-hub==0.29.3) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from huggingface-hub==0.29.3) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from huggingface-hub==0.29.3) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from huggingface-hub==0.29.3) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub==0.29.3) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from requests->huggingface-hub==0.29.3) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from requests->huggingface-hub==0.29.3) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from requests->huggingface-hub==0.29.3) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from requests->huggingface-hub==0.29.3) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers@git+https://github.com/huggingface/transformers@46350f5eae87ac1d168ddfdc57a0b39b64b9a029\n",
    "!pip install huggingface-hub==0.29.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting trl\n",
      "  Using cached trl-0.15.2-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: accelerate>=0.34.0 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from trl) (1.5.2)\n",
      "Collecting datasets>=2.21.0 (from trl)\n",
      "  Downloading datasets-3.4.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting rich (from trl)\n",
      "  Using cached rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: transformers>=4.46.0 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from trl) (4.50.0.dev0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from accelerate>=0.34.0->trl) (2.2.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from accelerate>=0.34.0->trl) (24.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from accelerate>=0.34.0->trl) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from accelerate>=0.34.0->trl) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from accelerate>=0.34.0->trl) (2.6.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from accelerate>=0.34.0->trl) (0.29.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from accelerate>=0.34.0->trl) (0.5.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from datasets>=2.21.0->trl) (3.18.0)\n",
      "Collecting pyarrow>=15.0.0 (from datasets>=2.21.0->trl)\n",
      "  Using cached pyarrow-19.0.1-cp313-cp313-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.21.0->trl)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets>=2.21.0->trl)\n",
      "  Using cached pandas-2.2.3-cp313-cp313-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from datasets>=2.21.0->trl) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from datasets>=2.21.0->trl) (4.67.1)\n",
      "Collecting xxhash (from datasets>=2.21.0->trl)\n",
      "  Using cached xxhash-3.5.0-cp313-cp313-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets>=2.21.0->trl)\n",
      "  Using cached multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=2.21.0->trl)\n",
      "  Using cached fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets>=2.21.0->trl)\n",
      "  Downloading aiohttp-3.11.14-cp313-cp313-win_amd64.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from transformers>=4.46.0->trl) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from transformers>=4.46.0->trl) (0.21.1)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->trl)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from rich->trl) (2.19.1)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets>=2.21.0->trl)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets>=2.21.0->trl)\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets>=2.21.0->trl)\n",
      "  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets>=2.21.0->trl)\n",
      "  Using cached frozenlist-1.5.0-cp313-cp313-win_amd64.whl.metadata (14 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets>=2.21.0->trl)\n",
      "  Downloading multidict-6.2.0-cp313-cp313-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets>=2.21.0->trl)\n",
      "  Using cached propcache-0.3.0-cp313-cp313-win_amd64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets>=2.21.0->trl)\n",
      "  Using cached yarl-1.18.3-cp313-cp313-win_amd64.whl.metadata (71 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate>=0.34.0->trl) (4.12.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->trl)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (2025.1.31)\n",
      "Requirement already satisfied: networkx in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (77.0.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate>=0.34.0->trl) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from tqdm>=4.66.3->datasets>=2.21.0->trl) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from pandas->datasets>=2.21.0->trl) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets>=2.21.0->trl)\n",
      "  Using cached pytz-2025.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets>=2.21.0->trl)\n",
      "  Using cached tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.21.0->trl) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate>=0.34.0->trl) (3.0.2)\n",
      "Using cached trl-0.15.2-py3-none-any.whl (318 kB)\n",
      "Downloading datasets-3.4.1-py3-none-any.whl (487 kB)\n",
      "Using cached rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Downloading aiohttp-3.11.14-cp313-cp313-win_amd64.whl (436 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Using cached pyarrow-19.0.1-cp313-cp313-win_amd64.whl (25.2 MB)\n",
      "Using cached pandas-2.2.3-cp313-cp313-win_amd64.whl (11.5 MB)\n",
      "Using cached xxhash-3.5.0-cp313-cp313-win_amd64.whl (30 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Using cached frozenlist-1.5.0-cp313-cp313-win_amd64.whl (51 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading multidict-6.2.0-cp313-cp313-win_amd64.whl (28 kB)\n",
      "Using cached propcache-0.3.0-cp313-cp313-win_amd64.whl (43 kB)\n",
      "Using cached pytz-2025.1-py2.py3-none-any.whl (507 kB)\n",
      "Using cached tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "Using cached yarl-1.18.3-cp313-cp313-win_amd64.whl (315 kB)\n",
      "Installing collected packages: pytz, xxhash, tzdata, pyarrow, propcache, multidict, mdurl, fsspec, frozenlist, dill, attrs, aiohappyeyeballs, yarl, pandas, multiprocess, markdown-it-py, aiosignal, rich, aiohttp, datasets, trl\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.0\n",
      "    Uninstalling fsspec-2025.3.0:\n",
      "      Successfully uninstalled fsspec-2025.3.0\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.11.14 aiosignal-1.3.2 attrs-25.3.0 datasets-3.4.1 dill-0.3.8 frozenlist-1.5.0 fsspec-2024.12.0 markdown-it-py-3.0.0 mdurl-0.1.2 multidict-6.2.0 multiprocess-0.70.16 pandas-2.2.3 propcache-0.3.0 pyarrow-19.0.1 pytz-2025.1 rich-13.9.4 trl-0.15.2 tzdata-2025.1 xxhash-3.5.0 yarl-1.18.3\n",
      "Requirement already satisfied: peft in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (0.15.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from peft) (2.2.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from peft) (24.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from peft) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from peft) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from peft) (2.6.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from peft) (4.50.0.dev0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from peft) (1.5.2)\n",
      "Requirement already satisfied: safetensors in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from peft) (0.5.3)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from peft) (0.29.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (2024.12.0)\n",
      "Requirement already satisfied: requests in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from torch>=1.13.0->peft) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from torch>=1.13.0->peft) (77.0.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from tqdm->peft) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from transformers->peft) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from transformers->peft) (0.21.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install trl\n",
    "!pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from torch) (77.0.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LucasMartins\\Documents\\llm_garage\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\LucasMartins\\.cache\\huggingface\\hub\\models--google--gemma-3-4b-pt. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading shards: 100%|██████████| 2/2 [25:42<00:00, 771.13s/it] \n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.75it/s]\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m tokenizer.padding_side = \u001b[33m\"\u001b[39m\u001b[33mright\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Load the Gemma 3 model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m model = \u001b[43mGemma3ForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m model.config.use_cache = \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# Disable caching for training\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Set up LoRA configuration for causal language modeling\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LucasMartins\\Documents\\llm_garage\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:273\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    271\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    272\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m273\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    275\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LucasMartins\\Documents\\llm_garage\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:4531\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4528\u001b[39m         device_map_kwargs[\u001b[33m\"\u001b[39m\u001b[33moffload_buffers\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   4530\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fsdp_enabled() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_deepspeed_zero3_enabled():\n\u001b[32m-> \u001b[39m\u001b[32m4531\u001b[39m         \u001b[43mdispatch_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdevice_map_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4533\u001b[39m \u001b[38;5;66;03m# This is needed for the RotaryEmbedding, which was not initialized on the correct device as it is\u001b[39;00m\n\u001b[32m   4534\u001b[39m \u001b[38;5;66;03m# not part of the state_dict (persistent=False)\u001b[39;00m\n\u001b[32m   4535\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m device_mesh \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LucasMartins\\Documents\\llm_garage\\.venv\\Lib\\site-packages\\accelerate\\big_modeling.py:501\u001b[39m, in \u001b[36mdispatch_model\u001b[39m\u001b[34m(model, device_map, main_device, state_dict, offload_dir, offload_index, offload_buffers, skip_keys, preload_module_classes, force_hooks)\u001b[39m\n\u001b[32m    499\u001b[39m     device = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mxpu:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    500\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m device != \u001b[33m\"\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m501\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    503\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    504\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou are trying to offload the whole model to the disk. Please use the `disk_offload` function instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    505\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LucasMartins\\Documents\\llm_garage\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:3263\u001b[39m, in \u001b[36mPreTrainedModel.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   3258\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[32m   3259\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   3260\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3261\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3262\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m3263\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LucasMartins\\Documents\\llm_garage\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1343\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1340\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1341\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LucasMartins\\Documents\\llm_garage\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:903\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    902\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    907\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    908\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    914\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LucasMartins\\Documents\\llm_garage\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:903\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    902\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    907\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    908\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    914\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LucasMartins\\Documents\\llm_garage\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:930\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    926\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    927\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    928\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    929\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    931\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    933\u001b[39m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LucasMartins\\Documents\\llm_garage\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1336\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1334\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1335\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1336\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m   1337\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1338\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mwhen moving module from meta to a different device.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1339\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1340\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1341\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mNotImplementedError\u001b[39m: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device."
     ]
    }
   ],
   "source": [
    "#test Gemma3\n",
    "\n",
    "from transformers import AutoTokenizer, TrainingArguments, Gemma3ForCausalLM\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load a sample dataset (for example purposes, we use wikitext-2)\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "\n",
    "# Define the model name\n",
    "model_name = \"google/gemma-3-4b-pt\"\n",
    "\n",
    "# Load the tokenizer and adjust padding settings\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Use eos token as pad token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Load the Gemma 3 model\n",
    "model = Gemma3ForCausalLM.from_pretrained(model_name, device_map=\"cpu\")\n",
    "model.config.use_cache = False  # Disable caching for training\n",
    "\n",
    "# Set up LoRA configuration for causal language modeling\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=1,\n",
    "    save_steps=25,\n",
    "    report_to=\"tensorboard\",\n",
    "    group_by_length=True,\n",
    ")\n",
    "\n",
    "# Create the SFTTrainer with LoRA parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=lora_config,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./gemma3_finetuned\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
